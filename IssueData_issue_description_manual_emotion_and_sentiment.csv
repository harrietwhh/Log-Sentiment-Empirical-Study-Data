Filename,Issue_Report,Manual_Emotion,Manual_Sentiment
AMQ-3176,"End result is lots of connections that are taking too long to shutdown and in particular: {code}2011-02-07 08:56:49,422 | INFO  | sport: tcp:///xxx:56982 | TransportConnection | The connection to '/xxx:56981' is taking a long time to shutdown.
....
2011-02-07 08:56:49,458 | INFO  | sport: tcp:///xxx:56981 | TransportConnection | The connection to '/xxx:56982' is taking a long time to shutdown.{code} where there is an overlap, with two connections trying to stop each other.
Problem appears when the initiator of a duplex network connector sees a failure and trys to reconnect and the responder sees the old transport connector in place. It tries to stop the existing connection but does it in a sync call so the potential to block and lock is present.







2011-01-26 16:35:54,618 [..] INFO TransportConnection - The connection to '/xx:51585' is taking a long time to shutdown.
2011-01-26 16:35:56,500 [..] INFO TransportConnection - The connection to '/xx:51585' is taking a long time to shutdown.{code}

In particular,
","Fear, Surprise",-1
AMQ-3251,"The following error is generated when trying to configure ActiveMQ with JTA/XA
{code}
15:09:39,373 | WARN  | tenerContainer-1 | PooledSession                    | 47 - org.apache.activemq.activemq-pool - 5.4.2.fuse-03-09 | Caught exception trying rollback() when putting session back into the pool: javax.jms.TransactionInProgressException: Cannot rollback() inside an XASession
javax.jms.TransactionInProgressException: Cannot rollback() inside an XASession
	at org.apache.activemq.ActiveMQXASession.rollback(ActiveMQXASession.java:76)
	at org.apache.activemq.pool.PooledSession.close(PooledSession.java:111)
	at org.apache.activemq.pool.XaConnectionPool$Synchronization.afterCompletion(XaConnectionPool.java:90)
	at org.apache.geronimo.transaction.manager.TransactionImpl.afterCompletion(TransactionImpl.java:542)
	at org.apache.geronimo.transaction.manager.TransactionImpl.afterCompletion(TransactionImpl.java:535)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:326)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:250)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1009)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:754)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:723)
	at org.apache.aries.transaction.GeronimoPlatformTransactionManager.commit(GeronimoPlatformTransactionManager.java:76)
	at sun.reflect.GeneratedMethodAccessor519.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)[:1.6.0_24]
	at java.lang.reflect.Method.invoke(Method.java:597)[:1.6.0_24]
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.doInvoke(ServiceInvoker.java:58)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.invoke(ServiceInvoker.java:62)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invokeUnprivileged(ServiceTCCLInterceptor.java:56)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invoke(ServiceTCCLInterceptor.java:39)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.importer.support.LocalBundleContextAdvice.invoke(LocalBundleContextAdvice.java:59)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:202)[60:org.springframework.aop:3.0.5.RELEASE]
	at $Proxy409.commit(Unknown Source)[:]
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:257)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1058)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1050)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:947)[110:org.springframework.jms:3.0.5.RELEASE]
	at java.lang.Thread.run(Thread.java:680)[:1.6.0_24]
15:09:39,381 | WARN  | tenerContainer-1 | Transaction                      | 49 - org.apache.aries.transaction.manager - 0.2.0.incubating | Unexpected exception from afterCompletion; continuing
java.lang.RuntimeException: javax.jms.JMSException: Failed to invalidate session: org.apache.activemq.AlreadyClosedException: Cannot use The session has already been closed as it has already been closed
	at org.apache.activemq.pool.XaConnectionPool$Synchronization.afterCompletion(XaConnectionPool.java:93)
	at org.apache.geronimo.transaction.manager.TransactionImpl.afterCompletion(TransactionImpl.java:542)
	at org.apache.geronimo.transaction.manager.TransactionImpl.afterCompletion(TransactionImpl.java:535)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:326)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:250)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1009)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:754)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:723)
	at org.apache.aries.transaction.GeronimoPlatformTransactionManager.commit(GeronimoPlatformTransactionManager.java:76)
	at sun.reflect.GeneratedMethodAccessor519.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)[:1.6.0_24]
	at java.lang.reflect.Method.invoke(Method.java:597)[:1.6.0_24]
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.doInvoke(ServiceInvoker.java:58)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.service.importer.support.internal.aop.ServiceInvoker.invoke(ServiceInvoker.java:62)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invokeUnprivileged(ServiceTCCLInterceptor.java:56)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.service.util.internal.aop.ServiceTCCLInterceptor.invoke(ServiceTCCLInterceptor.java:39)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.osgi.service.importer.support.LocalBundleContextAdvice.invoke(LocalBundleContextAdvice.java:59)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:131)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:119)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)[60:org.springframework.aop:3.0.5.RELEASE]
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:202)[60:org.springframework.aop:3.0.5.RELEASE]
	at $Proxy409.commit(Unknown Source)[:]
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:257)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1058)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1050)[110:org.springframework.jms:3.0.5.RELEASE]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:947)[110:org.springframework.jms:3.0.5.RELEASE]
	at java.lang.Thread.run(Thread.java:680)[:1.6.0_24]
Caused by: javax.jms.JMSException: Failed to invalidate session: org.apache.activemq.AlreadyClosedException: Cannot use The session has already been closed as it has already been closed
	at org.apache.activemq.util.JMSExceptionSupport.create(JMSExceptionSupport.java:35)
	at org.apache.activemq.pool.SessionPool.invalidateSession(SessionPool.java:77)
	at org.apache.activemq.pool.PooledSession.close(PooledSession.java:123)
	at org.apache.activemq.pool.XaConnectionPool$Synchronization.afterCompletion(XaConnectionPool.java:90)
	... 33 more
Caused by: org.apache.activemq.AlreadyClosedException: Cannot use The session has already been closed as it has already been closed
	at org.apache.activemq.pool.PooledSession.getInternalSession(PooledSession.java:291)
	at org.apache.activemq.pool.SessionPool.destroyObject(SessionPool.java:90)
	at org.apache.commons.pool.impl.GenericObjectPool.invalidateObject(GenericObjectPool.java:1258)
	at org.apache.activemq.pool.SessionPool.invalidateSession(SessionPool.java:75)
{code}

Here is the config used to access to AMQ Broker

{code}
    <!-- Tx Manager -->
    <osgi:reference id=""txManager"" interface=""org.springframework.transaction.PlatformTransactionManager""/>

    <!-- Transaction POLICY used by Camel Transactional Route
         We refer to the ServiceMiX TxManager -->
    <bean id=""PROPAGATION_REQUIRED"" class=""org.apache.camel.spring.spi.SpringTransactionPolicy"">
       <property name=""transactionManager"" ref=""txManager""/>
    </bean>

    <bean id=""activemq"" class=""org.apache.activemq.camel.component.ActiveMQComponent"">
        <!-- <property name=""configuration"" ref=""jmsConfig""/> -->
        <property name=""connectionFactory"">
            <osgi:reference interface=""javax.jms.ConnectionFactory""/>
        </property>
        <property name=""transactionManager"" ref=""txManager""/>
        <property name=""transacted"" value=""true""/>
        <property name=""cacheLevel"" value=""0"" />
    </bean>

    <camelContext id=""camel"" xmlns=""http://camel.apache.org/schema/spring"">

        <route id=""queue-create-incident"">
            <from uri=""activemq:queue:incident""/>
            <transacted ref=""PROPAGATION_REQUIRED""/>
            <log message="">>> Incident received : ${body}""/>
            <bean ref=""processIncident"" method=""saveReport""/>
            <!-- <bean ref=""processIncident"" method=""generateError""/> -->
            <log message="">>> Record inserted : ${body}""/>
        </route>

2) ActiveMQ

    <bean id=""activemqConnectionFactory"" class=""org.apache.activemq.ActiveMQXAConnectionFactory"">
        <property name=""brokerURL"" value=""tcp://localhost:61616"" />
        <property name=""redeliveryPolicy"" ref=""redeliveryPolicy""/>
    </bean>

    <bean id=""pooledConnectionFactory"" class=""org.apache.activemq.pool.XaPooledConnectionFactory"">
        <property name=""maxConnections"" value=""8"" />
        <property name=""connectionFactory"" ref=""activemqConnectionFactory"" />
        <property name=""transactionManager"" ref=""transactionManager""/>
    </bean>

    <bean id=""redeliveryPolicy"" class=""org.apache.activemq.RedeliveryPolicy"">
        <property name=""maximumRedeliveries"" value=""0""/>
    </bean>

    <reference id=""transactionManager"" interface=""javax.transaction.TransactionManager"" />

    <service ref=""pooledConnectionFactory"" interface=""javax.jms.ConnectionFactory"">
        <service-properties>
            <entry key=""name"" value=""localhost""/>
        </service-properties>
    </service>
{code}",,
AMQ-3567,"The process that activemq uses to check if there has been inactivity for a connection has a flaw when it tries to close the connection because of inactivity. The current process generates the following interrupt exception. 

{code} 
2011-10-25 12:13:56,878 | DEBUG | org.apache.activemq.util.ServiceSupport - Could not stop service: tcp://localhost/127.0.0.1:61616. Reason: java.lang.InterruptedException
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1302)
    at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:253)
    at org.apache.activemq.transport.tcp.TcpTransport.doStop(TcpTransport.java:553)
    at org.apache.activemq.util.ServiceSupport.stop(ServiceSupport.java:70)
    at org.apache.activemq.transport.tcp.TcpTransport.stop(TcpTransport.java:570)
    at org.apache.activemq.transport.InactivityMonitor.stop(InactivityMonitor.java:132)
    at org.apache.activemq.transport.TransportFilter.stop(TransportFilter.java:65)
    at org.apache.activemq.transport.WireFormatNegotiator.stop(WireFormatNegotiator.java:91)
    at org.apache.activemq.util.ServiceSupport.dispose(ServiceSupport.java:43)
    at org.apache.activemq.transport.failover.FailoverTransport.disposeTransport(FailoverTransport.java:207)
    at org.apache.activemq.transport.failover.FailoverTransport.handleTransportFailure(FailoverTransport.java:223)
    at org.apache.activemq.transport.failover.FailoverTransport$3.onException(FailoverTransport.java:184)
    at org.apache.activemq.transport.TransportFilter.onException(TransportFilter.java:101)
    at org.apache.activemq.transport.WireFormatNegotiator.onException(WireFormatNegotiator.java:160)
    at org.apache.activemq.transport.InactivityMonitor.onException(InactivityMonitor.java:265)
    at org.apache.activemq.transport.InactivityMonitor$4.run(InactivityMonitor.java:185)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
{code} 

This is caused because the spawned thread in the AbstractInactivityMonitor classes readCheck method calls the onException method. This method will then call the stopMonitorThreads method which subsequently calls the shutdownNow method of the ASYNC_TASKS executor. This call causes the executor to call the interrupt method for all active threads in the executor. The problem is that the calling thread is part of the ASYNC_TASKS executor and therefore it is generating the interrupt exception. 

Here is the stack trace of the call that is causing the interrupt. 

{code} 
Daemon Thread [InactivityMonitor Async Task: java.util.concurrent.ThreadPoolExecutor$Worker@66da9ea4] (Suspended (entry into method interrupt in Thread))   
    Thread.interrupt() line: 902   
    ThreadPoolExecutor$Worker.interruptNow() line: 855   
    ThreadPoolExecutor.shutdownNow() line: 1167   
    InactivityMonitor.stopMonitorThreads() line: 363   
    InactivityMonitor.onException(IOException) line: 264   
    InactivityMonitor$4.run() line: 185   
    ThreadPoolExecutor$Worker.runTask(Runnable) line: 886   
    ThreadPoolExecutor$Worker.run() line: 908   
    Thread.run() line: 680  
{code} 


The solution is to replace the shutdownNow method call with shutdown. Subsequent testing with this change does not cause the interrupt exception. 

I was able to create a testcase that reproduces this issue. The testcase uses the useInactivityMonitor=false attribute to reproduce this issue, thanks Gary for the hint. Unfortunately there aren't any steps that I can use to determine that the raised interrupted exception was raised or not. The test will pass either way. 

A patch will be added to this issue.


",Love,1
AMQ-3649,"
{noformat}
2012-01-03 19:17:50,646 | DEBUG | commit: TX:ID:foo-7365-1325617993757-0:1:1 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///bar:13219
2012-01-03 19:17:50,650 | ERROR | KahaDB failed to store to Journal | org.apache.activemq.store.kahadb.MessageDatabase | ActiveMQ Transport: tcp:///bar:13219
java.io.EOFException
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:383)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
	at org.apache.kahadb.page.PageFile.readPage(PageFile.java:779)
	at org.apache.kahadb.page.Transaction$2.readPage(Transaction.java:440)
	at org.apache.kahadb.page.Transaction$2.<init>(Transaction.java:431)
	at org.apache.kahadb.page.Transaction.openInputStream(Transaction.java:428)
	at org.apache.kahadb.page.Transaction.load(Transaction.java:404)
	at org.apache.kahadb.page.Transaction.load(Transaction.java:361)
	at org.apache.kahadb.index.BTreeIndex.loadNode(BTreeIndex.java:262)
	at org.apache.kahadb.index.BTreeIndex.getRoot(BTreeIndex.java:174)
	at org.apache.kahadb.index.BTreeIndex.put(BTreeIndex.java:189)
	at org.apache.activemq.store.kahadb.MessageDatabase.upadateIndex(MessageDatabase.java:1026)
	at org.apache.activemq.store.kahadb.MessageDatabase$AddOpperation.execute(MessageDatabase.java:1777)
	at org.apache.activemq.store.kahadb.MessageDatabase$18.execute(MessageDatabase.java:976)
	at org.apache.kahadb.page.Transaction.execute(Transaction.java:728)
	at org.apache.activemq.store.kahadb.MessageDatabase.process(MessageDatabase.java:973)
	at org.apache.activemq.store.kahadb.MessageDatabase$13.visit(MessageDatabase.java:874)
	at org.apache.activemq.store.kahadb.data.KahaCommitCommand.visit(KahaCommitCommand.java:130)
	at org.apache.activemq.store.kahadb.MessageDatabase.process(MessageDatabase.java:856)
	at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:783)
	at org.apache.activemq.store.kahadb.KahaDBTransactionStore.commit(KahaDBTransactionStore.java:270)
	at org.apache.activemq.transaction.LocalTransaction.commit(LocalTransaction.java:72)
	at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:173)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:103)
	at org.apache.activemq.broker.TransportConnection.processCommitTransactionOnePhase(TransportConnection.java:420)
	at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:100)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:306)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:179)
	at org.apache.activemq.transport.TransportFilter.onCommand(TransportFilter.java:69)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
	at org.apache.activemq.transport.InactivityMonitor.onCommand(InactivityMonitor.java:227)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:220)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:619)
2012-01-03 19:17:50,650 | WARN  | Store COMMIT FAILED:  | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///bar:13219
java.lang.NullPointerException
	at org.apache.activemq.util.DefaultIOExceptionHandler.handle(DefaultIOExceptionHandler.java:54)
	at org.apache.activemq.broker.BrokerService.handleIOException(BrokerService.java:2193)
	at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:804)
	at org.apache.activemq.store.kahadb.KahaDBTransactionStore.commit(KahaDBTransactionStore.java:270)
	at org.apache.activemq.transaction.LocalTransaction.commit(LocalTransaction.java:72)
	at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:173)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:103)
	at org.apache.activemq.broker.TransportConnection.processCommitTransactionOnePhase(TransportConnection.java:420)
	at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:100)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:306)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:179)
	at org.apache.activemq.transport.TransportFilter.onCommand(TransportFilter.java:69)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
	at org.apache.activemq.transport.InactivityMonitor.onCommand(InactivityMonitor.java:227)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:220)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:619)
2012-01-03 19:17:50,651 | DEBUG | rollback: TX:ID:foo-7365-1325617993757-0:1:1 syncCount: 2 | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: tcp:///bar:13219
2012-01-03 19:17:50,651 | DEBUG | Error occured while processing sync command: TransactionInfo {commandId = 229, responseRequired = true, type = 2, connectionId = ID:foo-7365-1325617993757-0:1, transactionId = TX:ID:foo-7365-1325617993757-0:1:1}, exception: javax.transaction.xa.XAException: STORE COMMIT FAILED: Transaction rolled back. | org.apache.activemq.broker.TransportConnection.Service | ActiveMQ Transport: tcp:///bar:13219
javax.transaction.xa.XAException: STORE COMMIT FAILED: Transaction rolled back.
	at org.apache.activemq.transaction.LocalTransaction.commit(LocalTransaction.java:77)
	at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:173)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:103)
	at org.apache.activemq.broker.TransportConnection.processCommitTransactionOnePhase(TransportConnection.java:420)
	at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:100)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:306)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:179)
	at org.apache.activemq.transport.TransportFilter.onCommand(TransportFilter.java:69)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
	at org.apache.activemq.transport.InactivityMonitor.onCommand(InactivityMonitor.java:227)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:220)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:202)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.activemq.util.DefaultIOExceptionHandler.handle(DefaultIOExceptionHandler.java:54)
	at org.apache.activemq.broker.BrokerService.handleIOException(BrokerService.java:2193)
	at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:804)
	at org.apache.activemq.store.kahadb.KahaDBTransactionStore.commit(KahaDBTransactionStore.java:270)
	at org.apache.activemq.transaction.LocalTransaction.commit(LocalTransaction.java:72)
	... 13 more
{noformat}",,
AMQ-4186,"with the latest dependencies we get:{code}
2012-11-22 12:20:52,678 | WARN | qtp2848306-261 | ServletHandler | lipse.jetty.util.log.JavaUtilLog 70 | 121 - org.eclipse.jetty.util - 7.6.7.v20120910 | /activemqweb/xml/topics.jsp
org.apache.jasper.JasperException: /xml/topics.jsp(1,1) PWC5988: Page directive: illegal to have multiple occurrences of contentType with different values (old: text/html;charset=UTF-8, new: text/xml;charset=ISO-8859-1)
at org.apache.jasper.compiler.DefaultErrorHandler.jspError(DefaultErrorHandler.java:78)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.ErrorDispatcher.dispatch(ErrorDispatcher.java:373)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.ErrorDispatcher.jspError(ErrorDispatcher.java:201)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.Validator$DirectiveVisitor.visit(Validator.java:171)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.Node$PageDirective.accept(Node.java:599)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.Node$Nodes.visit(Node.java:2291)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]
at org.apache.jasper.compiler.Node$Visitor.visitBody(Node.java:2341)[143:org.ops4j.pax.web.pax-web-jsp:1.1.9]{code}
The duplicate contentyType directive seems to only error out with the current jsp compiler.",,
AMQ-4369,"It is possible to get an IOException before the current default handler is installed, so it is bypassed. It needs to be set earlier.
{code}2013-03-11 09:51:24,409 | INFO  | No IOExceptionHandler registered, ignoring IO exception, java.io.IOException: Input/output error | org.apache.activemq.broker.BrokerService | ActiveMQ Transport: ssl:///xxx:yyy
java.io.IOException: Input/output error
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:355)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:414)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
	at org.apache.kahadb.page.PageFile.readPage(PageFile.java:833)
	at org.apache.kahadb.page.Transaction$2.readPage(Transaction.java:447)
	at org.apache.kahadb.page.Transaction$2.<init>(Transaction.java:438)
	at org.apache.kahadb.page.Transaction.openInputStream(Transaction.java:435)
	at org.apache.kahadb.page.Transaction.load(Transaction.java:411)
	at org.apache.kahadb.page.Transaction.load(Transaction.java:368)
	at org.apache.kahadb.index.BTreeIndex.loadNode(BTreeIndex.java:262)
	at org.apache.kahadb.index.BTreeIndex.getRoot(BTreeIndex.java:174)
	at org.apache.kahadb.index.BTreeIndex.remove(BTreeIndex.java:194)
	at org.apache.activemq.store.kahadb.MessageDatabase.updateIndex(MessageDatabase.java:1212)
	at org.apache.activemq.store.kahadb.MessageDatabase$15.execute(MessageDatabase.java:1028)
	at org.apache.kahadb.page.Transaction.execute(Transaction.java:771)
	at org.apache.activemq.store.kahadb.MessageDatabase.process(MessageDatabase.java:1026)
	at org.apache.activemq.store.kahadb.MessageDatabase$13.visit(MessageDatabase.java:961)
	at org.apache.activemq.store.kahadb.data.KahaRemoveMessageCommand.visit(KahaRemoveMessageCommand.java:220)
	at org.apache.activemq.store.kahadb.MessageDatabase.process(MessageDatabase.java:953)
	at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:865)
	at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.removeMessage(KahaDBStore.java:439)
	at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBMessageStore.removeAsyncMessage(KahaDBStore.java:411)
	at org.apache.activemq.store.kahadb.KahaDBTransactionStore.removeAsyncMessage(KahaDBTransactionStore.java:468)
	at org.apache.activemq.store.kahadb.KahaDBTransactionStore$1.removeAsyncMessage(KahaDBTransactionStore.java:171)
	at org.apache.activemq.broker.region.Queue.acknowledge(Queue.java:830)
	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1584)
	at org.apache.activemq.broker.region.QueueSubscription.acknowledge(QueueSubscription.java:59)
	at org.apache.activemq.broker.region.PrefetchSubscription.acknowledge(PrefetchSubscription.java:294)
	at org.apache.activemq.broker.region.AbstractRegion.acknowledge(AbstractRegion.java:426)
	at org.apache.activemq.broker.region.RegionBroker.acknowledge(RegionBroker.java:537)
	at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:77)
	at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:77)
	at org.apache.activemq.broker.TransactionBroker.acknowledge(TransactionBroker.java:287)
	at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:77)
	at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:77)
	at org.apache.activemq.broker.MutableBrokerFilter.acknowledge(MutableBrokerFilter.java:87)
	at org.apache.activemq.broker.TransportConnection.processMessageAck(TransportConnection.java:461)
	at org.apache.activemq.command.MessageAck.visit(MessageAck.java:236)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:292)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:150)
	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:268)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.SslTransport.doConsume(SslTransport.java:91)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:215)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:197)
	at java.lang.Thread.run(Thread.java:679){code}",,
AMQ-4411,"When the bundle is used on a platform where we don't have native libs we fail with:{code}
2013-03-27 14:19:47,241 | ERROR | Thread-6         | FeaturesServiceImpl              | s.internal.FeaturesServiceImpl$2 1143 | 20 - org.apache.karaf.features.core - 2.3.0 | Error installing boot features
java.lang.Exception: Could not start bundle mvn:org.apache.activemq/activemq-osgi/5.9-SNAPSHOT in feature(s) activemq-client-5.9-SNAPSHOT: Unresolved constraint in bundle org.apache.activemq.activemq-osgi [87]: No matching native libraries found.
        at org.apache.karaf.features.internal.FeaturesServiceImpl.installFeatures(FeaturesServiceImpl.java:476)[20:org.apache.karaf.features.core:2.3.0]
        at org.apache.karaf.features.internal.FeaturesServiceImpl$2.run(FeaturesServiceImpl.java:1141)[20:org.apache.karaf.features.core:2.3.0]
Caused by: org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.activemq.activemq-osgi [87]: No matching native libraries found.
        at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:3826)[org.apache.felix.framework-4.0.3.jar:]
        at org.apache.felix.framework.Felix.startBundle(Felix.java:1868)[org.apache.felix.framework-4.0.3.jar:]
        at org.apache.felix.framework.BundleImpl.start(BundleImpl.java:944)[org.apache.felix.framework-4.0.3.jar:]
        at org.apache.felix.framework.BundleImpl.start(BundleImpl.java:931)[org.apache.felix.framework-4.0.3.jar:]
        at org.apache.karaf.features.internal.FeaturesServiceImpl.installFeatures(FeaturesServiceImpl.java:473)[20:org.apache.karaf.features.core:2.3.0]
        ... 1 more
{code}

Adding a '*' clause to <Bundle-NativeCode> sorts this. levelDB will fallback to the java impl in cases where the jni deps are not found. But also, it allows the bundle to used with the default store where there are no native deps at all.",,
AMQ-4576,"When more than one topic is supplied to BlockingConnection.subscribe the BlockingConnection.receive fails and the following exception is thrown:
{code}
java.io.IOException: Could not connect: CONNECTION_REFUSED_SERVER_UNAVAILABLE
	at org.fusesource.mqtt.client.CallbackConnection$LoginHandler$1.onTransportCommand(CallbackConnection.java:331)
	at org.fusesource.hawtdispatch.transport.TcpTransport.drainInbound(TcpTransport.java:659)
	at org.fusesource.hawtdispatch.transport.SslTransport.drainInbound(SslTransport.java:264)
	at org.fusesource.hawtdispatch.transport.TcpTransport$6.run(TcpTransport.java:538)
	at org.fusesource.hawtdispatch.internal.NioDispatchSource$3.run(NioDispatchSource.java:209)
	at org.fusesource.hawtdispatch.internal.SerialDispatchQueue.run(SerialDispatchQueue.java:100)
	at org.fusesource.hawtdispatch.internal.pool.SimpleThread.run(SimpleThread.java:77)
{code}
On the server shows the following messages:
{code}
2013-06-06 15:06:00,125 WARN  [org.apache.activemq.transport.mqtt.MQTTProtocolConverter] (ActiveMQ BrokerService[localhost] Task-1) Exception occurred processing: 
null: javax.jms.JMSException: Durable consumer is in use for client: 6056@3232261834SOC and subscriptionName: 6056@3232261834SOC
2013-06-06 15:06:00,130 WARN  [org.apache.activemq.broker.TransportConnection] (ActiveMQ Transport: tcp:///127.0.0.1:53389@1883) Failed to add Connection ID:LTD-SFW004-53303-1370527418664-2:14, reason: javax.jms.InvalidClientIDException: Broker: localhost - Client: 6056@3232261834SOC already connected from tcp://127.0.0.1:53388
2013-06-06 15:06:00,130 WARN  [org.apache.activemq.broker.TransportConnection.Transport] (ActiveMQ Transport: tcp:///127.0.0.1:53389@1883) Transport Connection to: tcp://127.0.0.1:53389 failed: java.io.IOException: Broker: localhost - Client: 6056@3232261834SOC already connected from tcp://127.0.0.1:53388
2013-06-06 15:06:00,130 ERROR [pt.intellicare.onecare.mqtt.OneCareFuseMqttClient] (DefaultQuartzScheduler_Worker-8) Problem receiving mqtt messages: java.io.IOException: Could not connect: CONNECTION_REFUSED_SERVER_UNAVAILABLE
	at org.fusesource.mqtt.client.CallbackConnection$LoginHandler$1.onTransportCommand(CallbackConnection.java:331) [:1.5-SNAPSHOT]
	at org.fusesource.hawtdispatch.transport.TcpTransport.drainInbound(TcpTransport.java:659) [:1.17]
	at org.fusesource.hawtdispatch.transport.TcpTransport$6.run(TcpTransport.java:538) [:1.17]
	at org.fusesource.hawtdispatch.internal.NioDispatchSource$3.run(NioDispatchSource.java:209) [:1.17]
	at org.fusesource.hawtdispatch.internal.SerialDispatchQueue.run(SerialDispatchQueue.java:100) [:1.17]
	at org.fusesource.hawtdispatch.internal.pool.SimpleThread.run(SimpleThread.java:77) [:1.17]
{code}
Code example:
{code}
MQTT = new MQTT();
mqtt.setHost(url);
mqtt.setClientId(clientId);
mqtt.setUserName(user);
mqtt.setPassword(password);
mqtt.setCleanSession(false);

BlockingConnection connection = mqtt.blockingConnection();
connection.connect();
Topic[] topics = {new Topic(""TopicA"", QoS.EXACTLY_ONCE), new Topic(""TopicB"", QoS.EXACTLY_ONCE)};
byte[] qoses = connection.subscribe(topics);
while (true) {
    Message message = connection.receive();
    byte[] payload = message.getPayload();
    String messageContent = new String(payload);
    System.out.println(""Received message from topic: "" + message.getTopic() + "" Message content: "" + messageContent);
    message.ack();
}
{code}
The test failed when using the current fusesource client (1.5) on ActiveMQ 5.9, on Mosquitto mqtt the code works correctly.
",Surprise,-1
AMQ-5141,"If the broker handles a RemoveInfo command it may also kick off a message expiry check for (I presume) any prefetched messages. If messages are to be expired they get sent to ActiveMQ.DLQ by default. See stack trace in next comment.

If the broker is security enabled with authorization turned on and messages get sent to DLQ as a result of the expiry check then the broker uses the client's security context when sending the messages to DLQ. 
This implies the client user needs to have write access to ActiveMQ.DLQ. 

As this may happen with any other client, all client users will require write access to ActiveMQ.DLQ, which may not be appropriate from a security point of view. 

The broker regularly runs an expiry check and uses a broker internal security context for this task. In my opinion this same broker internal security context should be used when expiring messages as part of the RemoveInfo command. The broker should not use the client's security context. 

[1]
The current behavior can raise the following SecurityException if the client user does not have write access to ActiveMQ.DLQ

{code}
2014-04-11 08:11:22,229 | WARN  | 2.38:61201@61616 | RegionBroker | ivemq.broker.region.RegionBroker  703 | 
105 - org.apache.activemq.activemq-osgi - 5.8.0.redhat-60024 | Caught an exception sending to DLQ: Message 
ID:S930A3085-50865-635327964441522304-1:1:363:2:1 dropped=false acked=false locked=true
java.lang.SecurityException: User Test is not authorized to write to: queue://ActiveMQ.DLQ
	at org.apache.activemq.security.AuthorizationBroker.send(AuthorizationBroker.java:197)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.send(MutableBrokerFilter.java:135)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.util.BrokerSupport.doResend(BrokerSupport.java:68)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.util.BrokerSupport.resendNoCopy(BrokerSupport.java:38)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.RegionBroker.sendToDeadLetterQueue(RegionBroker.java:691)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.advisory.AdvisoryBroker.sendToDeadLetterQueue(AdvisoryBroker.java:413)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.sendToDeadLetterQueue(MutableBrokerFilter.java:274)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.util.RedeliveryPlugin.sendToDeadLetterQueue(RedeliveryPlugin.java:132)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.sendToDeadLetterQueue(BrokerFilter.java:262)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.sendToDeadLetterQueue(MutableBrokerFilter.java:274)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.RegionBroker.messageExpired(RegionBroker.java:659)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.advisory.AdvisoryBroker.messageExpired(AdvisoryBroker.java:283)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.messageExpired(MutableBrokerFilter.java:269)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.messageExpired(BrokerFilter.java:257)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.messageExpired(MutableBrokerFilter.java:269)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.Queue.messageExpired(Queue.java:1671)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.PrefetchSubscription.dispatchPending(PrefetchSubscription.java:648)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.PrefetchSubscription.add(PrefetchSubscription.java:162)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.Queue.doActualDispatch(Queue.java:1907)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.Queue.doDispatch(Queue.java:1834)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.Queue.removeSubscription(Queue.java:576)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.AbstractRegion.removeConsumer(AbstractRegion.java:380)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.region.RegionBroker.removeConsumer(RegionBroker.java:364)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.jmx.ManagedRegionBroker.removeConsumer(ManagedRegionBroker.java:247)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.advisory.AdvisoryBroker.removeConsumer(AdvisoryBroker.java:253)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.removeConsumer(MutableBrokerFilter.java:123)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.BrokerFilter.removeConsumer(BrokerFilter.java:117)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.MutableBrokerFilter.removeConsumer(MutableBrokerFilter.java:123)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.TransportConnection.processRemoveConsumer(TransportConnection.java:651)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.command.RemoveInfo.visit(RemoveInfo.java:76)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:329)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:184)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:288)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196)[105:org.apache.activemq.activemq-osgi:5.8.0.redhat-60024]
	at java.lang.Thread.run(Unknown Source)[:1.6.0_26]
{code}
 
","Fear, Sadness",-1
AMQ-5300,"While searching for a workaround for issue AMQ-5284, I came across this issue.

To work around the serialization issue (AMQ-5284), I deleted the index snapshots from the LevelDB datastore. This will replay the logs to regenerate the index. However, if a log rotation has already occurred, you will get an infinite loop upon restart.

Here are the steps to reproduce what I am seeing:

Configure ActiveMQ 5.10.0 to use a LevelDB data store with the log size of about 1MB.
{code}
<persistenceAdapter>
    <levelDB directory=""${activemq.data}/leveldb"" logSize=""1000000"" />
</persistenceAdapter>
{code}

Then I started up the broker and published 10,000 persistent messages to a queue, causing the log files to rotate (twice in my case). I see the following files in the data store folder:
{code}
-rw-rw-r--. 1 user users 1000071 Jul 30 11:15 0000000000000000.log
-rw-rw-r--. 1 user users 1000009 Jul 30 11:16 00000000000f4287.log
drwxrwxr-x. 2 user users    4096 Jul 30 11:16 00000000001e84d0.index
-rw-rw-r--. 1 user users 1000000 Jul 30 11:17 00000000001e84d0.log
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 dirty.index
-rw-rw-r--. 1 user users       0 Jul 30 11:11 lock
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 plist.index
-rw-rw-r--. 1 user users      24 Jul 30 11:11 store-version.txt
{code}

I then consume 5,000 messages, which causes the first log to be deleted since it is no longer being referenced. I see the following log statements:
{code}
2014-07-30 11:29:14,960 | DEBUG | Log no longer referenced: 0 | org.apache.activemq.leveldb.LevelDBClient | Thread-2
2014-07-30 11:29:14,967 | DEBUG | Deleting log at 0 | org.apache.activemq.leveldb.LevelDBClient | Thread-2
{code}

And I see the remaining files in the data store folder (notice the 0000000000000000.log is gone):
{code}
-rw-rw-r--. 1 user users 1000009 Jul 30 11:16 00000000000f4287.log
-rw-rw-r--. 1 user users 1000011 Jul 30 11:29 00000000001e84d0.log
drwxrwxr-x. 2 user users    4096 Jul 30 11:29 00000000002dc71b.index
-rw-rw-r--. 1 user users 1000000 Jul 30 11:29 00000000002dc71b.log
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 dirty.index
-rw-rw-r--. 1 user users       0 Jul 30 11:11 lock
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 plist.index
-rw-rw-r--. 1 user users      24 Jul 30 11:11 store-version.txt
{code}

At this point, I shut down the broker and here is the listing of what's left in the data store:
{code}
-rw-rw-r--. 1 user users 1000009 Jul 30 11:16 00000000000f4287.log
-rw-rw-r--. 1 user users 1000011 Jul 30 11:29 00000000001e84d0.log
-rw-rw-r--. 1 user users 1000000 Jul 30 11:29 00000000002dc71b.log
drwxrwxr-x. 2 user users    4096 Jul 30 11:36 0000000000301737.index
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 dirty.index
drwxrwxr-x. 2 user users    4096 Jul 30 11:11 plist.index
-rw-rw-r--. 1 user users      24 Jul 30 11:11 store-version.txt
{code}

I then delete the index folder within the data store (in my case ""0000000000301737.index""). I am doing this to force a replay of the logs to regenerate the index (due to the serialization issue I ran into).

And finally, this is the message I am getting once I start the broker back up (infinite loop of this same message, and I have to shut down the broker):

{code}
2014-07-30 11:40:27,415 | WARN  | No reader available for position: 0, log_infos: {1000071=LogInfo(/home/user/apache-activemq-5.10.0/data/leveldb/00000000000f4287.log,1000071,1000009), 2000080=LogInfo(/home/user/apache-activemq-5.10.0/data/leveldb/00000000001e84d0.log,2000080,1000011), 3000091=LogInfo(/home/user/apache-activemq-5.10.0/data/leveldb/00000000002dc71b.log,3000091,0)} | org.apache.activemq.leveldb.RecordLog | main
{code}",Surprise,-1
AMQ-5384,"AMQ 5.9 gets stuck under 30-50 req/second load when using JDBC persistence - this affects our application as it hangs during performance testing (this happens almost every night).
Following stacktraces indicate that there's a deadlock on DB connection:

Stack logged by C3P0, showing when first DB connection has been picked from the pool:
{noformat}
2014-10-06 08:44:40,646 | INFO  | Logging the stack trace by which the overdue resource was checked-out. | com.mchange.v2.resourcepool.BasicResourcePool | C3P0PooledConnectionPoolManager[identityToken->2x1e6s941raztn6mju110
java.lang.Exception: DEBUG STACK TRACE: Overdue resource check-out stack trace.
        at com.mchange.v2.resourcepool.BasicResourcePool.checkoutResource(BasicResourcePool.java:555)
        at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool.checkoutAndMarkConnectionInUse(C3P0PooledConnectionPool.java:756)
        at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool.checkoutPooledConnection(C3P0PooledConnectionPool.java:683)
        at com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource.getConnection(AbstractPoolBackedDataSource.java:140)
        at org.apache.activemq.store.jdbc.TransactionContext.getConnection(TransactionContext.java:58)
        at org.apache.activemq.store.jdbc.TransactionContext.begin(TransactionContext.java:163)
        at org.apache.activemq.store.jdbc.JDBCPersistenceAdapter.beginTransaction(JDBCPersistenceAdapter.java:510)
        at org.apache.activemq.store.memory.MemoryTransactionStore$Tx.commit(MemoryTransactionStore.java:92)
        at org.apache.activemq.store.memory.MemoryTransactionStore.commit(MemoryTransactionStore.java:259)
        at org.apache.activemq.transaction.XATransaction.storeCommit(XATransaction.java:85)
        at org.apache.activemq.transaction.XATransaction.commit(XATransaction.java:75)
        at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:253)
        at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:112)
        at org.apache.activemq.broker.TransportConnection.processCommitTransactionTwoPhase(TransportConnection.java:433)
        at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:102)
        at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:292)
        at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:149)
        at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)
        at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
        at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:270)
        at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
        at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214)
        at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Following stack shows the same thread pending for second DB connection (without releasing the first one):
{noformat}
""ActiveMQ Transport: tcp:///10.132.7.20:36431@5445"" daemon prio=10 tid=0x000000004119d000 nid=0x61bf in Object.wait() [0x00007f41120d7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000fabdd2c0> (a com.mchange.v2.resourcepool.BasicResourcePool)
	at com.mchange.v2.resourcepool.BasicResourcePool.awaitAvailable(BasicResourcePool.java:1414)
	at com.mchange.v2.resourcepool.BasicResourcePool.prelimCheckoutResource(BasicResourcePool.java:606)
	- locked <0x00000000fabdd2c0> (a com.mchange.v2.resourcepool.BasicResourcePool)
	at com.mchange.v2.resourcepool.BasicResourcePool.checkoutResource(BasicResourcePool.java:526)
	at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool.checkoutAndMarkConnectionInUse(C3P0PooledConnectionPool.java:756)
	at com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool.checkoutPooledConnection(C3P0PooledConnectionPool.java:683)
	at com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource.getConnection(AbstractPoolBackedDataSource.java:140)
	at org.apache.activemq.store.jdbc.TransactionContext.getConnection(TransactionContext.java:58)
	at org.apache.activemq.store.jdbc.adapter.DefaultJDBCAdapter.getStoreSequenceId(DefaultJDBCAdapter.java:290)
	at org.apache.activemq.store.jdbc.JDBCPersistenceAdapter.getStoreSequenceIdForMessageId(JDBCPersistenceAdapter.java:840)
	at org.apache.activemq.store.jdbc.JDBCMessageStore.removeMessage(JDBCMessageStore.java:194)
	at org.apache.activemq.store.memory.MemoryTransactionStore$4.run(MemoryTransactionStore.java:348)
	at org.apache.activemq.store.memory.MemoryTransactionStore$Tx.commit(MemoryTransactionStore.java:103)
	at org.apache.activemq.store.memory.MemoryTransactionStore.commit(MemoryTransactionStore.java:259)
	at org.apache.activemq.transaction.XATransaction.storeCommit(XATransaction.java:85)
	at org.apache.activemq.transaction.XATransaction.commit(XATransaction.java:69)
	at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:253)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:112)
	at org.apache.activemq.broker.TransportConnection.processCommitTransactionOnePhase(TransportConnection.java:424)
	at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:100)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:292)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:149)
	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113)
	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:270)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214)
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196)
	at java.lang.Thread.run(Thread.java:662) 
{noformat}

Problem seems to be related with JDBCMessageStore.removeMessage method:
{code:java}
    public void removeMessage(ConnectionContext context, MessageAck ack) throws IOException {
    	long seq = persistenceAdapter.getStoreSequenceIdForMessageId(ack.getLastMessageId(), destination)[0];
...
{code}

Call to {{removeMessage}} already has one DB connection passed in {{context}} method parameter, but calling {{persistenceAdapter.getStoreSequenceIdForMessageId}} creates another DB connection in the same transaction.

Deadlock occurs when all DB connections are used by {{context}}, so that  {{removeMessage}} can't fetch its own connection.

Possible solution would be to pass {{ConnectionContext}} object to {{persistenceAdapter.getStoreSequenceIdForMessageId}} method, so that the method would reuse same connection.","Anger, Sadness, Surprise",-1
AMQ-5525,"failures:
{code}  ActiveMQAMQPBrokerFeatureTest>ActiveMQBrokerFeatureTest.test:70->AbstractFeatureTest.withinReason:259 expected:<[JMS_BODY_FIELD:JMSText = 1421674632296]> but was:<[]>
  ActiveMQBrokerFeatureTest.test:70->AbstractFeatureTest.withinReason:259 expected:<[JMS_BODY_FIELD:JMSText = 1421674680492]> but was:<[]>{code} root cause - somewhere in blueprint converter. 
{code}java.lang.Exception: Unable to convert from [--amqurl, tcp://localhost:61616, --user, karaf, --password, karaf, -Vbody, 1421674680492] to java.util.ArrayList<java.lang.String>(error converting collection entry)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:342)
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:182)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.convert(ActiveMQCommand.java:143)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.prepare(ActiveMQCommand.java:134)
	at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.aries.proxy.impl.ProxyHandler$1.invoke(ProxyHandler.java:54)
	at org.apache.aries.proxy.impl.ProxyHandler.invoke(ProxyHandler.java:119)
	at org.apache.activemq.karaf.commands.$ActiveMQCommand695030723.execute(Unknown Source)
	at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)
	at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:477)
	at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:403)
	at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)
	at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:92)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:125)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:117)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:117)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:109)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Unable to convert value 1421674680492 to type java.lang.String
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:184)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:340)
	... 30 more
2015-01-19 13:38:05,367 | ERROR | Thread-30        | AbstractFeatureTest              | 125 - PAXEXAM-PROBE-a407b767-4b95-4f93-8124-64d04dc65582 - 0.0.0 | Execute: activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492 - Response:
activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492
java.lang.Exception: Unable to convert from [--amqurl, tcp://localhost:61616, --user, karaf, --password, karaf, -Vbody, 1421674680492] to java.util.ArrayList<java.lang.String>(error converting collection entry)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:342)
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:182)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.convert(ActiveMQCommand.java:143)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.prepare(ActiveMQCommand.java:134)
	at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.aries.proxy.impl.ProxyHandler$1.invoke(ProxyHandler.java:54)
	at org.apache.aries.proxy.impl.ProxyHandler.invoke(ProxyHandler.java:119)
	at org.apache.activemq.karaf.commands.$ActiveMQCommand695030723.execute(Unknown Source)
	at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)
	at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:477)
	at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:403)
	at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)
	at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:92)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:125)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:117)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:117)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:109)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Unable to convert value 1421674680492 to type java.lang.String
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:184)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:340)
	... 30 more
2015-01-19 13:38:06,379 | ERROR | Thread-30        | AbstractFeatureTest              | 125 - PAXEXAM-PROBE-a407b767-4b95-4f93-8124-64d04dc65582 - 0.0.0 | Execute: activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492 - Response:
activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492
java.lang.Exception: Unable to convert from [--amqurl, tcp://localhost:61616, --user, karaf, --password, karaf, -Vbody, 1421674680492] to java.util.ArrayList<java.lang.String>(error converting collection entry)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:342)
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:182)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.convert(ActiveMQCommand.java:143)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.prepare(ActiveMQCommand.java:134)
	at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.aries.proxy.impl.ProxyHandler$1.invoke(ProxyHandler.java:54)
	at org.apache.aries.proxy.impl.ProxyHandler.invoke(ProxyHandler.java:119)
	at org.apache.activemq.karaf.commands.$ActiveMQCommand695030723.execute(Unknown Source)
	at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)
	at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:477)
	at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:403)
	at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)
	at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:92)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:125)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:117)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:117)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:109)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Unable to convert value 1421674680492 to type java.lang.String
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:184)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:340)
	... 30 more
2015-01-19 13:38:07,390 | ERROR | Thread-30        | AbstractFeatureTest              | 125 - PAXEXAM-PROBE-a407b767-4b95-4f93-8124-64d04dc65582 - 0.0.0 | Execute: activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492 - Response:
activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492
java.lang.Exception: Unable to convert from [--amqurl, tcp://localhost:61616, --user, karaf, --password, karaf, -Vbody, 1421674680492] to java.util.ArrayList<java.lang.String>(error converting collection entry)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:342)
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:182)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.convert(ActiveMQCommand.java:143)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.prepare(ActiveMQCommand.java:134)
	at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.aries.proxy.impl.ProxyHandler$1.invoke(ProxyHandler.java:54)
	at org.apache.aries.proxy.impl.ProxyHandler.invoke(ProxyHandler.java:119)
	at org.apache.activemq.karaf.commands.$ActiveMQCommand695030723.execute(Unknown Source)
	at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)
	at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:477)
	at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:403)
	at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)
	at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:92)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:125)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:117)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:117)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:109)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Unable to convert value 1421674680492 to type java.lang.String
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:184)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:340)
	... 30 more
2015-01-19 13:38:08,400 | ERROR | Thread-30        | AbstractFeatureTest              | 125 - PAXEXAM-PROBE-a407b767-4b95-4f93-8124-64d04dc65582 - 0.0.0 | Execute: activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492 - Response:
activemq:browse --amqurl tcp://localhost:61616 --user karaf --password karaf -Vbody 1421674680492
java.lang.Exception: Unable to convert from [--amqurl, tcp://localhost:61616, --user, karaf, --password, karaf, -Vbody, 1421674680492] to java.util.ArrayList<java.lang.String>(error converting collection entry)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:342)
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:182)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.convert(ActiveMQCommand.java:143)
	at org.apache.activemq.karaf.commands.ActiveMQCommand$ActiveMQActionPreparator.prepare(ActiveMQCommand.java:134)
	at org.apache.felix.gogo.commands.basic.AbstractCommand.execute(AbstractCommand.java:34)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.aries.proxy.impl.ProxyHandler$1.invoke(ProxyHandler.java:54)
	at org.apache.aries.proxy.impl.ProxyHandler.invoke(ProxyHandler.java:119)
	at org.apache.activemq.karaf.commands.$ActiveMQCommand695030723.execute(Unknown Source)
	at org.apache.felix.gogo.runtime.CommandProxy.execute(CommandProxy.java:78)
	at org.apache.felix.gogo.runtime.Closure.executeCmd(Closure.java:477)
	at org.apache.felix.gogo.runtime.Closure.executeStatement(Closure.java:403)
	at org.apache.felix.gogo.runtime.Pipe.run(Pipe.java:108)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:183)
	at org.apache.felix.gogo.runtime.Closure.execute(Closure.java:120)
	at org.apache.felix.gogo.runtime.CommandSessionImpl.execute(CommandSessionImpl.java:92)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:125)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1$1.run(AbstractFeatureTest.java:117)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:117)
	at org.apache.activemq.karaf.itest.AbstractFeatureTest$1.call(AbstractFeatureTest.java:109)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: Unable to convert value 1421674680492 to type java.lang.String
	at org.apache.aries.blueprint.container.AggregateConverter.convert(AggregateConverter.java:184)
	at org.apache.aries.blueprint.container.AggregateConverter.convertToCollection(AggregateConverter.java:340){code}",,
AMQ-5851,"When lot of messages got expired because of JMS client Time to Live (TTL) property then below error will appear and consumer will freeze

{code:xml}
 Connection to broker failed: Unmatched acknowledge: MessageAck {commandId = 77, responseRequired = false, ackType = 2, consumerId =XXX firstMessageId = ID:XXX
 lastMessageId = ID:XXX
, destination = queue://abc, transactionId = null, messageCount = 1, poisonCause = null}; Could not find Message-ID in dispatched-list (start of ack)
         at org.apache.activemq.broker.region.PrefetchSubscription.assertAckMatchesDispatched(PrefetchSubscription.java:477) [activemq-broker-5.11.1.jar:5.11.1
 at org.apache.activemq.broker.region.PrefetchSubscription.acknowledge(PrefetchSubscription.java:212) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.region.AbstractRegion.acknowledge(AbstractRegion.java:441) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.region.RegionBroker.acknowledge(RegionBroker.java:484) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:87) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.BrokerFilter.acknowledge(BrokerFilter.java:87) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.TransactionBroker.acknowledge(TransactionBroker.java:277) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.MutableBrokerFilter.acknowledge(MutableBrokerFilter.java:97) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.TransportConnection.processMessageAck(TransportConnection.java:550) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.command.MessageAck.visit(MessageAck.java:245) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:334) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:188) [activemq-broker-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:113) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:270) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214) [activemq-client-5.11.1.jar:5.11.1]
 at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196) [activemq-client-5.11.1.jar:5.11.1]
 at java.lang.Thread.run(Thread.java:745) [rt.jar:1.8.0_25]
{code}


Steps to reproduce :
1. Enable TTL property for JMS client
2. Keep TTL value very low say 5 sec
3. Send lot of messages so some message will get expired
4. Make sure that some message should expired when they are in MDB means running inside MDB


Then we will see above error in the logs",,
AMQ-5854,"Use case :
                With Spring DMLC, Read a jms message in a queue, produce a jms message in an output queue and write data in database.

Problem description :

                Due to hight CPU usage, the inactity monitor closes connections between clients and broker while 16 messages were processed.

{noformat}
2015-06-01 04:39:01,130 | WARN  | Transport Connection to: tcp://*** failed: org.apache.activemq.transport.InactivityIOException: Channel was inactive for too (>30000) long: tcp://*** | org.apache.activemq.broker.TransportConnection.Transport | ActiveMQ InactivityMonitor Worker
{noformat}
                15 messages are rolled back and redilevered to another consummer.

                In the log we got 15 warnings :
{noformat}
ActiveMQMessageConsumer   |WARN |jmsContainer-173|rolling back transaction (XID:***) post failover recovery. 1 previously delivered message(s) not replayed to consumer: ***
{noformat}
                But one message is not rolled back (the transaction commit) and is also redileverd to another consummer. So it's processed twice by two different consummers (two inserts in database and two output JMS messages generated) and is not deduplicated.

                In the activeMq log we got the message :

{noformat}
WARN  | Async error occurred:  | org.apache.activemq.broker.TransportConnection.Service | ActiveMQ Transport: tcp:///***
                       javax.jms.JMSException: Unmatched acknowledge: MessageAck {commandId = 6665, responseRequired = false, ackType = 2, consumerId = ID:***, firstMessageId = ID:***-50800-1433109620591-1:2:31356:1:1, lastMessageId = ID:***-50800-1433109620591-1:2:31356:1:1, destination = queue://***, transactionId = XID:[1096044365,globalId=47524f55505f3030303038736572766963657472616974656d656e7431363536373030343133,branchId=47524f55505f3030303038736572766963657472616974656d656e743137343737], messageCount = 1, poisonCause = null}; Could not find Message-ID ID:***-50800-1433109620591-1:2:31356:1:1 in dispatched-list (start of ack)
{noformat}

                For this duplicated message, the failover occur during prepare phase of commit :

{noformat}
[{2015/06/01 04:39:50,322 |FailoverTransport         |WARN |jmsContainer-152|Transport (tcp://***) failed, reason:  , attempting to automatically reconnect}]
org.apache.activemq.transport.InactivityIOException: Cannot send, channel has already failed: ***
                at org.apache.activemq.transport.AbstractInactivityMonitor.doOnewaySend(AbstractInactivityMonitor.java:297)
                at org.apache.activemq.transport.AbstractInactivityMonitor.oneway(AbstractInactivityMonitor.java:286)
                at org.apache.activemq.transport.TransportFilter.oneway(TransportFilter.java:85)
                at org.apache.activemq.transport.WireFormatNegotiator.oneway(WireFormatNegotiator.java:104)
                at org.apache.activemq.transport.failover.FailoverTransport.oneway(FailoverTransport.java:658)
                at org.apache.activemq.transport.MutexTransport.oneway(MutexTransport.java:68)
                at org.apache.activemq.transport.ResponseCorrelator.oneway(ResponseCorrelator.java:60)
                at org.apache.activemq.ActiveMQConnection.doAsyncSendPacket(ActiveMQConnection.java:1321)
                at org.apache.activemq.ActiveMQConnection.asyncSendPacket(ActiveMQConnection.java:1315)
                at org.apache.activemq.ActiveMQSession.asyncSendPacket(ActiveMQSession.java:1933)
                at org.apache.activemq.ActiveMQSession.sendAck(ActiveMQSession.java:2099)
                at org.apache.activemq.ActiveMQSession.sendAck(ActiveMQSession.java:2094)
                at org.apache.activemq.ActiveMQMessageConsumer.acknowledge(ActiveMQMessageConsumer.java:1083)
                at org.apache.activemq.ActiveMQMessageConsumer$5.beforeEnd(ActiveMQMessageConsumer.java:1041)
                at org.apache.activemq.TransactionContext.beforeEnd(TransactionContext.java:202)
                at org.apache.activemq.TransactionContext.end(TransactionContext.java:409)
                at com.atomikos.datasource.xa.XAResourceTransaction.suspend(XAResourceTransaction.java:457)
                at com.atomikos.datasource.xa.XAResourceTransaction.prepare(XAResourceTransaction.java:608)
                at com.atomikos.icatch.imp.PrepareMessage.send(PrepareMessage.java:61)
                at com.atomikos.icatch.imp.PropagationMessage.submit(PropagationMessage.java:111)
                at com.atomikos.icatch.imp.Propagator$PropagatorThread.run(Propagator.java:87)
                at com.atomikos.icatch.imp.Propagator.submitPropagationMessage(Propagator.java:66)
                at com.atomikos.icatch.imp.ActiveStateHandler.prepare(ActiveStateHandler.java:173)
                at com.atomikos.icatch.imp.CoordinatorImp.prepare(CoordinatorImp.java:832)
                at com.atomikos.icatch.imp.CoordinatorImp.terminate(CoordinatorImp.java:1159)
                at com.atomikos.icatch.imp.CompositeTerminatorImp.commit(CompositeTerminatorImp.java:92)
                at com.atomikos.icatch.jta.TransactionImp.commit(TransactionImp.java:236)
                at com.atomikos.icatch.jta.TransactionManagerImp.commit(TransactionManagerImp.java:498)
                at com.atomikos.icatch.jta.UserTransactionImp.commit(UserTransactionImp.java:129)
                at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1011)
                at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:755)
                at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:724)
                at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:257)
                at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1101)
                at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:995)
                at java.lang.Thread.run(Thread.java:761)
{noformat}

Our analysis :

                We think that the duplicate message is caused by the failover during the prepare phase of the commit so we modify the source code to reproduce the case.

                Our modifications in config to produce failovers:
                               broker : transport.useKeepAlive=false
                               client : wireFormat.maxInactivityDuration=5000


                We add Thread.sleep in the source code of org.apache.activemq.ActiveMQMessageConsumer to force failover to be done exactly where we think it causes problems :

{code:title=org.apache.activemq.ActiveMQMessageConsumer#acknowledge()|borderStyle=solid}                

                    public void acknowledge() throws JMSException {
                               clearDeliveredList();
                               waitForRedeliveries();
                               synchronized(deliveredMessages) {

                                   // BEGIN MODIFIED CODE
                                   LOG.warn(""start sleeping 20 seconds to test failover"");
                                   try{
                                       Thread.sleep(1000 * 20 );
                                   }catch (InterruptedException e){
                                       LOG.error(""Exception :"",e);
                                   }
                                   LOG.warn(""end sleeping 20 seconds to test failover"");
                                   // END MODIFIED CODE

                                   // Acknowledge all messages so far.
                                   MessageAck ack = makeAckForAllDeliveredMessages(MessageAck.STANDARD_ACK_TYPE);
                                   if (ack == null)
                                       return; // no msgs

                                   if (session.getTransacted()) {
                                       rollbackOnFailedRecoveryRedelivery();
                                       session.doStartTransaction();
                                       ack.setTransactionId(session.getTransactionContext().getTransactionId());
                                   }

                                   pendingAck = null;
                                    session.sendAck(ack);

                                   // Adjust the counters
                                   deliveredCounter = Math.max(0, deliveredCounter - deliveredMessages.size());
                                   additionalWindowSize = Math.max(0, additionalWindowSize - deliveredMessages.size());

                                   if (!session.getTransacted()) {
                                       deliveredMessages.clear();
                                   }
                               }
                    }
{code}                               
                
                With these changes on the configuration and the code, the problem is easily reproduced.

                We also try with transactedIndividualAck=true, and we add a Thread.sleep in the code :

{code:title=org.apache.activemq.ActiveMQMessageConsumer#registerSync()|borderStyle=solid}                
                    private void registerSync() throws JMSException {
                               session.doStartTransaction();
                               if (!synchronizationRegistered) {
                                   synchronizationRegistered = true;
                                   session.getTransactionContext().addSynchronization(new Synchronization() {
                                       @Override
                                       public void beforeEnd() throws Exception {
                                           if (transactedIndividualAck) {
                                               clearDeliveredList();
                                               waitForRedeliveries();
                                               synchronized(deliveredMessages) {
                                                   
                                                   // BEGIN MODIFIED CODE
                                                   LOG.warn(""start sleeping 20 seconds to test failover"");
                                                   try{
                                                       Thread.sleep(1000 * 20 );
                                                   }catch (InterruptedException e){
                                                       LOG.error(""Exception :"",e);
                                                   }
                                                   LOG.warn(""end sleeping 20 seconds to test failover"");
                                                   // END MODIFIED CODE                            

                                                   rollbackOnFailedRecoveryRedelivery();
                                               }
                                           } else {
                                               acknowledge();
                                           }
                                           synchronizationRegistered = false;
                                       }

                                       @Override
                                       public void afterCommit() throws Exception {
                                           commit();
                                           synchronizationRegistered = false;
                                       }

                                       @Override
                                       public void afterRollback() throws Exception {
                                           rollback();
                                           synchronizationRegistered = false;
                                       }
                                   });
                               }
                    }
{code}                                               
                With these modifications, we still get duplicates messages.

                We think that the problem is that the statement synchronized(deliveredMessages) prevents the call of clearDeliveredList() by another ActiveMQConnection thread that clears messages in progress.
                By adding logs we observe that a thread is waiting deliveredMessages s lock in clearDeliveredList() method.

                
Question :
                
                We tried fixes described in https://issues.apache.org/jira/browse/AMQ-5068 and https://issues.apache.org/jira/browse/AMQ-3519 but it doesnt help to solve our problem.
                Is there a workaround or a config parameter that can help to prevent this problem ?
                
                We are working on our side to find a correction. An option may be to force rolling back transaction if there is a failover during the prepare phase of commit in ConnectionStateTracker.restoreTransactions().


","Sadness, Surprise",-1
AMQ-6152,"Something is holding onto KahaDB scheduler log files. We have reports of up to 400GB of scheduler log files. I have tried to isolate the issue and create a minimal example (attached). In the troubleshooting I have done, the scheduler GC process is running, it's just deciding not to GC files that it should be. I have also found behavior inconsistent on the log files it does remove. 

The ran the attached example/test on 5.10.0, 5.11.1, 5.12.0 and 5.13.0. The test schedules 20 messages that are large enough to cause 4 log files to be created. It then consumes all 20 messages. When on 5.10.0, it behaves like I would expect, files 1-3 are GC'd and the 4th (the current log file) is left. On all other versions I've tried it always leaves the first 2 files, and sometimes will GC the 3rd.

Below is a snippet from the log of the scheduler process and why it's deciding not to GC these files:
{noformat}
2016-02-01 16:32:33,327 DEBUG [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] Job Scheduler Store Checkpoint started.
2016-02-01 16:32:33,338 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] Last update: 4:29447367, full gc candidates set: [1, 2, 3, 4]
2016-02-01 16:32:33,338 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] gc candidates after reference check: [1, 2, 3]
2016-02-01 16:32:33,338 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] A remove in log 1 has an add still in existance.
2016-02-01 16:32:33,339 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] A remove in log 2 has an add still in existance.
2016-02-01 16:32:33,339 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] A remove in log 3 has an add still in existance.
2016-02-01 16:32:33,339 TRACE [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] gc candidates after removals check: []
2016-02-01 16:32:33,339 DEBUG [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] Job Scheduler Store Checkpoint complete.
2016-02-01 16:32:33,727 INFO  [main] [o.a.a.s.k.s.JobSchedulerStoreImpl] JobSchedulerStore: test-mq/localhost/scheduler stopped.
{noformat}

This issue was originally reported in the Open Source PuppetDB project, ticket [here|https://tickets.puppetlabs.com/browse/PDB-1411].
",Surprise,-1
AMQ-6262,"A regression from https://issues.apache.org/jira/browse/AMQ-5794 .
Connection watchdog is started for every initiated connection and stopped on WireFormatInfo command. HTTP transport doesn't send WireFormatInfo so the watchdog never realises that the connection has been successfully established.

The connection gets terminated every 30seconds by the watchdog.
At the beginning, everything looks fine, but then you start getting exceptions and start losing packets. I haven't seen that myself, but I had people reporting that if HTTP transports are in use, it eventually destabilises the broker and affects non-HTTP transports too.

{noformat}
2016-04-22 10:32:46.029244500 2016-04-22 10:32:46,029 WARN [ActiveMQ InactivityMonitor Worker] [Transport] Transport Connection to: blockingQueue_28120594 failed: org.apache.activemq.transport.InactivityIOException: Channel was inactive
2016-04-22 10:33:30.988313500 org.apache.activemq.transport.InactivityIOException: Channel was inactive (no connection attempt made) for too (>30000) long: blockingQueue_21644517
2016-04-22 10:33:30.988637500   at org.apache.activemq.transport.AbstractInactivityMonitor$1$1.run(AbstractInactivityMonitor.java:91)
2016-04-22 10:33:30.988667500   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2016-04-22 10:33:30.988689500   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2016-04-22 10:33:30.988717500   at java.lang.Thread.run(Thread.java:745)
{noformat}","Sadness, Surprise",-1
AMQ-6343,"I use several services, some of them connect over tcp and the LWT works properly.
I use several clients from a webapp' connected over a websocket link.

i use a javascript code:
{code:javascript}
var i_am_disconnected = new Paho.MQTT.Message(JSON.stringify({
        version: 1,
        state: 0
      }));
      i_am_disconnected.destinationName = ""device/"" + _token + ""/ping"";
      i_am_disconnected.retained = true;
      i_am_disconnected.qos = 1;
client.onConnectionLost = onConnectionLost;
      client.connect(
{code}

i traced the ActiveMQ log, i can see that the disconnection is detected, but i didn't receive my LWT message. (again, my LWT works when I use a tcp connexion)

{code}
2016-07-01 00:07:51,329 | INFO  | addConnection() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 info: ConnectionInfo {commandId = 0, responseRequired = true, connectionId = ID:deverylight-all-snapshot-42229-1467323807294-3:27, clientId = 60badfb1-3e6d-4637-b4ec-16b557d7db64, clientIp = ws://192.168.0.2:42372, userName = bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64, password = *****, brokerPath = null, brokerMasterConnector = false, manageable = false, clientMaster = true, faultTolerant = false, failoverReconnect = false} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-40
2016-07-01 00:07:51,333 | INFO  | addProducer() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 info: ProducerInfo {commandId = 2, responseRequired = true, producerId = ID:deverylight-all-snapshot-42229-1467323807294-3:27:-1:1, destination = null, brokerPath = null, dispatchAsync = false, windowSize = 0, sentCount = 0} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-40
2016-07-01 00:07:51,423 | INFO  | send() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 send: ActiveMQBytesMessage {commandId = 3, responseRequired = true, messageId = ID:deverylight-all-snapshot-42229-1467323807294-3:27:-1:1:1, originalDestination = null, originalTransactionId = null, producerId = ID:deverylight-all-snapshot-42229-1467323807294-3:27:-1:1, destination = topic://device.bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64.ping, transactionId = null, expiration = 0, timestamp = 1467324471422, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = null, replyTo = null, persistent = false, type = null, priority = 4, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = org.apache.activemq.util.ByteSequence@335bf90b, marshalledProperties = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = {ActiveMQ.MQTT.QoS=1, ActiveMQ.Retain=true}, readOnlyProperties = true, readOnlyBody = true, droppable = false, jmsXGroupFirstForConsumer = false} ActiveMQBytesMessage{ bytesOut = null, dataOut = null, dataIn = null } payload: {""version"":1,""state"":2,""timestamp"":1467324470.015} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-41
2016-07-01 00:07:51,447 | INFO  | addConsumer() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 info: ConsumerInfo {commandId = 4, responseRequired = true, consumerId = ID:deverylight-all-snapshot-42229-1467323807294-3:27:-1:1, destination = topic://mobile.167e0de6e20b55cc318aacb74f31752e2bffd339.*.position, prefetchSize = 32767, maximumPendingMessageLimit = 0, browser = false, dispatchAsync = true, selector = null, clientId = null, subscriptionName = null, noLocal = false, exclusive = false, retroactive = true, priority = 0, brokerPath = null, optimizedAcknowledge = false, noRangeAcks = false, additionalPredicate = null} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-39
2016-07-01 00:07:51,451 | INFO  | addConsumer() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 info: ConsumerInfo {commandId = 5, responseRequired = true, consumerId = ID:deverylight-all-snapshot-42229-1467323807294-3:27:-1:2, destination = topic://mobile.167e0de6e20b55cc318aacb74f31752e2bffd339.*.event, prefetchSize = 32767, maximumPendingMessageLimit = 0, browser = false, dispatchAsync = true, selector = null, clientId = null, subscriptionName = null, noLocal = false, exclusive = false, retroactive = true, priority = 0, brokerPath = null, optimizedAcknowledge = false, noRangeAcks = false, additionalPredicate = null} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-39
2016-07-01 00:08:51,151 | INFO  | removeConnection() clientId: 60badfb1-3e6d-4637-b4ec-16b557d7db64 userName: bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64 info: ConnectionInfo {commandId = 0, responseRequired = true, connectionId = ID:deverylight-all-snapshot-42229-1467323807294-3:27, clientId = 60badfb1-3e6d-4637-b4ec-16b557d7db64, clientIp = ws://192.168.0.2:42372, userName = bd525936-5238-4410-9579-79f8dfb27ea4::60badfb1-3e6d-4637-b4ec-16b557d7db64, password = *****, brokerPath = null, brokerMasterConnector = false, manageable = false, clientMaster = true, faultTolerant = false, failoverReconnect = false} | com.deveryware.deverylight.activemq.TokenAuthenticationBroker | qtp1143988572-40
{code}

this is my ActiveMQ configuration

{code}
<transportConnectors>
            <transportConnector name=""mqtt"" uri=""mqtt://0.0.0.0:1883""/>
            <transportConnector name=""mqtt+ws"" uri=""ws://0.0.0.0:9000""/>
        </transportConnectors>
{code}",Surprise,-1
AMQ-6451,"If the preallocationStrategy is set to 'zeros', ActiveMQ can intermittently become unable to allocate direct buffer memory with the default JVM settings. The exception isn't handled, and ends up both creating an empty journal file and, more importantly, leaking a file descriptor.

ActiveMQ eventually runs out of file descriptors and crashes.

In addition to handling this condition, perhaps the default ACTIVEMQ_OPTS_MEMORY settings should configure enough direct memory to allow some multiple of log files to be created near simultaneously, or at least this possibility documented in the KahaDB settings.

Relevant logs:

{noformat}
2016-10-03 12:47:24,154 | WARN  | Async error occurred: java.lang.OutOfMemoryError: Direct buffer memory | org.apache.activemq.broker.TransportConnection.Service | ActiveMQ Transport: ssl:///x.x.x.x:60805
2016-10-03 12:47:24,818 | WARN  | Async error occurred: java.lang.OutOfMemoryError: Direct buffer memory | org.apache.activemq.broker.TransportConnection.Service | ActiveMQ Transport: ssl:///x.x.x.x:60811
2016-10-03 12:47:25,477 | WARN  | Async error occurred: java.lang.OutOfMemoryError: Direct buffer memory | org.apache.activemq.broker.TransportConnection.Service | ActiveMQ Transport: ssl:///x.x.x.x:49830
2016-10-03 12:47:26,146 | WARN  | Store COMMIT FAILED:  | org.apache.activemq.transaction.LocalTransaction | ActiveMQ Transport: ssl:///x.x.x.x:65534
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:693)[:1.8.0_101]
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)[:1.8.0_101]
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)[:1.8.0_101]
        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)[:1.8.0_101]
        at sun.nio.ch.IOUtil.write(IOUtil.java:58)[:1.8.0_101]
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)[:1.8.0_101]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.doPreallocationZeros(Journal.java:366)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.preallocateEntireJournalDataFile(Journal.java:333)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.newDataFile(Journal.java:631)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.rotateWriteFile(Journal.java:595)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.getCurrentDataFile(Journal.java:984)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.DataFileAppender.enqueue(DataFileAppender.java:189)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.DataFileAppender.storeItem(DataFileAppender.java:128)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.disk.journal.Journal.write(Journal.java:890)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:1108)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.MessageDatabase.store(MessageDatabase.java:1090)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.store.kahadb.KahaDBTransactionStore.commit(KahaDBTransactionStore.java:301)[activemq-kahadb-store-5.14.0.jar:5.14.0]
        at org.apache.activemq.transaction.LocalTransaction.commit(LocalTransaction.java:70)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:252)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.BrokerFilter.commitTransaction(BrokerFilter.java:113)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.BrokerFilter.commitTransaction(BrokerFilter.java:113)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:118)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.TransportConnection.processCommitTransactionOnePhase(TransportConnection.java:535)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:100)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:333)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:197)[activemq-broker-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:125)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:300)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.tcp.SslTransport.doConsume(SslTransport.java:108)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:233)[activemq-client-5.14.0.jar:5.14.0]
        at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:215)[activemq-client-5.14.0.jar:5.14.0]
        at java.lang.Thread.run(Thread.java:745)[:1.8.0_101]
{noformat}

Empty journal files:

{noformat}
-rw-r--r-- 1 activemq activemq         0 Oct  3 12:47 db-51425.log
-rw-r--r-- 1 activemq activemq         0 Oct  3 12:47 db-51426.log
-rw-r--r-- 1 activemq activemq         0 Oct  3 12:47 db-51427.log
-rw-r--r-- 1 activemq activemq         0 Oct  3 12:47 db-51428.log
-rw-r--r-- 1 activemq activemq         0 Oct  3 12:47 db-51429.log
{noformat}

lsof output:

{noformat}
java    29263 activemq  452u      REG             202,48         0       160 /activemq/kahadb/db-51425.log
java    29263 activemq  455u      REG             202,48         0       153 /activemq/kahadb/db-51426.log
java    29263 activemq  456u      REG             202,48         0       133 /activemq/kahadb/db-51427.log
java    29263 activemq  462u      REG             202,48         0       157 /activemq/kahadb/db-51428.log
{noformat}
",Sadness,-1
AMQ-6548,"At ear stop jsm connection consumes messages, although there occured exception
{noformat}
2016-12-15 16:12:58,831 ERROR [org.jboss.as.ejb3.invocation.processInvocation] (default-threads - 23) WFLYEJB0034: EJB Invocation failed on component test.test.mdb for method public void test.test.onMessage(javax.jms.Message): org.jboss.as.ejb3.component.EJBComponentUnavailableException: WFLYEJB0421: Invocation cannot proceed as component is shutting down
        at org.jboss.as.ejb3.component.interceptors.ShutDownInterceptorFactory$1.processInvocation(ShutDownInterceptorFactory.java:59)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.as.ejb3.deployment.processors.EjbSuspendInterceptor.processInvocation(EjbSuspendInterceptor.java:53)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.as.ejb3.component.interceptors.LoggingInterceptor.processInvocation(LoggingInterceptor.java:66)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.as.ee.component.NamespaceContextInterceptor.processInvocation(NamespaceContextInterceptor.java:50)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.as.ejb3.component.interceptors.AdditionalSetupInterceptor.processInvocation(AdditionalSetupInterceptor.java:54)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.as.ejb3.component.messagedriven.MessageDrivenComponentDescription$5$1.processInvocation(MessageDrivenComponentDescription.java:213)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.invocation.ContextClassLoaderInterceptor.processInvocation(ContextClassLoaderInterceptor.java:64)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.invocation.InterceptorContext.run(InterceptorContext.java:356)
        at org.wildfly.security.manager.WildFlySecurityManager.doChecked(WildFlySecurityManager.java:634)
        at org.jboss.invocation.AccessCheckingInterceptor.processInvocation(AccessCheckingInterceptor.java:61)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.invocation.InterceptorContext.run(InterceptorContext.java:356)
        at org.jboss.invocation.PrivilegedWithCombinerInterceptor.processInvocation(PrivilegedWithCombinerInterceptor.java:80)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
        at org.jboss.as.ee.component.ViewService$View.invoke(ViewService.java:195)
        at org.jboss.as.ee.component.ViewDescription$1.processInvocation(ViewDescription.java:185)
        at org.jboss.invocation.InterceptorContext.proceed(InterceptorContext.java:340)
        at org.jboss.invocation.ChainedInterceptor.processInvocation(ChainedInterceptor.java:61)
        at org.jboss.as.ee.component.ProxyInvocationHandler.invoke(ProxyInvocationHandler.java:73)
        at test.test$$$view19.onMessage(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.jboss.as.ejb3.inflow.MessageEndpointInvocationHandler.doInvoke(MessageEndpointInvocationHandler.java:139)
        at org.jboss.as.ejb3.inflow.AbstractInvocationHandler.invoke(AbstractInvocationHandler.java:73)
        at test.test$$$endpoint12.onMessage(Unknown Source)
        at org.apache.activemq.ra.MessageEndpointProxy$MessageEndpointAlive.onMessage(MessageEndpointProxy.java:123)
        at org.apache.activemq.ra.MessageEndpointProxy.onMessage(MessageEndpointProxy.java:64)
        at org.apache.activemq.ActiveMQSession.run(ActiveMQSession.java:1041)
        at org.apache.activemq.ra.ServerSessionImpl.run(ServerSessionImpl.java:169)
        at org.jboss.jca.core.workmanager.WorkWrapper.run(WorkWrapper.java:226)
        at org.jboss.threads.SimpleDirectExecutor.execute(SimpleDirectExecutor.java:33)
        at org.jboss.threads.QueueExecutor.runTask(QueueExecutor.java:808)
        at org.jboss.threads.QueueExecutor.access$100(QueueExecutor.java:45)
        at org.jboss.threads.QueueExecutor$Worker.run(QueueExecutor.java:828)
        at java.lang.Thread.run(Thread.java:745)
        at org.jboss.threads.JBossThread.run(JBossThread.java:320)
{noformat}",Surprise,-1
AMQ-6707,"When ActiveMQ 5.14.5 is configured with jdbc persistence storage (postgres) from time to time below error occurs:
{code}
2017-06-15 01:41:37,418 | ERROR | enerContainer-21 | CommitTask                       | 67 - org.apache.aries.transaction.manager - 1.3.1 | Unexpected exception committing org.apache.geronimo.transaction.manager.WrapperNamedXAResource@34ac9d62; continuing to commit other RMs
javax.transaction.xa.XAException: STORE COMMIT FAILED: Transaction rolled back xaErrorCode:104
	at org.apache.activemq.TransactionContext.toXAException(TransactionContext.java:793)
	at org.apache.activemq.TransactionContext.commit(TransactionContext.java:622)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:54)
	at org.apache.geronimo.transaction.manager.CommitTask.run(CommitTask.java:64)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commitResources(TransactionImpl.java:688)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:327)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:252)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1020)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:761)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:730)
	at org.apache.aries.transaction.internal.AriesPlatformTransactionManager.commit(AriesPlatformTransactionManager.java:75)
	at sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.8.0_131]
	at java.lang.reflect.Method.invoke(Method.java:498)[:1.8.0_131]
	at com.ectsp.spring.osgi.PlatformTransactionManagerFactoryBean$ProxyTxManagerHandler.invoke(PlatformTransactionManagerFactoryBean.java:115)[169:ectsp-spring-osgi:1.0.0.SNAPSHOT]
	at com.sun.proxy.$Proxy68.commit(Unknown Source)[100:org.apache.servicemix.bundles.spring-tx:4.2.8.RELEASE_1]
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:245)[97:org.apache.servicemix.bundles.spring-jms:4.2.8.RELEASE_1]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1164)[97:org.apache.servicemix.bundles.spring-jms:4.2.8.RELEASE_1]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1156)[97:org.apache.servicemix.bundles.spring-jms:4.2.8.RELEASE_1]
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:1053)[97:org.apache.servicemix.bundles.spring-jms:4.2.8.RELEASE_1]
	at java.lang.Thread.run(Thread.java:748)[:1.8.0_131]
Caused by: javax.transaction.xa.XAException: STORE COMMIT FAILED: Transaction rolled back xaErrorCode:104
	at org.apache.activemq.transaction.Transaction.newXAException(Transaction.java:212)
	at org.apache.activemq.transaction.XATransaction.storeCommit(XATransaction.java:93)
	at org.apache.activemq.transaction.XATransaction.commit(XATransaction.java:76)
	at org.apache.activemq.broker.TransactionBroker.commitTransaction(TransactionBroker.java:252)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:118)
	at org.apache.activemq.broker.MutableBrokerFilter.commitTransaction(MutableBrokerFilter.java:118)
	at org.apache.activemq.broker.TransportConnection.processCommitTransactionTwoPhase(TransportConnection.java:547)
	at org.apache.activemq.command.TransactionInfo.visit(TransactionInfo.java:102)
	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:336)
	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:200)
	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:50)
	at org.apache.activemq.transport.WireFormatNegotiator.onCommand(WireFormatNegotiator.java:125)
	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:301)
	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:233)[47:org.apache.activemq.activemq-osgi:5.14.5]
	at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:215)[47:org.apache.activemq.activemq-osgi:5.14.5]
	at java.lang.Thread.run(Thread.java:745)[:1.8.0_131]
Caused by: java.io.IOException: Could not remove prepared transaction state from message add for sequenceId: 4025171
	at org.apache.activemq.store.jdbc.adapter.DefaultJDBCAdapter.doCommitAddOp(DefaultJDBCAdapter.java:1031)
	at org.apache.activemq.store.jdbc.JDBCPersistenceAdapter.commitAdd(JDBCPersistenceAdapter.java:780)
	at org.apache.activemq.store.jdbc.JdbcMemoryTransactionStore$CommitAddOutcome.run(JdbcMemoryTransactionStore.java:146)
	at org.apache.activemq.store.memory.MemoryTransactionStore$Tx.commit(MemoryTransactionStore.java:101)
	at org.apache.activemq.store.memory.MemoryTransactionStore.commit(MemoryTransactionStore.java:270)
	at org.apache.activemq.transaction.XATransaction.storeCommit(XATransaction.java:86)
	... 15 more
{code}
It seams that it the same issue as in https://issues.apache.org/jira/browse/AMQ-5567.",,
AMQ-6834,"Start karaf OSGi container, install Camel 2.19.2 and activemq-client feature (activemq-osgi bundle), then the ClassNotFoundException is thrown:
{code}
2017-10-10T10:12:30,012 | WARN  | pool-46-thread-2 | aries.blueprint.spring.Activator   57 | 291 - org.apache.aries.blueprint.spring - 0.4.0 | Error starting extension: org.apache.activemq.activemq-osgi/5.15.0
java.lang.ClassNotFoundException: org.apache.camel.osgi.CamelNamespaceHandler cannot be found by org.apache.activemq.activemq-osgi_5.15.0
	at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:461) [?:?]
	at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:372) [?:?]
	at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:364) [?:?]
	at org.eclipse.osgi.internal.loader.ModuleClassLoader.loadClass(ModuleClassLoader.java:161) [?:?]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) [?:?]
	at org.eclipse.osgi.internal.framework.EquinoxBundle.loadClass(EquinoxBundle.java:564) [?:?]
	at org.apache.aries.blueprint.spring.SpringExtension.start(SpringExtension.java:64) [291:org.apache.aries.blueprint.spring:0.4.0]
	at org.apache.felix.utils.extender.AbstractExtender$1.run(AbstractExtender.java:265) [291:org.apache.aries.blueprint.spring:0.4.0]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:?]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:?]
	at java.lang.Thread.run(Thread.java:745) [?:?]
{code}

From Camel 2.19.x, the camel-spring-dm feature/bundle has been totally removed, that's why the org.apache.camel.osgi.CamelNamespaceHandler couldn't be found anymore. 

{code}
http\://camel.apache.org/schema/osgi=org.apache.camel.osgi.CamelNamespaceHandler
{code}
There is still definition above in the spring.handlers of the activemq-osgi bundle. this may need to be removed or updated from spring.handlers.",,
HADOOP-10142,"Reduce the logs generated by ShellBasedUnixGroupsMapping.
For ex: Using WebHdfs from windows generates following log for each request

{noformat}2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who
org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)
        at org.apache.hadoop.util.Shell.run(Shell.java:417)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)
        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)
        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)
        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
2013-12-03 11:34:56,590 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user dr.who{noformat}",,
HADOOP-10468,"{{TestMetricsSystemImpl.testMultiThreadedPublish}} can fail intermediately due to the insufficient size of the sink queue:

{code}
2014-04-06 21:34:55,269 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
2014-04-06 21:34:55,270 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
2014-04-06 21:34:55,271 WARN  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:putMetricsImmediate(107)) - Collector has a full queue and can't consume the given metrics.
{code}

The unit test should increase the default queue size to avoid intermediate failure.",,
HADOOP-10937,"Touchz-ing a file results in a Null Pointer Exception

{noformat}
[hdfs@mynode hadoop-common]$ hdfs dfs -touchz /enc3/touchFIle
2014-08-01 08:45:10,148 INFO  [main] hdfs.DFSClient (DFSClient.java:<init>(605)) - Found KeyProvider: KeyProviderCryptoExtension: KMSClientProvider[http://mynode.myhost.com:16000/kms/v1/]
-touchz: Fatal internal error
java.lang.NullPointerException
	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)
	at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)
	at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)
	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)
	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)
{noformat}",,
HADOOP-11329,"Currently, HADOOP_HOME isn't part of the start up options of KMS. If I add the the following configuration to core-site.xml of kms,
{code} <property>
  <name>hadoop.security.crypto.codec.classes.aes.ctr.nopadding</name>
  <value>org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec</value>
 </property>
{code} kms server will throw the following exception when receive ""generateEncryptedKey"" request
{code}
2014-11-24 10:23:18,189 DEBUG org.apache.hadoop.crypto.OpensslCipher: Failed to load OpenSSL Cipher.
java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCodeLoader.buildSupportsOpenssl()Z
        at org.apache.hadoop.util.NativeCodeLoader.buildSupportsOpenssl(Native Method)
        at org.apache.hadoop.crypto.OpensslCipher.<clinit>(OpensslCipher.java:85)
        at org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec.<init>(OpensslAesCtrCryptoCodec.java:50)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:129)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:67)
        at org.apache.hadoop.crypto.CryptoCodec.getInstance(CryptoCodec.java:100)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:256)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:371)
        at org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension$CryptoExtension$EncryptedQueueRefiller.fillQueueForKey(EagerKeyGeneratorKeyProviderCryptoExtension.java:77)
        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:181)
        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:175)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)
        at org.apache.hadoop.crypto.key.kms.ValueQueue.getAtMost(ValueQueue.java:256)
        at org.apache.hadoop.crypto.key.kms.ValueQueue.getNext(ValueQueue.java:226)
        at org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension$CryptoExtension.generateEncryptedKey(EagerKeyGeneratorKeyProviderCryptoExtension.java:126)
        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.generateEncryptedKey(KeyProviderCryptoExtension.java:371)
        at org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider.generateEncryptedKey(KeyAuthorizationKeyProvider.java:192)
        at org.apache.hadoop.crypto.key.kms.server.KMS$9.run(KMS.java:379)
        at org.apache.hadoop.crypto.key.kms.server.KMS$9.run(KMS.java:375
{code}
The reason is that it cannot find libhadoop.so. This will prevent KMS to response to ""generateEncryptedKey"" requests.",,
HADOOP-11693,"One of our customers' production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it.

However, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state.

{code}
2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:
ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller
Cause:
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)
	at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 8 more

2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)
	at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)
	... 4 more
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 11 more

Sun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.Thread.run(Thread.java:745)
{code}

When archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled.","Fear, Sadness, Surprise",-1
HADOOP-11754,"RM fails to start in the non-secure mode with the following exception:

{noformat}
2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}
javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)
	at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)
	at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
	at org.mortbay.jetty.Server.doStart(Server.java:224)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret
	at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)
	... 23 more
...
2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
Caused by: java.io.IOException: Problem in starting http server. Server handlers failed
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)
	... 4 more
{noformat}

This is likely a regression introduced by HADOOP-10670.",,
HADOOP-12186,"ActiveStandbyElector shouldn't call {{monitorLockNodeAsync}} before StatCallback for previous {{zkClient.exists}} is received.
We saw RM shutdown because ActiveStandbyElector retrying monitorLockNodeAsync exceeded limit. The following is the logs.
Based on the log, it looks like multiple {{monitorLockNodeAsync}} are called at the same time due to back-to-back SyncConnected event received.
The current code doesn't prevent {{zkClient.exists}} from being called before AsyncCallback.StatCallback for previous {{zkClient.exists}} is received.
So the retry for {{monitorLockNodeAsync}} doesn't work correctly sometimes.
{code}
2015-07-01 19:24:12,806 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6674ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:12,919 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:14,704 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:14,704 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43487, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:14,707 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:14,712 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:21,374 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6667ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:21,477 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:22,640 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:22,640 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43526, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:22,641 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:22,642 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:29,310 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6669ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:29,413 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:30,738 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:30,739 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43574, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:30,739 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:30,740 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:37,409 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6670ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:37,512 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2015-07-01 19:24:38,979 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server node-1.internal/192.168.123.3:2181. Will not attempt to authenticate using SASL (unknown error)
2015-07-01 19:24:38,979 INFO org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /192.168.123.3:43598, server: node-1.internal/192.168.123.3:2181
2015-07-01 19:24:38,980 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server node-1.internal/192.168.123.3:2181, sessionid = 0x14e47693cc20007, negotiated timeout = 10000
2015-07-01 19:24:38,981 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2015-07-01 19:24:45,649 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 6669ms for sessionid 0x14e47693cc20007, closing socket connection and attempting reconnect
2015-07-01 19:24:45,752 FATAL org.apache.hadoop.ha.ActiveStandbyElector: Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-07-01 19:24:45,855 INFO org.apache.zookeeper.ZooKeeper: Session: 0x14e47693cc20007 closed
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,932 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type EMBEDDED_ELECTOR_FAILED. Cause:
Received stat error from Zookeeper. code:CONNECTIONLOSS. Not retrying further znode monitoring connection errors.
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,932 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,933 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x14e47693cc20007
2015-07-01 19:25:07,933 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2015-07-01 19:25:08,036 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{code}",Surprise,-1
HADOOP-12602,"I have seen this test failed a few times in the past.
Error Message
{noformat}
metricsSink.putMetrics(<Capturing argument>);
Wanted 2 times:
-> at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
But was 1 time:
-> at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:183)
{noformat}
Stacktrace
{noformat}
org.mockito.exceptions.verification.TooLittleActualInvocations: 
metricsSink.putMetrics(<Capturing argument>);
Wanted 2 times:
-> at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
But was 1 time:
-> at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:183)

	at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testQSize(TestMetricsSystemImpl.java:472)
{noformat}
Standard Output
{noformat}
2015-11-25 19:07:49,867 INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(115)) - loaded properties from hadoop-metrics2-test.properties
2015-11-25 19:07:49,932 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled snapshot period at 10 second(s).
2015-11-25 19:07:49,932 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(192)) - Test metrics system started
2015-11-25 19:07:50,134 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(203)) - Sink slowSink started
2015-11-25 19:07:50,135 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink slowSink
2015-11-25 19:07:50,135 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:start(203)) - Sink dataSink started
2015-11-25 19:07:50,136 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:registerSink(301)) - Registered sink dataSink
2015-11-25 19:07:50,746 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(211)) - Stopping Test metrics system...
2015-11-25 19:07:50,747 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(140)) - slowSink thread interrupted.
2015-11-25 19:07:50,748 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(140)) - dataSink thread interrupted.
2015-11-25 19:07:50,748 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(217)) - Test metrics system stopped.
{noformat}",,
HADOOP-12622,"In debugging a NM retry connection to RM (non-HA), the NM log during RM down time is very misleading:
{noformat}
2015-12-07 11:37:14,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:15,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:16,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:17,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:18,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:19,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:20,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:21,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:22,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:23,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:54,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:55,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:56,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:57,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:58,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:37:59,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 11:38:00,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
{noformat}
It actually only log client side retry on NetworkConnection failure but not include any info on RetryInvocationHandler where the real retry policy works. From the code below in RetryInvocationHandler.java, even the retry ends, we don't put warn messages to include how much/many time/ counts we spent on retry logic that make it harder to debug.

{code}
        if (failAction != null) {
          if (failAction.reason != null) {
            LOG.warn(""Exception while invoking "" + currentProxy.proxy.getClass()
                + ""."" + method.getName() + "" over "" + currentProxy.proxyInfo
                + "". Not retrying because "" + failAction.reason, ex);
          }
          throw ex;
        }
{code}
We should add failAction.reason as much as we can in multiple retry policies. In addition, we should keep consistent in log level for message during the retry attempts: now the ipc.client is INFO, but RetryInvocationHandler is DEBUG (if not fail_over). We should keep them consistent or it could be very confusing.","Sadness, Surprise",-1
HADOOP-12655,"Saw it in a pre-commit jenkins job https://builds.apache.org/job/PreCommit-HADOOP-Build/8242/testReport/org.apache.hadoop.http/TestHttpServer/testBindAddress/

It also appeared previously in Hadoop-common-trunk-Java8 jenkins on Oct 21.

In the following case, the first server bound to port 53212, and the second one bound to port 53225, which violated the assertion in the test case (the second port is supposed to be no more than the first + 8)
{noformat}
Stacktrace

java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.http.TestHttpServer.checkBindAddress(TestHttpServer.java:539)
	at org.apache.hadoop.http.TestHttpServer.testBindAddress(TestHttpServer.java:503)

{noformat}

{noformat}
Standard Output

2015-12-15 05:26:44,123 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(304)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-15 05:26:44,125 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.test is not defined
2015-12-15 05:26:44,126 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(701)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 05:26:44,127 INFO  http.HttpServer2 (HttpServer2.java:addFilter(676)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context test
2015-12-15 05:26:44,128 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 05:26:44,128 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 05:26:44,131 INFO  http.HttpServer2 (HttpServer2.java:openListeners(906)) - Jetty bound to port 41406
2015-12-15 05:26:44,155 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(304)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-15 05:26:44,157 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.test is not defined
2015-12-15 05:26:44,158 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(701)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 05:26:44,159 INFO  http.HttpServer2 (HttpServer2.java:addFilter(676)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context test
2015-12-15 05:26:44,159 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 05:26:44,160 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 05:26:44,161 INFO  http.HttpServer2 (HttpServer2.java:openListeners(906)) - Jetty bound to port 53212
2015-12-15 05:26:44,188 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(304)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2015-12-15 05:26:44,189 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.test is not defined
2015-12-15 05:26:44,190 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(701)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 05:26:44,191 INFO  http.HttpServer2 (HttpServer2.java:addFilter(676)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context test
2015-12-15 05:26:44,191 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 05:26:44,192 INFO  http.HttpServer2 (HttpServer2.java:addFilter(684)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 05:26:45,500 INFO  http.HttpServer2 (HttpServer2.java:openListeners(906)) - Jetty bound to port 53225

{noformat}",Surprise,-1
HADOOP-1712,"
One of the un-handled IOException during BlockCRC upgrade results in the upgrade thread to exit with out proper upgrade. 

exception on the datanode :
{noformat}
2007-08-13 22:18:27,324 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: Block blk_-6404399692543439055 is not valid.
        at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:492)
        at org.apache.hadoop.dfs.BlockCrcUpgradeObjectDatanode.doUpgrade(BlockCrcUpgrade.java:1431)
        at org.apache.hadoop.dfs.UpgradeObjectDatanode.run(UpgradeObjectDatanode.java:95)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Will also check if there are any more of such misses.
",Sadness,-1
HADOOP-1717,"TestDFSUpgradeFromImage is broken on Solaris so all patch builds will fail until it is fixed.  I believe Raghu is working on a patch which will remove the non-standard tar -z dependency.

From Enis Soztutar:
TestDFSUpgradeFromImage fails for hadoop-patch and hudson-nightly builds on hudson. 
The error thrown is :
{noformat}
java.io.IOException: tar: z: unknown function modifier
	at org.apache.hadoop.fs.Command.run(Command.java:33)
	at org.apache.hadoop.fs.Command.execCommand(Command.java:89)
	at org.apache.hadoop.dfs.TestDFSUpgradeFromImage.setUp(TestDFSUpgradeFromImage.java:75)

Standard Output

2007-08-15 13:22:38,601 INFO  dfs.TestDFSUpgradeFromImage (TestDFSUpgradeFromImage.java:setUp(72)) - Unpacking the tar file /export/home/hudson/hudson/jobs/Hadoop-Patch/workspace/trunk/build/test/cache/hadoop-12-dfs-dir.tgz
{noformat}
",,
HADOOP-1955,"When replicating corrupted block, receiving side rejects the block due to checksum error. Namenode keeps on retrying (with the same source datanode).
Fsck shows those blocks as under-replicated.


[Namenode log]
{noformat} 
2007-09-27 02:00:05,273 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.heartbeatCheck: lost heartbeat from 99.2.99.111
...
2007-09-27 02:01:02,618 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.37:9999
2007-09-27 02:10:03,843 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_-5925066143536023890
2007-09-27 02:10:08,248 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.35:9999
2007-09-27 02:20:03,848 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_-5925066143536023890
2007-09-27 02:20:08,646 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask 99.9.99.11:9999 to replicate blk_-5925066143536023890 to datanode(s) 99.9.99.19:9999
(repeats)
{noformat} 

[Datanode(sender) 99.9.99.11 log]
{noformat} 
2007-09-27 02:01:04,493 INFO org.apache.hadoop.dfs.DataNode: Starting thread to transfer block blk_-5925066143536023890 to [Lorg.apache.hadoop.dfs.DatanodeInfo;@e58187
2007-09-27 02:01:05,153 WARN org.apache.hadoop.dfs.DataNode: Failed to transfer blk_-5925066143536023890 to 74.6.128.37:50010 got java.net.SocketException: Connection reset
  at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
  at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
  at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
  at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)
  at java.io.DataOutputStream.write(DataOutputStream.java:90)
  at org.apache.hadoop.dfs.DataNode.sendBlock(DataNode.java:1231)
  at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:1280)
  at java.lang.Thread.run(Thread.java:619)
(repeats)
{noformat} 

[Datanode(one of the receiver) 99.9.99.37 log]
{noformat} 
2007-09-27 02:01:05,150 ERROR org.apache.hadoop.dfs.DataNode: DataXceiver: java.io.IOException: Unexpected checksum mismatch while writing blk_-5925066143536023890 from /74.6.128.33:57605
  at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:902)
  at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:727)
  at java.lang.Thread.run(Thread.java:619)
{noformat} ",,
HADOOP-2256,"
I will attach the log. Part of the log :

{noformat}
junit] 2007-11-20 18:44:13,559 WARN  dfs.DataNode (DataNode.java:copyBlock(1128)) - Got exception 
while serving blk_-8824434176426942280 to 127.0.0.1:50011: java.io.IOException: 
Block blk_-8824434176426942280 is not valid.
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getBlockFile(FSDataset.java:549)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getMetaFile(FSDataset.java:466)
    [junit] 	at org.apache.hadoop.dfs.FSDataset.getMetaDataInputStream(FSDataset.java:480)
    [junit] 	at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1282)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.copyBlock(DataNode.java:1098)
    [junit] 	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:861)
    [junit] 	at java.lang.Thread.run(Thread.java:595)
{noformat}

Summery might look like same as HADOOP-2200 but symptoms in log are different and I think the reason is different.
",,
HADOOP-2486,"Note: I'm really not sure if this is a bug in my code or in mapred. 

With my mapreduce job without combiner,  I sometimes see   # of total Map output records != # of total Reduce input records. What's weird to me is, when I rerun my code with exact same input, usually I get an expected #map output recs == #reduce output recs.

Both jobs finish successfully. No failed tasks. No speculative execution. 

I ran separate linecount mapred jobs on both the input and the output to see if  the counters are reporting the correct number. 


When I looked at all the 513 reducer counter, I found single reducer with different counts for the two runs. 
Only error stood out in that  reducer userlog is, 
{noformat} 
2007-12-22 00:19:07,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 done copying task_200712220008_0003_m_000288_0 output from qqq856.ppp.com.
2007-12-22 00:19:07,640 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 Copying task_200712220008_0003_m_000327_0 output from qqq887.ppp.com.
2007-12-22 00:19:07,640 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$FileAttributes.access$300(InMemoryFileSystem.java:366)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem$InMemoryFileStatus.(InMemoryFileSystem.java:380)
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:283)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:423)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:386)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:716)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:637)

2007-12-22 00:19:07,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 done copying task_200712220008_0003_m_000228_0 output from qqq801.ppp.com.
2007-12-22 00:19:07,641 INFO org.apache.hadoop.mapred.ReduceTask: task_200712220008_0003_r_000024_0 Copying task_200712220008_0003_m_000337_0 output from qqq841.ppp.com.
{noformat} 

Could this error be somehow related to my having different # of records? 
","Joy, Surprise","1, -1"
HADOOP-2756,"Saw this in logs:

{code}
2008-01-31 18:55:02,128 ERROR org.apache.hadoop.hbase.HRegionServer: Compaction failed for region TestTable,0009438931,1201805282651
java.lang.NullPointerException
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.close(DFSClient.java:2262)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:51)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:67)
        at org.apache.hadoop.hbase.HStoreFile.writeInfo(HStoreFile.java:365)
        at org.apache.hadoop.hbase.HStore.compact(HStore.java:1236) 
        at org.apache.hadoop.hbase.HRegion.compactStores(HRegion.java:775)
        at org.apache.hadoop.hbase.HRegion.compactIfNeeded(HRegion.java:707)
        at org.apache.hadoop.hbase.HRegionServer$CompactSplitThread.run(HRegionServer.java:253)
{code}

Look to see if the response data method needs to be made volatile (There's a test for null just before we use it on line #2262).",,
HADOOP-2814,"The test passes. But there is an NPE in datanode (using branch-0.16) :
{noformat}
...
2008-02-13 21:25:37,534 INFO  dfs.TestDataTransferProtocol (TestDataTransferProtocol.java:sendRecvData(71)) - Testing : wrong bytesPerChecksum while writing
2008-02-13 21:25:37,535 INFO  dfs.DataNode (DataNode.java:writeBlock(1048)) - Receiving block blk_7408940144175038455 src: /127.0.0.1:4964
6 dest: /127.0.0.1:49637
2008-02-13 21:25:37,536 ERROR dfs.DataNode (DataNode.java:run(961)) - 127.0.0.1:49637:DataXceiver: java.lang.NullPointerException
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.close(DataNode.java:2001)
        at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:131)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:1993)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1074)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:938)
        at java.lang.Thread.run(Thread.java:595)

2008-02-13 21:25:37,537 INFO  dfs.TestDataTransferProtocol (TestDataTransferProtocol.java:sendRecvData(87)) - Got EOF as expected.
...
{noformat}
",Surprise,-1
HADOOP-2971,"
TestJobStatusPersistency failed and contained DataNode stacktraces similar to the following :

{noformat}
2008-03-07 21:27:00,410 ERROR dfs.DataNode (DataNode.java:run(976)) - 127.0.0.1:57790:DataXceiver: java.net.SocketTimeoutException: 0 millis 
timeout while waiting for Unknown Addr (local: /127.0.0.1:57790) to be ready for read
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:188)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:135)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:121)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2434)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1170)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:953)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

This is mostly related to HADOOP-2346. The error is strange. socket.getRemoteSocketAddress() returned null implying this socket is not connected yet. But we have already read a few bytes from it!.

",Surprise,-1
HADOOP-3035,"Currently if a crc error occurs when data-node replicates a block to another node it throws an exception, and continues.
{code}
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:transferBlocks(811)) - 127.0.0.1:3730 Starting thread to transfer block blk_-1962819020391742554 to 127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:writeBlock(1067)) - Receiving block blk_-1962819020391742554 src: /127.0.0.1:3791 dest: /127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,855 INFO  dfs.DataNode (DataNode.java:receiveBlock(2504)) - Exception in receiveBlock for block blk_-1962819020391742554 java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit] 2008-03-17 19:46:11,871 INFO  dfs.DataNode (DataNode.java:run(2626)) - 127.0.0.1:3730:Transmitted block blk_-1962819020391742554 to /127.0.0.1:3740
    [junit] 2008-03-17 19:46:11,871 INFO  dfs.DataNode (DataNode.java:writeBlock(1192)) - writeBlock blk_-1962819020391742554 received exception java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit] 2008-03-17 19:46:11,871 ERROR dfs.DataNode (DataNode.java:run(979)) - 127.0.0.1:3740:DataXceiver: java.io.IOException: Unexpected checksum mismatch while writing blk_-1962819020391742554 from /127.0.0.1
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveChunk(DataNode.java:2246)
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2416)
    [junit]     at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2474)
    [junit]     at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1173)
    [junit]     at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:956)
    [junit]     at java.lang.Thread.run(Thread.java:595)
{code}
The data-node should report the error to the name-node so that the corrupted replica could be removed and replicated.",,
HADOOP-3108,"Not sure if this is fixed in later release, but I'm seeing many NPE in the namenode log.
Permission is disabled on this cluster.
{noformat} 
2008-03-27 03:22:39,984 INFO org.apache.hadoop.ipc.Server: IPC Server handler 18 on 8020, 
call setPermission(/user/knoguchi/file0, rwxr-xr-x) from 99.9.99.9:55555: 
error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSDirectory.unprotectedSetPermission(FSDirectory.java:411)
        at org.apache.hadoop.dfs.FSDirectory.setPermission(FSDirectory.java:405)
        at org.apache.hadoop.dfs.FSNamesystem.setPermission(FSNamesystem.java:716)
        at org.apache.hadoop.dfs.NameNode.setPermission(NameNode.java:297)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:409)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:899)
{noformat} ",Sadness,-1
HADOOP-3418,"How to reproduce :

{{$ bin/hadoop fs -put largeFile tmp/tmpFile}}
...before this finishes
{{$ bin/hadoop fs -rmr tmp}}
Now restart NameNode.
Restart fails with :
{noformat}
2008-05-20 02:21:34,731 ERROR org.apache.hadoop.dfs.NameNode: java.io.IOException: saveLeases found path /user/rangadi/tmp/tmpFile but no matching entry in namespace.
        at org.apache.hadoop.dfs.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4215)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:848)
        at org.apache.hadoop.dfs.FSImage.saveFSImage(FSImage.java:866)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:82)
        at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:253)
        at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
{noformat}",,
HADOOP-3576,"hadoop dfs -mv command throws NullPointerException while moving a directory to its subdirecotry. In 0.17 version, such a move was not allowed. 

Consider the example
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -mv /a/b /a/b/c
mv: java.io.IOException: java.lang.NullPointerException
[lohit@ hadoop-core-trunk]$ 
{noformat}

After this, the namespace of /a/b is gone
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -lsr /a
[lohit@ hadoop-core-trunk]$ 
{noformat}

Restarting the namenode recovers this namespace and everything seems to be normal.
On the other hand, before restarting the namenode, if we delete the directory /a, it succeeds. 
{noformat}
[lohit@ hadoop-core-trunk]$ ./bin/hadoop dfs -rmr /a
Deleted hdfs://jamba.juice:8020/a
[lohit@ hadoop-core-trunk]$ 
{noformat}

But, restarting now, throws an exception on NameNode and NameNode wouldn't start.
{noformat}
2008-06-17 00:58:50,422 ERROR org.apache.hadoop.dfs.NameNode: java.lang.NullPointerException
  at org.apache.hadoop.dfs.FSNamesystem.changeLease(FSNamesystem.java:4339)
  at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:561)
  at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:846)
  at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:675)
  at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:289)
  at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:80)
  at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:273)
  at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:252)
  at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:148)
  at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:193)
  at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:179)
  at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:825)
  at org.apache.hadoop.dfs.NameNode.main(NameNode.java:834)
{noformat}

In hadoop 0.17, we never allowed such a move.
{noformat}
[lohit@ branch-0.17]$ ./bin/hadoop dfs -mv /a/b /a/b/c
mv: Failed to rename /a/b to /a/b/c
[lohit@ branch-0.17]$ 
{noformat}

This is the issue seen with HADOOP-3561. Opening this JIRA to fix the underlying problem while HADOOP-3561 could be committed. ",Surprise,-1
HADOOP-3635,"I see bunch of datanodes stop verifying local blocks.

"".out"" showed 
{noformat}
-rw-r--r--  1 hdfs users 614 Jun 23 10:24 datanode.out

Exception in thread ""org.apache.hadoop.dfs.DataBlockScanner@aadc97"" java.lang.NumberFormatException: For input string: ""121195228080date=""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:412)
        at java.lang.Long.valueOf(Long.java:518)
        at org.apache.hadoop.dfs.DataBlockScanner$LogEntry.parseEntry(DataBlockScanner.java:351)
        at org.apache.hadoop.dfs.DataBlockScanner.assignInitialVerificationTimes(DataBlockScanner.java:481)
        at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:534)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Namenode log also showed 
{noformat}
2008-06-23 10:24:12,831 WARN org.apache.hadoop.dfs.DataBlockScanner: RuntimeException during DataBlockScanner.run() : java.lang.NumberFormatException: For input string: ""121195228080date=""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Long.parseLong(Long.java:412)
        at java.lang.Long.valueOf(Long.java:518)
        at org.apache.hadoop.dfs.DataBlockScanner$LogEntry.parseEntry(DataBlockScanner.java:351)
        at org.apache.hadoop.dfs.DataBlockScanner.assignInitialVerificationTimes(DataBlockScanner.java:481)
        at org.apache.hadoop.dfs.DataBlockScanner.run(DataBlockScanner.java:534)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

Datanode was still up and running but no verification.
Jstack didn't show DataBlockScanner.
",Surprise,-1
HADOOP-5539,"hadoop-site.xml :
mapred.compress.map.output = true

map output files are compressed but when the in memory merger closes 
on the reduce the on disk merger runs to reduce input files to <= io.sort.factor if needed. 

when this happens it outputs files called intermediate.x files these 
do not maintain compression setting the writer (o.a.h.mapred.Merger.class line 432)
passes the codec but I added some logging and its always null map output compression set true or false.

This causes task to fail if they can not hold the uncompressed size of the data of the reduce its holding
I thank this is just and oversight of the codec not getting set correctly for the on disk merges.

{code}
2009-03-20 01:30:30,005 INFO org.apache.hadoop.mapred.Merger: Merging 30 intermediate segments out of a total of 3000
2009-03-20 01:30:30,005 INFO org.apache.hadoop.mapred.Merger: intermediate.1 used codec: null
{code}

I added 
{code}
          // added my me
	   if (codec != null){
	     LOG.info(""intermediate."" + passNo + "" used codec: "" + codec.toString());
	   } else {
	     LOG.info(""intermediate."" + passNo + "" used codec: Null"");
	   }
	   // end added by me
{code}
Just before the creation of the writer o.a.h.mapred.Merger.class line 432
and it outputs the second line above.

I have confirmed this with the logging and I have looked at the files on the disk of the tasktracker. I can read the data in 
the intermediate files clearly telling me that there not compressed but I can not read the map.out files direct from the map output
telling me the compression is working on the map end but not on the on disk merge that produces the intermediate.

I can see no benefit for these not maintaining the compression setting and as it looks they where intended to maintain it.
","Fear, Sadness, Surprise",-1
HADOOP-7629,"MAPREDUCE-2289 introduced the following change:

{noformat}
+        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);
{noformat}

JOB_DIR_PERMISSION is an immutable FsPermission which cannot be used in RPC calls, it results in the following exception:

{noformat}
2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1
java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)
        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)
        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)
        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)
Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()
        at java.lang.Class.getConstructor0(Class.java:2706)
        at java.lang.Class.getDeclaredConstructor(Class.java:1985)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)
        ... 8 more
{noformat}
",Sadness,-1
HADOOP-8721,"Scenario:
Active NN on machine1
Standby NN on machine2
Machine1 is isolated from the network (machine1 network cable unplugged)
After zk session timeout ZKFC at machine2 side gets notification that NN1 is not there.
ZKFC tries to failover NN2 as active.
As part of this during fencing it tries to connect to machine1 and kill NN1. (sshfence technique configured)
This connection retry happens for 45 times( as it takes  ipc.client.connect.max.socket.retries)
Also after that standby NN is not able to take over as active (because of fencing failure).
Suggestion: If ZKFC is not able to reach other NN for specified time/no of retries it can consider that NN as dead and instruct the other NN to take over as active as there is no chance of the other NN (NN1) retaining its state as active after zk session timeout when its isolated from network

From ZKFC log:
{noformat}
2012-06-21 17:46:14,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 22 time(s).
2012-06-21 17:46:35,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 23 time(s).
2012-06-21 17:46:56,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 24 time(s).
2012-06-21 17:47:17,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 25 time(s).
2012-06-21 17:47:38,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 26 time(s).
2012-06-21 17:47:59,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 27 time(s).
2012-06-21 17:48:20,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 28 time(s).
2012-06-21 17:48:41,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 29 time(s).
2012-06-21 17:49:02,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 30 time(s).
2012-06-21 17:49:23,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: HOST-xx-xx-xx-102/xx.xx.xx.102:65110. Already tried 31 time(s).
{noformat}
 

",,
HADOOP-9865,"I discovered the problem when running unit test TestMRJobClient on Windows. The cause is indirect in this case. In the unit test, we try to launch a job and list its status. The job failed, and caused the list command get a result of 0, which triggered the unit test assert. From the log and debug, the job failed because we failed to create the Jar with classpath (see code around {{FileUtil.createJarWithClassPath}}) in {{ContainerLaunch}}. This is a Windows specific step right now; so the test still passes on Linux. This step failed because we passed in a relative path to {{FileContext.globStatus()}} in {{FileUtil.createJarWithClassPath}}. The relevant log looks like the following.

{noformat}
2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.
org.apache.hadoop.HadoopIllegalArgumentException: Path is relative
	at org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)
	at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)
	at org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:128)
	at org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)
	at org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I think this is a regression from HADOOP-9817. I modified some code and the unit test passed. (See the attached patch.) However, I think the impact is larger. I will add some unit tests to verify the behavior, and work on a more complete fix.",,
HDFS-10512,"VolumeScanner may terminate due to unexpected NullPointerException thrown in {{DataNode.reportBadBlocks()}}. This is different from HDFS-8850/HDFS-9190

I observed this bug in a production CDH 5.5.1 cluster and the same bug still persist in upstream trunk.

{noformat}
2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn
2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)
        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)
        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)
        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)
        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)
2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting.
{noformat}

I think the NPE comes from the volume variable in the following code snippet. Somehow the volume scanner know the volume, but the datanode can not lookup the volume using the block.
{code}
public void reportBadBlocks(ExtendedBlock block) throws IOException{
    BPOfferService bpos = getBPOSForBlock(block);
    FsVolumeSpi volume = getFSDataset().getVolume(block);
    bpos.reportBadBlocks(
        block, volume.getStorageID(), volume.getStorageType());
  }
{code}",Surprise,-1
HDFS-10609,"In normal operations, if SASL negotiation fails due to {{InvalidEncryptionKeyException}}, it is typically a benign exception, which is caught and retried :

{code:title=SaslDataTransferServer#doSaslHandshake}
  if (ioe instanceof SaslException &&
      ioe.getCause() != null &&
      ioe.getCause() instanceof InvalidEncryptionKeyException) {
    // This could just be because the client is long-lived and hasn't gotten
    // a new encryption key from the NN in a while. Upon receiving this
    // error, the client will get a new encryption key from the NN and retry
    // connecting to this DN.
    sendInvalidKeySaslErrorMessage(out, ioe.getCause().getMessage());
  } 
{code}

{code:title=DFSOutputStream.DataStreamer#createBlockOutputStream}
if (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {
            DFSClient.LOG.info(""Will fetch a new encryption key and retry, "" 
                + ""encryption key was invalid when connecting to ""
                + nodes[0] + "" : "" + ie);
{code}

However, if the exception is thrown during pipeline recovery, the corresponding code does not handle it properly, and the exception is spilled out to downstream applications, such as SOLR, aborting its operation:

{quote}
2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.
org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)
2016-07-06 12:12:51,997 ERROR org.apache.solr.update.CommitTracker: auto commit error...:org.apache.solr.common.SolrException: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619
        at org.apache.solr.update.HdfsTransactionLog.close(HdfsTransactionLog.java:316)
        at org.apache.solr.update.TransactionLog.decref(TransactionLog.java:505)
        at org.apache.solr.update.UpdateLog.addOldLog(UpdateLog.java:380)
        at org.apache.solr.update.UpdateLog.postCommit(UpdateLog.java:676)
        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:623)
        at org.apache.solr.update.CommitTracker.run(CommitTracker.java:216)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)
{quote}

This exception should be contained within HDFS, caught and retried just like in {{createBlockOutputStream()}}",Sadness,-1
HDFS-10760,"DataXceiver#run() just log InvalidToken exception as an error.
When client has an expired token and just refetch a new token, the DN log will has an error like below:
{noformat}
2016-08-11 02:41:09,817 ERROR datanode.DataNode (DataXceiver.java:run(269)) - XXXXXXX:50010:DataXceiver error processing READ_BLOCK operation  src: /10.17.1.5:38844 dst: /10.17.1.5:50010
org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.
        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)
        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)
        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
This is not a server error and the DataXceiver#checkAccess() has already loged the InvalidToken as a warning.
A simple fix by catching the InvalidToken exception in DataXceiver#run(), only keeping the warning logged by DataXceiver#checkAccess() in the DN log.",Surprise,-1
HDFS-11164,"When mover is trying to move a pinned block to another datanode, it will internally hits the following IOException and mark the block movement as {{failure}}. Since the Mover has {{dfs.mover.retry.max.attempts}} configs, it will continue moving this block until it reaches {{retryMaxAttempts}}. If the block movement failure(s) are only due to block pinning, then retry is unnecessary. The idea of this jira is to avoid retry attempts of pinned blocks as they won't be able to move to a different node. 

{code}
2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501
java.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed
	at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)
	at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)
	at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)
	at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)
	at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,
HDFS-11377,"When running balancer on large cluster which have more than 3000 Datanodes, it might be hung due to ""No mover threads available"".
The stack trace shows it waiting forever like below.
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)
        at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)
{code}

In the log, there are lots of WARN about ""No mover threads available"".
{quote}
2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13700554102_1112815018180 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010
2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_4009558842_1103118359883 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010
2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13881956058_1112996460026 with size=133509566 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.36:50010
{quote}

What happened here is, when there are no mover threads available, DDatanode.isPendingQEmpty() will return false, so Balancer hung.",,
HDFS-11508,"Bind can fail in SimpleTCPServer & Portmap because socket is in TIME_WAIT state.

Socket options should be changed here to use the setReuseAddress option.

{noformat}
2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1
2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.
org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242
	at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)
	at org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)
	at org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)
	at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)
	at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)
	at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)
	at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1
2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: 
/************************************************************
{noformat}",Fear,-1
HDFS-11515,"HDFS-10797 fixed a disk summary (-du) bug, but it introduced a new bug.

The bug can be reproduced running the following commands:
{noformat}
bash-4.1$ hdfs dfs -mkdir /tmp/d0
bash-4.1$ hdfs dfsadmin -allowSnapshot /tmp/d0
Allowing snaphot on /tmp/d0 succeeded
bash-4.1$ hdfs dfs -touchz /tmp/d0/f4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s1
Created snapshot /tmp/d0/.snapshot/s1
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s2
Created snapshot /tmp/d0/.snapshot/s2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -du -h /tmp/d0
du: java.util.ConcurrentModificationException
0 0 /tmp/d0/f4
{noformat}

A ConcurrentModificationException forced du to terminate abruptly.

Correspondingly, NameNode log has the following error:
{noformat}
2017-03-08 14:32:17,673 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSumma
ry from 10.0.0.198:49957 Call#2 Retry#0
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
        at java.util.HashMap$KeyIterator.next(HashMap.java:956)
        at org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext.tallyDeletedSnapshottedINodes(ContentSummaryComputationContext.java:209)
        at org.apache.hadoop.hdfs.server.namenode.INode.computeAndConvertContentSummary(INode.java:507)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2302)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:4535)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1087)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getContentSummary(AuthorizationProviderProxyClientProtocol.java:5
63)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.jav
a:873)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)
{noformat}

The bug is due to a improper use of HashSet, not concurrent operations. Basically, a HashSet can not be updated while an iterator is traversing it.",Surprise,-1
HDFS-11526,"The following error message is wrong.
{code:title=BlockRecoveryWorker#recover}
        } catch (IOException e) {
          ++errorCount;
          InterDatanodeProtocol.LOG.warn(
              ""Failed to obtain replica info for block (="" + block
                  + "") from datanode (="" + id + "")"", e);
        }
{code}
The operation performed in the try block is an attempt to recover the block, not obtain replica info from the datanode.

This is the error message printed by the above code:

{noformat}
2017-03-01 16:15:35,884 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-1949147302-10.0.0.140-1423905184563:blk_1074852850_1112215) from datanode (=DatanodeInfoWithStorage[10.0.0.53:50010,null,null])
java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1112223, block=blk_1074852850_1112215, replica=FinalizedReplica, blk_1074852850_1112231, FINALIZED
  getNumBytes()     = 12823160
  getBytesOnDisk()  = 12823160
  getVisibleLength()= 12823160
  getVolume()       = /dfs/dn/current
  getBlockFile()    = /dfs/dn/current/BP-1949147305-10.0.0.140-1423905184563/current/finalized/subdir16/subdir243/blk_1074852850
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2318)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2277)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2548)
        at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)
        at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:2983)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
        at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker.callInitReplicaRecovery(BlockRecoveryWorker.java:339)
        at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker.access$300(BlockRecoveryWorker.java:46)
        at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous.recover(BlockRecoveryWorker.java:118)
        at org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1.run(BlockRecoveryWorker.java:374)
        at java.lang.Thread.run(Thread.java:745)
{noformat}",Sadness,-1
HDFS-1158,"Whenever we restart a cluster, there's a chance of losing some blocks if more than three datanodes don't come up.
HDFS-457 increases this chance by keeping the datanodes up even when 
   # /tmp disk goes read-only
   # /disk0 that is used for storing PID goes read-only 
and probably more.

In our environment, /tmp and /disk0 are from the same device.

When trying to restart a datanode, it would fail with
1) 
{noformat}
2010-05-15 05:45:45,575 WARN org.mortbay.log: tmpdir
java.io.IOException: Read-only file system
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.checkAndCreate(File.java:1704)
        at java.io.File.createTempFile(File.java:1792)
        at java.io.File.createTempFile(File.java:1828)
        at org.mortbay.jetty.webapp.WebAppContext.getTempDirectory(WebAppContext.java:745)
{noformat}
or 
2) 
{noformat}
hadoop-daemon.sh: line 117: /disk/0/hadoop-datanode....com.out: Read-only file system
hadoop-daemon.sh: line 118: /disk/0/hadoop-datanode.pid: Read-only file system
{noformat}

I can recover the missing blocks but it takes some time.

Also, we are losing track of block movements since log directory can also go to read-only but datanode would continue running.

For 0.21 release, can we revert HDFS-457 or make it configurable?
","Sadness, Surprise",-1
HDFS-11593,"A busy datanode may have many client disconnect exception logged with stack like below, which does not provide much useful information. Propose to reduce the log level from info to debug.

{code}
2017-03-29 20:28:55,225 INFO  web.DatanodeHttpServer (SimpleHttpProxyHandler.java:exceptionCaught(147)) - Proxy for / failed. cause: 
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.UnpooledUnsafeDirectByteBuf.setBytes(UnpooledUnsafeDirectByteBuf.java:446)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:225)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:745)
{code}",Sadness,-1
HDFS-11741,"We found a long running balancer may fail despite using keytab, because KeyManager returns expired DataEncryptionKey, and it throws the following exception:

{noformat}
2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010
org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)
        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)
        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

This bug is similar in nature to HDFS-10609. While balancer KeyManager actively synchronizes itself with NameNode w.r.t block keys, it does not update DataEncryptionKey accordingly.

In a specific cluster, with Kerberos ticket life time 10 hours, and default block token expiration/life time 10 hours, a long running balancer failed after 20~30 hours.",Sadness,-1
HDFS-12363,"Saw NN going down with NPE below:

{noformat}
ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.
java.lang.NullPointerException
at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)
at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)
at java.lang.Thread.run(Thread.java:745)
2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: 
{noformat}

In that version, {{BlockManager}} code is:
{code}
3896  try {
3897           DatanodeStorageInfo storage = datanodeManager.
3898                 getDatanode(datanodesAndStorages.get(i)).
3899                getStorageInfo(datanodesAndStorages.get(i + 1));
3900            if (storage != null) {
{code}",,
HDFS-12383,"Seen an instance where the re-encryption updater exited due to an exception, and later tasks no longer executes. Logs below:
{noformat}
2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Zone /tmp/encryption-zone-3(16819) is submitted for re-encryption.
2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Executing re-encrypt commands on zone 16819. Current zones:[zone:16787 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16813 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16819 state:Submitted lastProcessed:null filesReencrypted:0 fileReencryptionFailures:0]
2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 starts re-encryption processing
2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Re-encrypting zone /tmp/encryption-zone-3(id=16819)
2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submitted batch (start:/tmp/encryption-zone-3/data1, size:1) of zone 16819 to re-encrypt.
2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submission completed of zone 16819 for re-encryption.
2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Processing batched re-encryption for zone 16819, batch size 1, start:/tmp/encryption-zone-3/data1
2017-08-31 09:54:08,979 INFO BlockStateChange: BLOCK* BlockManager: ask 172.26.1.71:20002 to delete [blk_1073742291_1467]
2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Cancelling 1 re-encryption tasks
2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Cancelled zone /tmp/encryption-zone-3(16819) for re-encryption.
2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 completed re-encryption.
2017-08-31 09:54:18,296 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Completed re-encrypting one batch of 1 edeks from KMS, time consumed: 10.19 s, start: /tmp/encryption-zone-3/data1.
2017-08-31 09:54:18,296 ERROR org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Re-encryption updater thread exiting.
java.util.concurrent.CancellationException
        at java.util.concurrent.FutureTask.report(FutureTask.java:121)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)
        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Updater should be fixed to handle canceled tasks better.",Sadness,-1
HDFS-12498,"Journal Syncer is not getting started in HDFS + Federated cluster, when dfs.shared.edits.dir.<<nameserviceId>> is provided, instead of dfs.namenode.shared.edits.dir 

*Log Snippet:*

{code:java}
2017-09-19 21:42:40,598 WARN org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer: Could not construct Shared Edits Uri
2017-09-19 21:42:40,598 WARN org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer: Other JournalNode addresses not available. Journal Syncing cannot be done
2017-09-19 21:42:40,598 WARN org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer: Failed to start SyncJournal daemon for journal ns1
{code}

",,
HDFS-12833,"Basically Delete option applicable only with update or overwrite options. I tried as per usage message am getting the bellow exception.

{noformat}
bin:> ./hadoop distcp -delete /Dir1/distcpdir /Dir/distcpdir5
2017-11-17 20:48:09,828 ERROR tools.DistCp: Invalid arguments:
java.lang.IllegalArgumentException: Delete missing is applicable only with update or overwrite options
        at org.apache.hadoop.tools.DistCpOptions$Builder.validate(DistCpOptions.java:528)
        at org.apache.hadoop.tools.DistCpOptions$Builder.build(DistCpOptions.java:487)
        at org.apache.hadoop.tools.OptionsParser.parse(OptionsParser.java:233)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:141)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:432)
Invalid arguments: Delete missing is applicable only with update or overwrite options
usage: distcp OPTIONS [source_path...] <target_path>
              OPTIONS
 -append                       Reuse existing data in target files and
                               append new data to them if possible
 -async                        Should distcp execution be blocking
 -atomic                       Commit all changes or none
 -bandwidth <arg>              Specify bandwidth per map in MB, accepts
                               bandwidth as a fraction.
 -blocksperchunk <arg>         If set to a positive value, fileswith more
                               blocks than this value will be split into
                               chunks of <blocksperchunk> blocks to be
                               transferred in parallel, and reassembled on
                               the destination. By default,
                               <blocksperchunk> is 0 and the files will be
                               transmitted in their entirety without
                               splitting. This switch is only applicable
                               when the source file system implements
                               getBlockLocations method and the target
                               file system implements concat method
 -copybuffersize <arg>         Size of the copy buffer to use. By default
                               <copybuffersize> is 8192B.
 -delete                       Delete from target, files missing in source
 -diff <arg>                   Use snapshot diff report to identify the
                               difference between source and target
{noformat}

Even in Document also it's not updated proper usage.",Surprise,-1
HDFS-12836,"When {{dfs.ha.tail-edits.in-progress}} is true, edit log tailer will also tail those in progress edit log segments. However, in the following code:

{code}
        if (onlyDurableTxns && inProgressOk) {
          endTxId = Math.min(endTxId, committedTxnId);
        }

        EditLogInputStream elis = EditLogFileInputStream.fromUrl(
            connectionFactory, url, remoteLog.getStartTxId(),
            endTxId, remoteLog.isInProgress());
{code}

it is possible that {{remoteLog.getStartTxId()}} could be greater than {{endTxId}}, and therefore will cause the following error:

{code}
2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87
Recent opcode offsets: 1048576
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)
2017-11-17 19:55:41,165 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Error while reading edits from disk. Will try again.
org.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 1048576.  Expected transaction ID was 87
Recent opcode offsets: 1048576
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:218)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)
Caused by: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)
        ... 9 more
{code}",Fear,-1
HDFS-129,"When bringing back a decommissioned node, we forgot to take out the hostname from dfs.hosts.exclude and call dfsadmin -refreshNodes.  
Somehow, instead of getting 'reject' message, datanode shutdown with NPE.  After dfsadmin -refreshNodes, datanode was able to join back. 

Stack trace, 
{noformat} 
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ____.____.com/99.9.99.9
STARTUP_MSG:   args = []
************************************************************/
2008-02-26 20:30:56,523 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with
processName=DataNode, sessionId=null
2008-02-26 20:30:57,818 INFO org.apache.hadoop.dfs.DataNode: Opened server at -----
2008-02-26 20:30:57,938 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-02-26 20:30:57,982 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-02-26 20:30:58,000 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-02-26 20:30:58,001 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-02-26 20:30:58,360 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@20fa83
2008-02-26 20:30:58,462 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-02-26 20:30:58,464 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:-----
2008-02-26 20:30:58,464 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@16dc861
2008-02-26 20:30:58,464 INFO org.apache.hadoop.dfs.DataNode: Starting to run script to get datanode network location
2008-02-26 20:30:58,591 INFO org.mortbay.util.ThreadedServer: Stopping Acceptor
ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=-----]
2008-02-26 20:30:58,593 INFO org.mortbay.http.SocketListener: Stopped SocketListener on 0.0.0.0:-----
2008-02-26 20:30:58,642 INFO org.mortbay.util.Container: Stopped HttpContext[/static,/static]
2008-02-26 20:30:58,680 INFO org.mortbay.util.Container: Stopped HttpContext[/logs,/logs]
2008-02-26 20:30:58,681 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.servlet.WebApplicationHandler@20fa83
2008-02-26 20:30:58,718 INFO org.mortbay.util.Container: Stopped WebApplicationContext[/,/]
2008-02-26 20:30:58,719 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.Server@16dc861
2008-02-26 20:30:58,719 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.ipc.RemoteException:
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.checkDecommissionStateInternal(FSNamesystem.java:2918)
        at org.apache.hadoop.dfs.FSNamesystem.verifyNodeRegistration(FSNamesystem.java:3134)
        at org.apache.hadoop.dfs.FSNamesystem.registerDatanode(FSNamesystem.java:1679)
        at org.apache.hadoop.dfs.NameNode.register(NameNode.java:538)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.dfs.$Proxy0.register(Unknown Source)
        at org.apache.hadoop.dfs.DataNode.register(DataNode.java:391)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:287)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:206)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1575)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1519)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:1540)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1711)

2008-02-26 20:30:58,720 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ___.____.com/99.9.99.9
************************************************************/


{noformat} ",Surprise,-1
HDFS-13023,"Fails with the following exception.

{code}

2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster
 2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485
 com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
 at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)
 at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)
 at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)
 at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)
 at org.apache.hadoop.ipc.Client.call(Client.java:1437)
 at org.apache.hadoop.ipc.Client.call(Client.java:1347)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
 ... 6 more

{code}",,
HDFS-13145,"With edit log in-progress edit log tailing enabled, {{QuorumOutputStream}} will send two batches to JNs, one normal edit batch followed by a dummy batch to update the commit ID on JNs.

{code}
      QuorumCall<AsyncLogger, Void> qcall = loggers.sendEdits(
          segmentTxId, firstTxToFlush,
          numReadyTxns, data);
      loggers.waitForWriteQuorum(qcall, writeTimeoutMs, ""sendEdits"");
      
      // Since we successfully wrote this batch, let the loggers know. Any future
      // RPCs will thus let the loggers know of the most recent transaction, even
      // if a logger has fallen behind.
      loggers.setCommittedTxId(firstTxToFlush + numReadyTxns - 1);

      // If we don't have this dummy send, committed TxId might be one-batch
      // stale on the Journal Nodes
      if (updateCommittedTxId) {
        QuorumCall<AsyncLogger, Void> fakeCall = loggers.sendEdits(
            segmentTxId, firstTxToFlush,
            0, new byte[0]);
        loggers.waitForWriteQuorum(fakeCall, writeTimeoutMs, ""sendEdits"");
      }
{code}

Between each batch, it will wait for the JNs to reach a quorum. However, if the ANN crashes in between, then SBN will crash while transiting to ANN:

{code}
java.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)
        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2490)
2018-02-13 00:43:20,728 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{code}

This is because without the dummy batch, the {{commitTxnId}} will lag behind the {{endTxId}}, which caused the check in {{openForWrite}} to fail:
{code}
    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();
    journalSet.selectInputStreams(streams, segmentTxId, true, false);
    if (!streams.isEmpty()) {
      String error = String.format(""Cannot start writing at txid %s "" +
        ""when there is a stream available for read: %s"",
        segmentTxId, streams.get(0));
      IOUtils.cleanupWithLogger(LOG,
          streams.toArray(new EditLogInputStream[0]));
      throw new IllegalStateException(error);
    }
{code}

In our environment, this can be reproduced pretty consistently, which will leave the cluster with no running namenodes. Even though we are using a 2.8.2 backport, I believe the same issue also exist in 3.0.x. ",,
HDFS-13164,"This is found during yarn log aggregation but theoretically could happen to any client.

If the dir's space quota is exceeded, the following would happen when a file is created:
 - client {{startFile}} rpc to NN, gets a {{DFSOutputStream}}.
 - writing to the stream would trigger the streamer to {{getAdditionalBlock}} rpc to NN, which would get the DSQuotaExceededException
 - client closes the stream
 
 The fact that this would leave a 0-sized (or whatever size left in the quota) file in HDFS is beyond the scope of this jira. However, the file would be left in openforwrite status (shown in{{fsck -openforwrite)}} at least, and could potentially leak leaseRenewer too.

This is because in the close implementation,
 # {{isClosed}} is first checked, and the close call will be a no-op if {{isClosed == true}}.
 # {{flushInternal}} checks {{isClosed}}, and throws the exception right away iftrue

{{isClosed}} does this: {{return closed || getStreamer().streamerClosed;}}

When the disk quota is reached, {{getAdditionalBlock}} will throw when the streamer calls. Because the streamer runs in a separate thread, at the time the client calls close on the stream, the streamer may or may not have reached the Quota exception. If it has, then due to #1, the close call on the stream will be no-op. If it hasn't, then due to #2 the {{completeFile}} logic will be skipped.
{code:java}
protected synchronized void closeImpl() throws IOException {
    if (isClosed()) {
      IOException e = lastException.getAndSet(null);
      if (e == null)
        return;
      else
        throw e;
    }
  try {
    flushBuffer(); // flush from all upper layers
    ...
    flushInternal(); // flush all data to Datanodes

    // get last block before destroying the streamer
    ExtendedBlock lastBlock = getStreamer().getBlock();

    try (TraceScope ignored =
       dfsClient.getTracer().newScope(""completeFile"")) {
       completeFile(lastBlock);
    }
   } catch (ClosedChannelException ignored) {
   } finally {
     closeThreads(true);
   }
 }

{code}
Log snippets:
{noformat}
2018-02-16 15:59:32,916 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer Quota Exception
org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1833)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1626)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:788)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /tmp/logs/systest/logs is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)

        at org.apache.hadoop.ipc.Client.call(Client.java:1504)
        at org.apache.hadoop.ipc.Client.call(Client.java:1441)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy82.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:423)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy83.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1830)
        ... 2 more
{noformat}",Fear,-1
HDFS-13368,"WithHDFS-13300, the hostName and IpAdress in the DatanodeDetails.proto file made required fiields. These parameters are not set in TestEndPoint which lead these to fail consistently.

TestEndPoint#testRegisterToInvalidEndpoint
{code:java}
com.google.protobuf.UninitializedMessageException: Message missing required fields: ipAddress, hostName

at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:770)
at org.apache.hadoop.hdsl.protocol.proto.HdslProtos$DatanodeDetailsProto$Builder.build(HdslProtos.java:1756)
at org.apache.hadoop.ozone.container.common.TestEndPoint.registerTaskHelper(TestEndPoint.java:236)
at org.apache.hadoop.ozone.container.common.TestEndPoint.testRegisterToInvalidEndpoint(TestEndPoint.java:257)

{code}
TestEndPoint#testHeartbeatTaskToInvalidNode
{code:java}
2018-03-29 18:14:54,140 WARN impl.RaftServerProxy: FAILED new RaftServerProxy attempt #1/5: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: com/codahale/metrics/Timer, sleep 500ms and then retry.
java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: com/codahale/metrics/Timer
at org.apache.ratis.server.storage.RaftLogWorker.<init>(RaftLogWorker.java:104)
at org.apache.ratis.server.storage.SegmentedRaftLog.<init>(SegmentedRaftLog.java:113)
at org.apache.ratis.server.impl.ServerState.initLog(ServerState.java:151)
at org.apache.ratis.server.impl.ServerState.<init>(ServerState.java:101){code}",,
HDFS-13485,"curl -k -i --negotiate -u : ""https://hadoop3-4.example.com:20004/webhdfs/v1""

DataNode Web UI should do a better error checking/handling. 

{noformat}
2018-04-19 10:07:49,338 WARN org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler: INTERNAL_SERVER_ERROR
java.lang.NullPointerException
        at org.apache.hadoop.security.token.Token.decodeWritable(Token.java:364)
        at org.apache.hadoop.security.token.Token.decodeFromUrlString(Token.java:383)
        at org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser.delegationToken(ParameterParser.java:128)
        at org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider.ugi(DataNodeUGIProvider.java:76)
        at org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler.channelRead0(WebHdfsHandler.java:129)
        at org.apache.hadoop.hdfs.server.datanode.web.URLDispatcher.channelRead0(URLDispatcher.java:51)
        at org.apache.hadoop.hdfs.server.datanode.web.URLDispatcher.channelRead0(URLDispatcher.java:31)
        at com.cloudera.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at com.cloudera.io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at com.cloudera.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)
        at com.cloudera.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at com.cloudera.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1379)
        at com.cloudera.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1158)
        at com.cloudera.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1193)
        at com.cloudera.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
        at com.cloudera.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
        at com.cloudera.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at com.cloudera.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at com.cloudera.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at com.cloudera.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
        at com.cloudera.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
        at com.cloudera.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at com.cloudera.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at com.cloudera.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at com.cloudera.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at com.cloudera.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
        at com.cloudera.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
{noformat}",Sadness,-1
HDFS-13721,"{noformat}
2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException
 ***** TRACEBACK 4 *****
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
 at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)
 at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)
 at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
 at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
 at org.eclipse.jetty.server.Server.handle(Server.java:534)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
 at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
 at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
 at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
 at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
 at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
 at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
 at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
2018-06-28 05:12:08,400 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException
{noformat}

We have seen the above exception at datanode startup time. Should improve the NPE. Changing it to an IOE will also allow jmx to return '' correctly for \{{getDiskBalancerStatus}}

.

",,
HDFS-2245,"{noformat}
2011-08-10 20:20:51,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call: addBlock(/user/had
oopqa/passwd.1108102020.<NN hostname>.txt, DFSClient_NONMAPREDUCE_1875954430_1, null, null), rpc
 version=1, client version=68, methodsFingerPrint=-1239577025 from <gateway>:38874, error:
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)
        ...
{noformat}
",,
HDFS-3326,"""dfs.support.append"" is set to true
started NN in non-HA mode

At the NN side log the append enable is set to false.

This is because in code append enabled is set to HA enabled value.Since Started NN in non-HA mode the value for append is false
Code:
=====
{noformat}
this.supportAppends = conf.getBoolean(DFS_SUPPORT_APPEND_KEY, DFS_SUPPORT_APPEND_DEFAULT);
      LOG.info(""Append Enabled: "" + haEnabled);{noformat}
NN logs
========
{noformat}
2012-04-25 21:11:09,693 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2012-04-25 21:11:09,702 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: false{noformat}
",Surprise,-1
HDFS-3332,"There is 1 NN and 1 DN (NN is started with HA conf)
I corrupted 1 block and found 
{code}
2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(401)) - BlockReport of 2 blocks took 0 msec to generate and 5 msecs for RPC and NN processing
2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(420)) - sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b756db3
2012-04-27 09:59:01,726 INFO  datanode.DirectoryScanner (DirectoryScanner.java:scan(390)) - BlockPool BP-2087868617-10.18.40.95-1335500488012 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:1
2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1366)) - Updating size of block -4466699320171028643 from 1024 to 1034
2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1374)) - Reporting the block blk_-4466699320171028643_1004 as corrupt due to length mismatch
2012-04-27 09:59:01,728 DEBUG ipc.Client (Client.java:sendParam(807)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root sending #257
2012-04-27 09:59:01,730 DEBUG ipc.Client (Client.java:receiveResponse(848)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root got value #257
2012-04-27 09:59:01,730 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(193)) - Call: reportBadBlocks 2
2012-04-27 09:59:01,731 ERROR datanode.DirectoryScanner (DirectoryScanner.java:run(288)) - Exception during DirectoryScanner execution - will continue next cycle
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)
	at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)
	at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)
	at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{code}

Here when Directory scanner is trying to report badblock we got a NPE.",,
HDFS-3374,"The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.

{code}

    [junit] 2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1
    [junit] 2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible
    [junit] java.lang.Exception: No edit streams are accessible
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)
    [junit]     at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] Running org.apache.hadoop.hdfs.security.TestDelegationToken
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)
{code}",,
HDFS-3398,"Scenario:
=========
Start NN and three DN""S


Get the datanode to which blocks has to be replicated.
from 
{code}
nodes = nextBlockOutputStream(src);

{code}
Before start writing to the DN ,kill the primary DN.
{code}
// write out data to remote datanode
          blockStream.write(buf.array(), buf.position(), buf.remaining());
          blockStream.flush();
{code}

Now write will fail with the exception 

{noformat}
2012-05-10 14:21:47,993 WARN  hdfs.DFSClient (DFSOutputStream.java:run(552)) - DataStreamer Exception
java.io.IOException: An established connection was aborted by the software in your host machine
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(Unknown Source)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(Unknown Source)
	at sun.nio.ch.IOUtil.write(Unknown Source)
	at sun.nio.ch.SocketChannelImpl.write(Unknown Source)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)
	at java.io.BufferedOutputStream.write(Unknown Source)
	at java.io.DataOutputStream.write(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:513)

{noformat}


.

",,
HDFS-35,"If a file has a replicaiton of 3 and setReplication() is used to set the replication to 1 we will see following log in NameNode log : 

{noformat}
2007-08-07 12:18:27,370 INFO  fs.FSNamesystem (FSNamesystem.java:setReplicationInternal(661)) - Increasing replication for file /srcdat/2725423627829963655. New replication is 1
2007-08-07 12:18:27,370 INFO  fs.FSNamesystem (FSNamesystem.java:setReplicationInternal(668)) - Reducing replication for file /srcdat/2725423627829963655. New replication is 1
{noformat}

Fixing this could be trivial.",,
HDFS-3597,"When upgrading from 1.x to 2.0.0, the SecondaryNameNode can fail to start up:
{code}
2012-06-16 09:52:33,812 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -40 namespaceID = 64415959 cTime = 1339813974990 ; clusterId = CID-07a82b97-8d04-4fdd-b3a1-f40650163245 ; blockpoolId = BP-1792677198-172.29.121.67-1339813967723.
Expecting respectively: -19; 64415959; 0; ; .
at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:120)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:454)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:334)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:301)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:438)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:297)
at java.lang.Thread.run(Thread.java:662)
{code}
The error check we're hitting came from HDFS-1073, and it's intended to verify that we're connecting to the correct NN.  But the check is too strict and considers ""different metadata version"" to be the same as ""different clusterID"".

I believe the check in {{doCheckpoint}} simply needs to explicitly check for and handle the update case.",Sadness,-1
HDFS-3824,"{{testHdfsDelegationToken}} fails if not run before {{testSelectHftpDelegationToken}} and {{testSelectHsftpDelegationToken}}:

{noformat}
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.541 sec <<< FAILURE!
testHdfsDelegationToken(org.apache.hadoop.hdfs.TestHftpDelegationToken)  Time elapsed: 0.039 sec  <<< FAILURE!
java.lang.AssertionError: wrong token expected same:<Kind: HDFS_DELEGATION_TOKEN, Service: 127.0.0.1:8020, Ident: > was not:<null>
{noformat}

Debug output:

{noformat}
2012-08-20 18:46:54,742 INFO  fs.FileSystem (HftpFileSystem.java:run(251)) - Couldn't get a delegation token from http://localhost:50470 using http.
2012-08-20 18:46:54,743 DEBUG fs.FileSystem (HftpFileSystem.java:run(254)) - error was
java.io.IOException: Unable to obtain remote token
        at org.apache.hadoop.hdfs.tools.DelegationTokenFetcher.getDTfromRemote(DelegationTokenFetcher.java:239)
        at org.apache.hadoop.hdfs.HftpFileSystem$2.run(HftpFileSystem.java:249)
        at org.apache.hadoop.hdfs.HftpFileSystem$2.run(HftpFileSystem.java:243)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:416)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
        at org.apache.hadoop.hdfs.HftpFileSystem.getDelegationToken(HftpFileSystem.java:243)
        at org.apache.hadoop.hdfs.HftpFileSystem.initDelegationToken(HftpFileSystem.java:196)
        at org.apache.hadoop.hdfs.HftpFileSystem.initialize(HftpFileSystem.java:185)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2284)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:85)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2318)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2300)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:315)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken$1.run(TestHftpDelegationToken.java:131)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken$1.run(TestHftpDelegationToken.java:128)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:416)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
        at org.apache.hadoop.hdfs.TestHftpDelegationToken.testHdfsDelegationToken(TestHftpDelegationToken.java:61)
...
Caused by: java.net.ConnectException: Connection refused
...
{noformat}
",,
HDFS-3828,"{{BlockPoolSliceScanner#scan}} calls cleanUp every time it's invoked from {{DataBlockScanner#run}} via {{scanBlockPoolSlice}}.  But cleanUp unconditionally roll()s the verificationLogs, so after two iterations we have lost the first iteration of block verification times.  As a result a cluster with just one block repeatedly rescans it every 10 seconds:
{noformat}
2012-08-16 15:59:57,884 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
2012-08-16 16:00:07,904 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
2012-08-16 16:00:17,925 INFO  datanode.BlockPoolSliceScanner (BlockPoolSliceScanner.java:verifyBlock(391)) - Verification succeeded for BP-2101131164-172.29.122.91-1337906886255:blk_7919273167187535506_4915
{noformat}
{quote}

To fix this, we need to avoid roll()ing the logs multiple times per period.
",Surprise,-1
HDFS-3919,"A test run hung due to a known system config issue, but the hang was interesting:
{noformat}
2012-09-11 13:22:41,888 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:42,889 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:43,889 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
2012-09-11 13:22:44,890 WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitClusterUp(925)) - Waiting for the Mini HDFS Cluster to start...
{noformat}
The MiniDFSCluster should give up after a few seconds.",Joy,1
HDFS-4006,"TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit, due to an NPE while checkpointing. It looks like the background checkpoint fails, conflicts with the explicit checkpoints done by the tests (note the backtrace is not for the doCheckpoint calls in the tests.

{noformat}
2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit
org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null
stack trace
java.lang.NullPointerException
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)
at java.lang.Thread.run(Thread.java:662)
{noformat}",Surprise,-1
HDFS-4128,"We saw the following issue in a cluster:
- The 2NN downloads an edit log segment:
{code}
2012-10-29 12:30:57,433 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxxxx/current/edits_0000000000049136809-0000000000049176162 expecting start txid #49136809
{code}
- It fails in the middle of replay due to an OOME:
{code}
2012-10-29 12:31:21,021 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, path=/xxxxxxxx
java.lang.OutOfMemoryError: Java heap space
{code}
- Future checkpoints then fail because the prior edit log replay only got halfway through the stream:
{code}
2012-10-29 12:32:21,214 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxx/current/edits_0000000000049176163-0000000000049177224 expecting start txid #49144432
2012-10-29 12:32:21,216 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 49144432, but got txid 49176163.
{code}",,
HDFS-4201,"Saw the following NPE in a log.

Think this is likely due to {{dn}} or {{dn.getFSDataset()}} being null, (not {{bpRegistration}}) due to a configuration or local directory failure.

{code}
2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000
2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)
        at java.lang.Thread.run(Thread.java:722)
{code}",,
HDFS-4426,"After HADOOP-9181 went in, the secondary namenode immediately shuts down after it is started.  From the startup logs:

{noformat}
2013-01-22 19:54:28,826 INFO  namenode.SecondaryNameNode (SecondaryNameNode.java:initialize(299)) - Checkpoint Period   :3600 secs (60 min)
2013-01-22 19:54:28,826 INFO  namenode.SecondaryNameNode (SecondaryNameNode.java:initialize(301)) - Log Size Trigger    :40000 txns
2013-01-22 19:54:28,845 INFO  namenode.SecondaryNameNode (StringUtils.java:run(616)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at xx
************************************************************/
{noformat}

I looked into the issue, and it's shutting down because SecondaryNameNode.main starts a bunch of daemon threads then returns.  With nothing but daemon threads remaining, the JVM sees no reason to keep going and proceeds to shutdown.  Apparently we were implicitly relying on the fact that the HttpServer QueuedThreadPool threads were not daemon threads to keep the secondary namenode process up.",,
HDFS-4813,"TestBlocksWithNotEnoughRacks may fail occasionally due to the bug:
{noformat}
java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1416)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks.__CLR3_0_226e5kj1ntz(TestBlocksWithNotEnoughRacks.java:373)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks.testReduceReplFactorDueToRejoinRespectsRackPolicy(TestBlocksWithNotEnoughRacks.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	...
{noformat}
At the end of the log:
{noformat}
2013-05-10 13:05:53,828 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1413)) - Test resulted in an unexpected exit
org.apache.hadoop.util.ExitUtil$ExitException: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:77)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1153)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1128)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3127)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3089)
	at java.lang.Thread.run(Thread.java:662)
	...
{noformat}",,
HDFS-4841,"Hadoop version:
{code}
bash-4.1$ $HADOOP_HOME/bin/hadoop version
Hadoop 3.0.0-SNAPSHOT
Subversion git://github.com/apache/hadoop-common.git -r d5373b9c550a355d4e91330ba7cc8f4c7c3aac51
Compiled by root on 2013-05-22T08:06Z
From source with checksum 8c4cc9b1e8d6e8361431e00f64483f
This command was run using /var/lib/hadoop-hdfs/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar
{code}

I'm seeing a problem when issuing FsShell commands using the webhdfs:// URI when security is enabled. The command completes but leaves a warning that ShutdownHook 'ClientFinalizer' failed.

{code}
bash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/
2013-05-22 09:46:55,710 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0
Found 3 items
drwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/hbase
drwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/tmp
drwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/user
2013-05-22 09:46:58,660 WARN  [Thread-3] util.ShutdownHookManager (ShutdownHookManager.java:run(56)) - ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook
java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)
	at org.apache.hadoop.security.token.Token.cancel(Token.java:382)
	at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)
	at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)
	at org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}

I've checked that FsShell + hdfs:// commands and WebHDFS operations through curl work successfully:

{code}
bash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls /
2013-05-22 09:46:43,663 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0
Found 3 items
drwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 /hbase
drwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /tmp
drwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /user
bash-4.1$ curl -i --negotiate -u : ""http://hdfs-upgrade-pseudo.ent.cloudera.com:50070/webhdfs/v1/?op=GETHOMEDIRECTORY""
HTTP/1.1 401 
Cache-Control: must-revalidate,no-cache,no-store
Date: Wed, 22 May 2013 16:47:14 GMT
Pragma: no-cache
Date: Wed, 22 May 2013 16:47:14 GMT
Pragma: no-cache
Content-Type: text/html; charset=iso-8859-1
WWW-Authenticate: Negotiate
Set-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMT
Content-Length: 1358
Server: Jetty(6.1.26)

HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 01-Jan-1970 00:00:00 GMT
Date: Wed, 22 May 2013 16:47:14 GMT
Pragma: no-cache
Date: Wed, 22 May 2013 16:47:14 GMT
Pragma: no-cache
Content-Type: application/json
Set-Cookie: hadoop.auth=""u=hdfs&p=hdfs/hdfs-upgrade-pseudo.ent.cloudera.com@ENT.CLOUDERA.COM&t=kerberos&e=1369277234852&s=m3vJ7/pV831tBLkpOBb0Naa5N+g="";Path=/
Transfer-Encoding: chunked
Server: Jetty(6.1.26)

{""Path"":""/user/hdfs""}bash-4.1$ 
{code}

When I disable security, the warning goes away.

I'll attach my core-site.xml, hdfs-site.xml, NN and DN output logs.
","Joy, Surprise","1, -1"
HDFS-4850,"I deployed hadoop-trunk HDFS and created _/user/schu/_. I then forced a checkpoint, fetched the fsimage, and ran the default OfflineImageViewer successfully on the fsimage.

{code}
schu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000004 -o oiv_out_1
schu-mbp:~ schu$ cat oiv_out_1
drwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /
drwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user
drwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user/schu
schu-mbp:~ schu$ 
{code}

I then touched an empty file _/user/schu/testFile1_
{code}
schu-mbp:~ schu$ hadoop fs -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - schu supergroup          0 2013-05-24 16:59 /user
drwxr-xr-x   - schu supergroup          0 2013-05-24 17:00 /user/schu
-rw-r--r--   1 schu supergroup          0 2013-05-24 17:00 /user/schu/testFile1
{code}

and forced another checkpoint, fetched the fsimage, and reran the OfflineImageViewer. I encountered a NegativeArraySizeException:


{code}
schu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000008 -o oiv_out_2
Input ended unexpectedly.
2013-05-24 17:01:13,622 ERROR [main] offlineImageViewer.OfflineImageViewer (OfflineImageViewer.java:go(140)) - image loading failed at offset 402
Exception in thread ""main"" java.lang.NegativeArraySizeException
	at org.apache.hadoop.io.Text.readString(Text.java:458)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)
	at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)
{code}

This is reproducible. I've reproduced this scenario after formatting HDFS and restarting and touching an empty file _/testFile1_.

Attached are the data dirs, the fsimage before creating the empty file (fsimage_0000000000000000004) and the fsimage afterwards (fsimage_0000000000000000004) and their outputs, oiv_out_1 and oiv_out_2 respectively.


The oiv_out_2 does not include the empty _/user/schu/testFile1_.

I don't run into this problem using hadoop-2.0.4-alpha.","Joy, Surprise","1, -1"
HDFS-5185,"DataNode fails to startup if one of the data dirs configured is out of space. 


fails with following exception
{noformat}2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110
java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)
        at java.lang.Thread.run(Thread.java:662)
{noformat}


It should continue to start-up with other data dirs available.",Surprise,-1
HDFS-5291,"In our test, we saw NN immediately went into safemode after transitioning to active state. This can cause HBase region server to timeout and kill itself. We should allow clients to retry when HA is enabled and ANN is in SafeMode.

============================================
Some log snippets:

standby state to active transition
{code}
2013-10-02 00:13:49,482 INFO  ipc.Server (Server.java:run(2068)) - IPC Server handler 69 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from IP:33911 Call#1483 Retry#1: error: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby
2013-10-02 00:13:49,689 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for nn/hostname@EXAMPLE.COM (auth:SIMPLE)
2013-10-02 00:13:49,696 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for nn/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ha.HAServiceProtocol
2013-10-02 00:13:49,700 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1013)) - Stopping services started for standby state
2013-10-02 00:13:49,701 WARN  ha.EditLogTailer (EditLogTailer.java:doWork(336)) - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail
2013-10-02 00:13:49,704 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(885)) - Starting services required for active state
2013-10-02 00:13:49,719 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(419)) - Starting recovery process for unclosed journal segments...
2013-10-02 00:13:49,755 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for hbase/hostname@EXAMPLE.COM (auth:SIMPLE)
2013-10-02 00:13:49,761 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for hbase/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(421)) - Successfully started new epoch 85
2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(249)) - Beginning recovery of unclosed segment starting at txid 887112
2013-10-02 00:13:49,874 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(258)) - Recovery prepare phase complete. Responses:
IP:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530
172.18.145.97:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530
2013-10-02 00:13:49,875 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recover
{code}


And then we get into safemode

{code}
Construction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,277 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[IP:1019|RBW], ReplicaUnderConstruction[172.18.145.96:1019|RBW], ReplicaUnde
rConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,279 INFO  hdfs.StateChange (FSNamesystem.java:reportStatus(4703)) - STATE* Safe mode ON.
The reported blocks 1071 needs additional 5 blocks to reach the threshold 1.0000 of total blocks 1075.
Safe mode will be turned off automatically
2013-10-02 00:13:50,279 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,280 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.99:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,281 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.97:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
{code}","Fear, Surprise",-1
HDFS-5322,"While running HA tests we have seen issues were we see HDFS delegation token not found in cache errors causing jobs running to fail.

{code}
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
|2013-10-06 20:14:51,193 INFO  [main] mapreduce.Job: Task Id : attempt_1381090351344_0001_m_000007_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache
at org.apache.hadoop.ipc.Client.call(Client.java:1347)
at org.apache.hadoop.ipc.Client.call(Client.java:1300)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
{code}",,
HDFS-5710,"From https://builds.apache.org/job/hbase-0.96-hadoop2/166/testReport/junit/org.apache.hadoop.hbase.mapreduce/TestTableInputFormatScan1/org_apache_hadoop_hbase_mapreduce_TestTableInputFormatScan1/ :
{code}
2014-01-01 00:10:15,571 INFO  [IPC Server handler 2 on 50198] blockmanagement.BlockManager(1009): BLOCK* addToInvalidates: blk_1073741967_1143 127.0.0.1:40188 127.0.0.1:46149 127.0.0.1:41496 
2014-01-01 00:10:16,559 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] namenode.FSDirectory(1854): Could not get full path. Corresponding file might have deleted already.
2014-01-01 00:10:16,560 FATAL [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] blockmanagement.BlockManager$ReplicationMonitor(3127): ReplicationMonitor thread received Runtime exception. 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)
	at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)
	at java.lang.Thread.run(Thread.java:724)
{code}
Looks like getRelativePathINodes() returned null but getFullPathName() didn't check inodes against null, leading to NPE.",Surprise,-1
HDFS-6102,"Found by [~schu] during testing. We were creating a bunch of directories in a single directory to blow up the fsimage size, and it ends up we hit this error when trying to load a very large fsimage:

{noformat}
2014-03-13 13:57:03,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 24523605 INodes.
2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)
com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)
        at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)
        at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)
        at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)
        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)
        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)
        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)
        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)
        at 52)
...
{noformat}

Some further research reveals there's a 64MB max size per PB message, which seems to be what we're hitting here.",Surprise,-1
HDFS-6178,"Currently decommissioning machines in HA-enabled cluster requires running refreshNodes in both active and standby nodes. Sometimes decommissioning won't finish from standby NN's point of view.  Here is the diagnosis of why it could happen.

Standby NN's blockManager manages blocks replication and block invalidation as if it is the active NN; even though DNs will ignore block commands coming from standby NN. When standby NN makes block operation decisions such as the target of block replication and the node to remove excess blocks from, the decision is independent of active NN. So active NN and standby NN could have different states. When we try to decommission nodes on standby nodes; such state inconsistency might prevent standby NN from making progress. Here is an example.

Machine A
Machine B
Machine C
Machine D
Machine E
Machine F
Machine G
Machine H

1. For a given block, both active and standby have 5 replicas on machine A, B, C, D, E. So both active and standby decide to pick excess nodes to invalidate.

Active picked D and E as excess DNs. After the next block reports from D and E, active NN has 3 active replicas (A, B, C), 0 excess replica.

{noformat}
2014-03-27 01:50:14,410 INFO BlockStateChange: BLOCK* chooseExcessReplicates: (E:50010, blk_-5207804474559026159_121186764) is added to invalidated blocks set
2014-03-27 01:50:15,539 INFO BlockStateChange: BLOCK* chooseExcessReplicates: (D:50010, blk_-5207804474559026159_121186764) is added to invalidated blocks set
{noformat}

Standby pick C, E as excess DNs. Given DNs ignore commands from standby, After the next block reports from C, D, E,  standby has 2 active replicas (A, B), 1 excess replica (C).

{noformat}
2014-03-27 01:51:49,543 INFO BlockStateChange: BLOCK* chooseExcessReplicates: (E:50010, blk_-5207804474559026159_121186764) is added to invalidated blocks set
2014-03-27 01:51:49,894 INFO BlockStateChange: BLOCK* chooseExcessReplicates: (C:50010, blk_-5207804474559026159_121186764) is added to invalidated blocks set
{noformat}


2. Machine A decomm request was sent to standby. Standby only had one live replica and picked machine G, H as targets, but given standby commands was ignored by DNs, G, H remained in pending replication queue until they are timed out. At this point, you have one decommissioning replica (A), 1 active replica (B), one excess replica (C).
{noformat}
2014-03-27 04:42:52,258 INFO BlockStateChange: BLOCK* ask A:50010 to replicate blk_-5207804474559026159_121186764 to datanode(s) G:50010 H:50010
{noformat}

3. Machine A decomm request was sent to active NN. Active NN picked machine F as the target. It finished properly. So active NN had 3 active replicas (B, C, F), one decommissioned replica (A).

{noformat}
2014-03-27 04:44:15,239 INFO BlockStateChange: BLOCK* ask 10.42.246.110:50010 to replicate blk_-5207804474559026159_121186764 to datanode(s) F:50010
2014-03-27 04:44:16,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: F:50010 is added to blk_-5207804474559026159_121186764 size 7100065
{noformat}

4. Standby NN picked up F as a new replica. Thus standby had one decommissioning replica (A), 2 active replicas (B, F), one excess replica (C). Standby NN kept trying to schedule replication work, but DNs ignored the commands.

{noformat}
2014-03-27 04:44:16,084 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: F:50010 is added to blk_-5207804474559026159_121186764 size 7100065

2014-03-28 23:06:11,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block: blk_-5207804474559026159_121186764, Expected Replicas: 3, live replicas: 2, corrupt replicas: 0, decommissioned replicas: 1, excess replicas: 1, Is Open File: false, Datanodes having this block: C:50010 B:50010 A:50010 F:50010 , Current Datanode: A:50010, Is current datanode decommissioning: true
{noformat}",Surprise,-1
HDFS-62,"I have set up {{dfs.secondary.http.address}} like this:

{code}
<property>
  <name>dfs.secondary.http.address</name>
  <value>secondary.example.com:50090</value>
</property>
{code}

In my setup {{secondary.example.com}} resolves to an IP address (say, 192.168.0.10) which is not the same as the host's name (as returned by {{InetAddress.getLocalHost().getHostAddress()}}, say 192.168.0.1).

In this situation, edit log related transfers fail. From the namenode log:

{code}
2009-04-05 13:32:39,128 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.0.10
2009-04-05 13:32:39,168 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at java.net.Socket.connect(Socket.java:469)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:163)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:394)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:529)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:233)
        at sun.net.www.http.HttpClient.New(HttpClient.java:306)
        at sun.net.www.http.HttpClient.New(HttpClient.java:323)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:837)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:778)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:703)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1026)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        ...
{code}

From the secondary namenode log:

{code}
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint: 
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.FileNotFoundException: http://nn.example.com:50070/getimage?putimage=1&port=50090&machine=
192.168.0.1&token=-19:1243068779:0:1238929357000:1238929031783
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1288)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(SecondaryNameNode.java:294)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:333)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:239)
        at java.lang.Thread.run(Thread.java:619)
{code}",,
HDFS-6348,"Secondary Namenode is not exiting when there is RuntimeException occurred during startup.

Say I configured wrong configuration, due to that validation failed and thrown RuntimeException as shown below. But when I check the environment SecondaryNamenode process is alive. When analysed, RMI Thread is still alive, since it is not a daemon thread JVM is nit exiting. 

I'm attaching threaddump to this JIRA for more details about the thread.
{code}
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)
	... 6 more
Caused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)
	... 7 more
2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state
2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state
2014-05-07 14:31:04,926 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
{code}

","Sadness, Surprise",-1
HDFS-6462,"Fsstat fails in secure environment with below error.

Steps to reproduce:
1) Create user named UserB and UserA
2) Create group named GroupB
3) Add root and UserB users to GroupB
    Make sure UserA is not in GroupB
4) Set below properties
{noformat}
===================================
hdfs-site.xml
===================================
 <property>
    <name>dfs.nfs.keytab.file</name>
    <value>/tmp/keytab/UserA.keytab</value>
  </property>
  <property>
    <name>dfs.nfs.kerberos.principal</name>
    <value>UserA@EXAMPLE.COM</value>
  </property>
==================================
core-site.xml
==================================
<property>
    <name>hadoop.proxyuser.UserA.groups</name>
   <value>GroupB</value>
 </property>
<property>
   <name>hadoop.proxyuser.UserA.hosts</name>
   <value>*</value>
 </property>
{noformat}
4) start nfs server as UserA
5) mount nfs as root user
6) run below command 
{noformat}
[root@host1 ~]# df /tmp/tmp_mnt/
df: `/tmp/tmp_mnt/': Input/output error
df: no file systems processed
{noformat}

NFS Logs complains as below
{noformat}
2014-05-29 00:09:13,698 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1654)) - NFS FSSTAT fileId: 16385
2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
2014-05-29 00:09:13,710 WARN  nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1681)) - Exception
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: ""host1/0.0.0.0""; destination host is: ""host1"":8020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1414)
        at org.apache.hadoop.ipc.Client.call(Client.java:1363)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)
        at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)
        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)
        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)
        at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)
        at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)
        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)
        at org.apache.hadoop.ipc.Client.call(Client.java:1381)
        ... 42 more
{noformat}",Anger,-1
HDFS-6481,"Ian Brooks reported the following stack trace:
{code}
2014-06-03 13:05:03,915 WARN  [DataStreamer for file /user/hbase/WALs/############,16020,1401716790638/############%2C16020%2C1401716790638.1401796562200 block BP-2121456822-10.143.38.149-1396953188241:blk_1074073683_332932] hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 0
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeManager.java:467)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:2779)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:594)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:430)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)

        at org.apache.hadoop.ipc.Client.call(Client.java:1347)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy13.getAdditionalDatanode(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolTranslatorPB.java:352)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy14.getAdditionalDatanode(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:266)
        at com.sun.proxy.$Proxy15.getAdditionalDatanode(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:919)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:919)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1031)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:823)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:475)
2014-06-03 13:05:48,489 ERROR [RpcServer.handler=22,port=16020] wal.FSHLog: syncer encountered error, will retry. txid=211
org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 0
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeManager.java:467)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:2779)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:594)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:430)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)

        at org.apache.hadoop.ipc.Client.call(Client.java:1347)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy13.getAdditionalDatanode(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolTranslatorPB.java:352)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy14.getAdditionalDatanode(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:266)
        at com.sun.proxy.$Proxy15.getAdditionalDatanode(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:919)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1031)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:823)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:475)
{code}
The loop is controlled by the length of datanodeID:

{code}
    for(int i = 0; i < datanodeID.length; i++) {
      final DatanodeDescriptor dd = getDatanode(datanodeID[i]);
      storages[i] = dd.getStorageInfo(storageIDs[i]);
    }
{code}
However, when the length of storageIDs is shorter than that of datanodeID, we would get ArrayIndexOutOfBoundsException.",Surprise,-1
HDFS-6797,"Before upgrade, data node version was -55. The new data node version remained at -55. During upgrade we got he following messages:

{code}
2014-07-15 12:59:55,253 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -56
...
...
...
...
2014-07-15 12:59:56,479 INFO org.apache.hadoop.hdfs.server.common.Storage: Upgrading block pool storage directory /hadoop/1/data1/current/BP-825373266-xx.xxx.xxx.xx-1379095203239.
   old LV = -55; old CTime = 1402508907789.
   new LV = -56; new CTime = 1405453914270
2014-07-15 13:00:07,697 INFO org.apache.hadoop.hdfs.server.common.Storage: Upgrade of block pool BP-825373266-xx.xxx.xxx.xx-1379095203239 at /hadoop/12/data1/current/BP-825373266-xx.xxx.xxx.xx-1379095203239 is complete
2014-07-15 13:00:07,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=859725752;bpid=BP-825373266-xx.xxx.xxx.xx-1379095203239;lv=-55;nsInfo=lv=-56;cid=CID-303ee504-e03c-4a5e-bc59-2b275b308152;nsid=859725752;c=1405453914270;bpid=BP-82537326
6-xx.xxx.xxx.xx-1379095203239;dnuuid=b1011b87-d7cd-48ce-92cc-f7cca0e8cbae
{code}

after upgrade completing, restart of DN still shows message regarding version difference:
{code}
INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -56
{code}
This causes confusion to the operators as if upgrade did not succeed since data node's layout version is not updated to the ""new LV"" value

Actually name node's layout version is displayed as the ""new LV"" value.
Since the data node and name node layout versions are separate now, the new data node layout version should be shown as the new LV.  

Thanks to [~ehf] who found and reported this issue.

","Love, Sadness, Surprise","1, -1"
HDFS-6823,"Starting the namenode on a system not running Kerberos results in the following showing up in the log:

{code}
2014-08-05 15:02:33,747 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use aw-mbp-work.local:9000 to access this namenode/service.
2014-08-05 15:02:33,831 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-08-05 15:02:33,948 INFO org.apache.hadoop.hdfs.DFSUtil: Starting web server as: ${dfs.web.authentication.kerberos.principal}
2014-08-05 15:02:33,948 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://aw-mbp-work.local:50070
{code}

If Kerberos isn't configured, we shouldn't try to display the principal message, never mind the raw text of the configuration option.",Surprise,-1
HDFS-7236,"Per the following report
{code}
****Recently FAILED builds in url: https://builds.apache.org/job/Hadoop-Hdfs-trunk
    THERE ARE 4 builds (out of 5) that have failed tests in the past 7 days, as listed below:

===>https://builds.apache.org/job/Hadoop-Hdfs-trunk/1898/testReport (2014-10-11 04:30:40)
    Failed test: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing.testQueueingWithAppend
    Failed test: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.testFencingStress
    Failed test: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot.testOpenFilesWithMultipleSnapshots
===>https://builds.apache.org/job/Hadoop-Hdfs-trunk/1897/testReport (2014-10-10 04:30:40)
    Failed test: org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode.testDeadDatanode
    Failed test: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing.testQueueingWithAppend
    Failed test: org.apache.hadoop.tracing.TestTracing.testReadTraceHooks
    Failed test: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot.testOpenFilesWithMultipleSnapshots
    Failed test: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.testFencingStress
    Failed test: org.apache.hadoop.tracing.TestTracing.testWriteTraceHooks
...
Among 5 runs examined, all failed tests <#failedRuns: testName>:
    4: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.testFencingStress
    2: org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing.testQueueingWithAppend
    2: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot.testOpenFilesWithMultipleSnapshots
    1: org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode.testDeadDatanode
...
{code}

TestOpenFilesWithSnapshot.testOpenFilesWithMultipleSnapshots failed in most recent two runs in trunk. Creating this jira for it (The other two tests that failed more often were reported in separate jira HDFS-7221 and HDFS-7226)

Symptom:

{code}
Error Message

Timed out waiting for Mini HDFS Cluster to start
Stacktrace

java.io.IOException: Timed out waiting for Mini HDFS Cluster to start
	at org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1194)
	at org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:1819)
	at org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:1789)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot.doTestMultipleSnapshots(TestOpenFilesWithSnapshot.java:184)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot.testOpenFilesWithMultipleSnapshots(TestOpenFilesWithSnapshot.java:162)
{code}

AND

{code}
2014-10-11 12:38:24,385 ERROR datanode.DataNode (DataXceiver.java:run(243)) - 127.0.0.1:55303:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:32949 dst: /127.0.0.1:55303
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:196)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:468)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:772)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:720)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)
	at java.lang.Thread.run(Thread.java:662)
{code}

AND

{code}
2014-10-11 12:38:28,552 WARN  datanode.DataNode (BPServiceActor.java:offerService(751)) - RemoteException in offerService
org.apache.hadoop.ipc.RemoteException(java.io.IOException): Got incremental block report from unregistered or dead node
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processIncrementalBlockReport(BlockManager.java:3021)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.processIncrementalBlockReport(FSNamesystem.java:6355)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReceivedAndDeleted(NameNodeRpcServer.java:1137)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReceivedAndDeleted(DatanodeProtocolServerSideTranslatorPB.java:209)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26304)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:639)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:966)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2125)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2121)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1640)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2119)

	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy18.blockReceivedAndDeleted(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReceivedAndDeleted(DatanodeProtocolClientSideTranslatorPB.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks(BPServiceActor.java:307)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:711)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:662)
{code}

",,
HDFS-7305,"{code}
2014-10-23 10:22:49,066 WARN [main] org.apache.hadoop.mapred.Task: Task status: ""Failed at running due to java.lang.NullPointerException
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:358)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:91)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:529)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:605)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.append(WebHdfsFileSystem.java:1154)
	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1163)
	at org.apache.hadoop.fs.slive.AppendOp.run(AppendOp.java:80)
	at org.apache.hadoop.fs.slive.ObserveableOp.run(ObserveableOp.java:63)
	at org.apache.hadoop.fs.slive.SliveMapper.runOperation(SliveMapper.java:122)
	at org.apache.hadoop.fs.slive.SliveMapper.map(SliveMapper.java:168)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
"" truncated to max limit (512 characters)

Activity
{code}",,
HDFS-7916,"if any badblock found, then BPSA for StandbyNode will go for infinite times to report it.

{noformat}2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010
org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:
        at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
",,
HDFS-7996,"When removing a disk from an actively writing DataNode, the BlockReceiver working on the disk throws {{ReplicaNotFoundException}} because the replicas are removed from the memory:

{code}
2015-03-26 08:02:43,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removed volume: /data/2/dfs/dn/current
2015-03-26 08:02:43,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Removing block level storage: /data/2/dfs/dn/current/BP-51301509-10.20.202.114-1427296597742
2015-03-26 08:02:43,163 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)
        at java.lang.Thread.run(Thread.java:745)
{code}

{{FsVolumeList#removeVolume}} waits all threads release {{FsVolumeReference}} on the volume to be removed, however, in {{PacketResponder#finalizeBlock()}}, it calls

{code}
private void finalizeBlock(long startTime) throws IOException {
      BlockReceiver.this.close();
      final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime()
          : 0;
      block.setNumBytes(replicaInfo.getNumBytes());
      datanode.data.finalizeBlock(block);
{code}

The {{FsVolumeReference}} was released in {{BlockReceiver.this.close()}} before calling {{datanode.data.finalizeBlock(block)}}.",Surprise,-1
HDFS-8173,"NPE thrown at Datanode startup --

{code}
2015-04-17 17:37:01,069 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exception shutting down DataNode HttpServer
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1703)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:433)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2392)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2279)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2326)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2503)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2527)
{code}",,
HDFS-8276,"bq. but I think it is simple enough to change the meaning of the value so that zero means 'never scrub'. Let me post an updated patch.

As discussed in [HDFS-6929|https://issues.apache.org/jira/browse/HDFS-6929], scrubber should be disable if *dfs.namenode.lazypersist.file.scrub.interval.sec* is zero.

Currently namenode startup is failing if interval configured zero

{code}
2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.
java.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)
{code}",,
HDFS-872,"After upgrading to that latest HDFS 0.20.2 (r896310 from /branches/branch-0.20), old DFS clients (0.20.1) seem to not work anymore. HBase uses the 0.20.1 hadoop core jars and the HBase master will no longer startup. Here is the exception from the HBase master log:

{code}
2010-01-06 09:59:46,762 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read: java.io.IOException: Could not obtain block: blk_338051
2596555557728_1002 file=/hbase/hbase.version
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1788)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1616)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1743)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1673)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:189)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:208)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:208)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1241)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1282)

2010-01-06 09:59:46,763 FATAL org.apache.hadoop.hbase.master.HMaster: Not starting HMaster because:
java.io.IOException: Could not obtain block: blk_3380512596555557728_1002 file=/hbase/hbase.version
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:1788)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1616)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1743)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1673)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:189)
        at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:208)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:208)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1241)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1282)
{code}

If I switch the hadoop jars in the hbase/lib directory with 0.20.2 version it works well, which what led me to open this bug here and not in the HBASE project.",Surprise,-1
HDFS-8807,"if you add a space between the storage type and file URI then datanodes fail during startup.
Here is an example of ""mis-configration"" that leads to datanode failure.
{code}
<property>
    <name>dfs.datanode.data.dir</name>
    <value>
      [DISK] file://tmp/hadoop-aengineer/disk1/dfs/data
    </value>
  </property>
{code}
Here is the ""fixed"" version. Please *note* the lack of space between \[DISK\] and file URI
{code}
<property>
    <name>dfs.datanode.data.dir</name>
    <value>
      [DISK]file://tmp/hadoop-aengineer/disk1/dfs/data
    </value>
  </property>
{code}
we fail with a parsing error, here is the info from the datanode logs.
{code}
2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data
        at org.apache.hadoop.fs.Path.initialize(Path.java:204)
        at org.apache.hadoop.fs.Path.<init>(Path.java:170)
        at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)
Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data
        at java.net.URI$Parser.fail(URI.java:2829)
        at java.net.URI$Parser.checkChars(URI.java:3002)
        at java.net.URI$Parser.checkChar(URI.java:3012)
        at java.net.URI$Parser.parse(URI.java:3028)
        at java.net.URI.<init>(URI.java:753)
        at org.apache.hadoop.fs.Path.initialize(Path.java:201)
        ... 7 more
{code}",,
HDFS-9624,"It seems starting datanode so slowly when I am finishing migration of datanodes and restart them.I look the dn logs:
{code}
2016-01-06 16:05:08,118 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-70097061-42f8-4c33-ac27-2a6ca21e60d4
2016-01-06 16:05:08,118 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/data/data/hadoop/dfs/data/data12/current, StorageType: DISK
2016-01-06 16:05:08,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2016-01-06 16:05:08,177 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1942012336-xx.xx.xx.xx-1406726500544
2016-01-06 16:05:08,178 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data2/current...
2016-01-06 16:05:08,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data3/current...
2016-01-06 16:05:08,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data4/current...
2016-01-06 16:05:08,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data5/current...
2016-01-06 16:05:08,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data6/current...
2016-01-06 16:05:08,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data7/current...
2016-01-06 16:05:08,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data8/current...
2016-01-06 16:05:08,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data9/current...
2016-01-06 16:05:08,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data10/current...
2016-01-06 16:05:08,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data11/current...
2016-01-06 16:05:08,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on volume /home/data/data/hadoop/dfs/data/data12/current...
2016-01-06 16:09:49,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data7/current: 281466ms
2016-01-06 16:09:54,235 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data9/current: 286054ms
2016-01-06 16:09:57,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data2/current: 289680ms
2016-01-06 16:10:00,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data5/current: 292153ms
2016-01-06 16:10:05,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data8/current: 297516ms
2016-01-06 16:10:11,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data6/current: 303049ms
2016-01-06 16:10:28,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data12/current: 319894ms
2016-01-06 16:10:33,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data4/current: 324838ms
2016-01-06 16:10:40,177 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data10/current: 331996ms
2016-01-06 16:10:44,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data3/current: 336703ms
2016-01-06 16:11:14,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1942012336-xx.xx.xx.xx-1406726500544 on /home/data/data/hadoop/dfs/data/data11/current: 366060ms
2016-01-06 16:11:14,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1942012336-xx.xx.xx.xx-1406726500544: 366065ms
{code}
And I know that Scanning blocks on volume and then calculating the dfsUsed costs the most of time. Because my datanode's migiration costs the much time, so that dfsUsed value can't use cache-dfsused and should be doing du operations. But actually I don't need do it again because there has no operations in these datanodes. The info is these:
{code}
/**
   * Read in the cached DU value and return it if it is less than 600 seconds
   * old (DU update interval). Slight imprecision of dfsUsed is not critical and
   * skipping DU can significantly shorten the startup time. If the cached value
   * is not available or too old, -1 is returned.
   * */
{code}
The 600 seconds is a dead code. And it looks not suitable for here.
","Sadness, Surprise",-1
HIVE-10690,"Noticed a bunch of these stack traces in hive.log while running some unit tests:

{noformat}
2015-05-11 21:18:59,371 WARN  [main]: metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2420)) - Direct SQL failed
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.rangeCheck(ArrayList.java:635)
        at java.util.ArrayList.get(ArrayList.java:411)
        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions(MetaStoreDirectSql.java:1132)
        at org.apache.hadoop.hive.metastore.ObjectStore$8.getSqlResult(ObjectStore.java:6162)
        at org.apache.hadoop.hive.metastore.ObjectStore$8.getSqlResult(ObjectStore.java:6158)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2385)
        at org.apache.hadoop.hive.metastore.ObjectStore.get_aggr_stats_for(ObjectStore.java:6158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
        at com.sun.proxy.$Proxy84.get_aggr_stats_for(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_aggr_stats_for(HiveMetaStore.java:5662)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy86.get_aggr_stats_for(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2064)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy87.getAggrColStatsFor(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3110)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:245)
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.updateColStats(RelOptHiveTable.java:329)
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.getColStat(RelOptHiveTable.java:399)
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.getColStat(RelOptHiveTable.java:392)
        at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.getColStat(HiveTableScan.java:150)
        at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:77)
        at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:64)
        at sun.reflect.GeneratedMethodAccessor296.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ReflectiveRelMetadataProvider$1$1.invoke(ReflectiveRelMetadataProvider.java:182)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.CachingRelMetadataProvider$CachingInvocationHandler.invoke(CachingRelMetadataProvider.java:132)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.CachingRelMetadataProvider$CachingInvocationHandler.invoke(CachingRelMetadataProvider.java:132)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getDistinctRowCount(RelMetadataQuery.java:469)
        at org.apache.calcite.rel.metadata.RelMdDistinctRowCount.getDistinctRowCount(RelMdDistinctRowCount.java:241)
        at sun.reflect.GeneratedMethodAccessor300.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ReflectiveRelMetadataProvider$1$1.invoke(ReflectiveRelMetadataProvider.java:182)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.CachingRelMetadataProvider$CachingInvocationHandler.invoke(CachingRelMetadataProvider.java:132)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.ChainedRelMetadataProvider$ChainedInvocationHandler.invoke(ChainedRelMetadataProvider.java:109)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.calcite.rel.metadata.CachingRelMetadataProvider$CachingInvocationHandler.invoke(CachingRelMetadataProvider.java:132)
        at com.sun.proxy.$Proxy108.getDistinctRowCount(Unknown Source)
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getDistinctRowCount(RelMetadataQuery.java:469)
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.computeJoinCardinality(LoptOptimizeJoinRule.java:615)
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.getBestNextFactor(LoptOptimizeJoinRule.java:808)
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.createOrdering(LoptOptimizeJoinRule.java:706)
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.findBestOrderings(LoptOptimizeJoinRule.java:458)
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.onMatch(LoptOptimizeJoinRule.java:128)
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:326)
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:515)
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:392)
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
        at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:125)
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:207)
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:194)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:856)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:768)
        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:109)
        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:730)
        at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:145)
        at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:105)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:607)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:244)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10051)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)
        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)
        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:1234)
        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_auto_sortmerge_join_1(TestMiniTezCliDriver.java:166)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at junit.framework.TestCase.runTest(TestCase.java:176)
        at junit.framework.TestCase.runBare(TestCase.java:141)
        at junit.framework.TestResult$1.protect(TestResult.java:122)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at junit.framework.TestResult.run(TestResult.java:125)
        at junit.framework.TestCase.run(TestCase.java:129)
        at junit.framework.TestSuite.runTest(TestSuite.java:255)
        at junit.framework.TestSuite.run(TestSuite.java:250)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
{noformat}",,
HIVE-10736,"The shutdown process throws concurrent modification exceptions and fails to clean up the app masters per queue.

{code}
2015-05-17 20:24:00,464 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:OperationManager is stopped.
2015-05-17 20:24:00,464 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:SessionManager is stopped.
2015-05-17 20:24:00,464 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true
2015-05-17 20:24:00,465 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:CLIService is stopped.
2015-05-17 20:24:00,465 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:HiveServer2 is stopped.
2015-05-17 20:24:00,465 INFO  [Thread-6()]: tez.TezSessionState (TezSessionState.java:close(332)) - Closing Tez Session
2015-05-17 20:24:00,466 INFO  [Thread-6()]: client.TezClient (TezClient.java:stop(495)) - Shutting down Tez Session, sessionName=HIVE-94cc629d-63bc-490a-a135-af85c0cc0f2e, applicationId=application_1431919257083_0012
2015-05-17 20:24:00,570 ERROR [Thread-6()]: server.HiveServer2 (HiveServer2.java:stop(322)) - Tez session pool manager stop had an error during stop of HiveServer2. Shutting down HiveServer2 anyway.
java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)
        at java.util.LinkedList$ListItr.next(LinkedList.java:888)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)
        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)
        at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)
{code}",,
HIVE-10801,"When trying to drop a view, hive log shows:
{code}
2015-05-21 11:53:06,126 ERROR [HiveServer2-Background-Pool: Thread-197]: hdfs.KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy8.dropTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)
	... 40 more

2015-05-21 11:53:06,135 ERROR [HiveServer2-Background-Pool: Thread-197]: exec.DDLTask (DDLTask.java:failed(520)) - org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy8.dropTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)
	... 40 more

2015-05-21 11:53:06,135 INFO  [HiveServer2-Background-Pool: Thread-197]: hooks.ATSHook (ATSHook.java:<init>(84)) - Created ATS Hook
2015-05-21 11:53:06,136 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>
2015-05-21 11:53:06,136 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1432209186136 end=1432209186136 duration=0 from=org.apache.hadoop.hive.ql.Driver>
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)
2015-05-21 11:53:06,136 ERROR [HiveServer2-Background-Pool: Thread-197]: ql.Driver (SessionState.java:printError(957)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)
2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=Driver.execute start=1432209185565 end=1432209186137 duration=572 from=org.apache.hadoop.hive.ql.Driver>
2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1432209186137 end=1432209186137 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2015-05-21 11:53:06,139 ERROR [HiveServer2-Background-Pool: Thread-197]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)
	at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)
	... 11 more
Caused by: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy8.dropTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)
	... 40 more
{code}

The following code in HiveMetaStore seems to have caused this issue :
{code}
        if(!ifPurge) {
.....
            if (shim.isPathEncrypted(tblPath)) {
              throw new MetaException(""Unable to drop table because it is in an encryption zone"" +
                "" and trash is enabled.  Use PURGE option to skip trash."");
            }
          }
        }
{code}
It seems that the tblPath is still null when shims.isPathEncrypted is called.

Thanks to [~asreekumar] for uncovering this issue !
",Love,1
HIVE-11301,"On metastore side it looks like this:
{noformat}
2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)
        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

and then
{noformat}
2015-07-17 20:32:27,796 WARN  [pool-3-thread-150]: transport.TIOStreamTransport (TIOStreamTransport.java:close(112)) - Error closing output stream.
java.net.SocketException: Socket closed
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:153)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)
        at org.apache.thrift.transport.TSocket.close(TSocket.java:196)
        at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}


Which on client manifests as
{noformat}
2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.
org.apache.thrift.transport.TTransportException
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)
        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(Stat
sRulesProcFactory.java:111)
{noformat}

and CLI hangs for a really long time while this thing is retrying.",Surprise,-1
HIVE-11303,"{noformat}
2015-07-17 18:18:11,830 INFO  [main]: counters.Limits (Limits.java:ensureInitialized(59)) - Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=500, COUNTER_NAME_MAX=64, MAX_COUNTERS=1200
2015-07-17 18:18:11,841 ERROR [main]: exec.Task (TezTask.java:execute(189)) - Failed to execute tez graph.
org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200
        at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)
        at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)
        at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)
        at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)
        at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)
        at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)
        at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)
        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}",,
HIVE-11540,"Hello,

I am streaming weblogs to Kafka and then to Flume 1.6 using a Hive sink, with an average of 20 million records a day. I have 5 compactors running at various times (30m/5m/5s), no matter what time I give, the compactors seem to run out of memory cleaning up a couple thousand delta files and ultimately falls behind compacting/cleaning delta files. Any suggestions on what I can do to improve performance? Or can Hive streaming not handle this kind of load?

I used this post as reference: http://henning.kropponline.de/2015/05/19/hivesink-for-flume/

{noformat}
2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory

Max block location exceeded for split: CompactorInputSplit{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406, bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, delta_1056411_1056412, delta_1056413_1056414, delta_1056415_1056416, delta_1056417_1056418,
, delta_1074039_1074040, delta_1074041_1074042, delta_1074043_1074044, delta_1074045_1074046, delta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]} splitsize: 8772 maxsize: 10
2015-08-12 15:34:25,271 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:3
2015-08-12 15:34:25,367 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1439397150426_0068
2015-08-12 15:34:25,603 INFO  [upladevhwd04v.researchnow.com-18]: impl.YarnClientImpl (YarnClientImpl.java:submitApplication(274)) - Submitted application application_1439397150426_0068
2015-08-12 15:34:25,610 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://upladevhwd02v.researchnow.com:8088/proxy/application_1439397150426_0068/
2015-08-12 15:34:25,611 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1439397150426_0068
2015-08-12 15:34:30,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:33,756 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1439397150426_0068 running in uber mode : false
2015-08-12 15:34:33,757 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%
2015-08-12 15:34:35,147 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:40,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:45,184 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:50,201 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:55,256 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:00,205 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:02,975 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 33% reduce 0%
2015-08-12 15:35:02,982 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_0, Status : FAILED
2015-08-12 15:35:03,000 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_0, Status : FAILED
2015-08-12 15:35:04,008 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%
2015-08-12 15:35:05,132 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:10,206 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:15,228 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:20,207 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:25,148 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:28,154 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_1, Status : FAILED
2015-08-12 15:35:29,161 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_1, Status : FAILED
2015-08-12 15:35:30,142 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:35,140 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:40,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:45,153 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:50,150 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:52,268 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_2, Status : FAILED
2015-08-12 15:35:53,274 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_2, Status : FAILED
2015-08-12 15:35:55,149 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:00,160 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:05,145 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:10,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:15,158 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:17,397 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%
2015-08-12 15:36:18,409 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_1439397150426_0068 failed with state FAILED due to: Task failed task_1439397150426_0068_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0

2015-08-12 15:36:18,443 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 10
	Job Counters 
		Failed map tasks=7
		Killed map tasks=1
		Launched map tasks=8
		Other local map tasks=6
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=191960
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=191960
		Total vcore-seconds taken by all map tasks=191960
		Total megabyte-seconds taken by all map tasks=884551680
2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)

2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327)) - Expected to remove at least one row from completed_txn_components when marking compaction entry as clean!
^C
{noformat}
[ngmathew@upladevhwd04v ~]$ tail -f /var/log/hive/hivemetastore.log
2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)



Settings:
hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
hive.compactor.initiator.on = true
hive.compactor.worker.threads = 5
Table stored as ORC
hive.vectorized.execution.enabled = false
hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat",Sadness,-1
HIVE-11902,"When cleaning left over transactions we see the DeadTxnReaper code threw the following exception:
{noformat}
2015-09-21 05:23:38,148 WARN  [DeadTxnReaper-0]: txn.TxnHandler (TxnHandler.java:performTimeOuts(1876)) - Aborting timedout transactions failed due to You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1(SQLState=42000,ErrorCode=1064)
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
        at com.mysql.jdbc.Util.getInstance(Util.java:360)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)
        at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)
        at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
The problem here is that the method {{abortTxns(Connection dbConn, List<Long> txnids)}} in metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java creates the following bad query when txnids list is empty.
{code}
delete from HIVE_LOCKS where hl_txnid in ();
{code}
",Sadness,-1
HIVE-12364,"PROBLEM:

insert into/overwrite directory '/path' invokes distcp for moveTask and fails
query when execution engine is Tez 

set hive.exec.copyfile.maxsize=40000;
insert overwrite into '/tmp/testinser' select * from customer;
failed at moveTask
hive client log:
{code}
2015-11-05 16:02:53,254 INFO  [main]: exec.FileSinkOperator (Utilities.java:mvFileToFinalPath(1882)) - Moving tmp dir: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/_tmp.-ext-10000 to: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000
2015-11-05 16:02:53,611 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=task.DEPENDENCY_COLLECTION.Stage-2 from=org.apache.hadoop.hive.ql.Driver>
2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode
2015-11-05 16:02:53,612 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=task.MOVE.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-0:MOVE] in serial mode
2015-11-05 16:02:53,612 INFO  [main]: exec.Task (SessionState.java:printInfo(951)) - Moving data to: /tmp/testindir from hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000
2015-11-05 16:02:53,637 INFO  [main]: common.FileUtils (FileUtils.java:copy(551)) - Source is 491763261 bytes. (MAX: 40000)
2015-11-05 16:02:53,638 INFO  [main]: common.FileUtils (FileUtils.java:copy(552)) - Launch distributed copy (distcp) job.
2015-11-05 16:03:03,924 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/
2015-11-05 16:03:04,081 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/
2015-11-05 16:03:20,210 INFO  [main]: hdfs.DFSClient (DFSClient.java:getDelegationToken(1047)) - Created HDFS_DELEGATION_TOKEN token 1069 for haha on ha-hdfs:hdpsecehdfs
2015-11-05 16:03:20,249 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hdpsecehdfs, Ident: (HDFS_DELEGATION_TOKEN token 1069 for haha)
2015-11-05 16:03:20,250 WARN  [main]: token.Token (Token.java:getClassForIdentifier(121)) - Cannot find class for token kind kms-dt
2015-11-05 16:03:20,250 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: kms-dt, Service: 172.25.17.102:9292, Ident: 00 04 68 61 68 61 02 72 6d 00 8a 01 50 da 1a ca 29 8a 01 50 fe 27 4e 29 03 02
2015-11-05 16:03:22,561 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb
2015-11-05 16:03:22,562 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor
2015-11-05 16:03:33,733 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir
org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)
        at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)
        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)
        ... 21 more
Caused by: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.
        at org.apache.hadoop.mapreduce.Job.ensureNotSet(Job.java:1194)
        at org.apache.hadoop.mapreduce.Job.setUseNewAPI(Job.java:1229)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1283)
        at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:183)
        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:153)
        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1153)
        ... 23 more

2015-11-05 16:03:33,734 INFO  [main]: hooks.ATSHook (ATSHook.java:<init>(84)) - Created ATS Hook

{code}
",,
HIVE-12662,"L96 of HiveSortJoinReduceRule, you will see 

{noformat}
    // Finally, if we do not reduce the input size, we bail out
    if (RexLiteral.intValue(sortLimit.fetch)
            >= RelMetadataQuery.getRowCount(reducedInput)) {
      return false;
    }
{noformat}

It is using  RelMetadataQuery.getRowCount which is always at least 1. This is the problem that we resolved in CALCITE-987.

To confirm this, I just run the q file :

{noformat}
set hive.mapred.mode=nonstrict;
set hive.optimize.limitjointranspose=true;
set hive.optimize.limitjointranspose.reductionpercentage=1f;
set hive.optimize.limitjointranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;
{noformat}

  And I got

{noformat}
2015-12-11T10:21:04,435 ERROR [c1efb099-f900-46dc-9f74-97af0944a99d main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(301)) - CBO failed, skipping CBO.
java.lang.RuntimeException: java.lang.StackOverflowError
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.rethrowCalciteException(CalcitePlanner.java:749) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:645) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:264) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10076) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:223) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:456) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1138) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1187) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1063) [hive-exec-2.1.0-SNAPSHOT.jar:?]
{noformat}

via [~pxiong]",,
HIVE-12815,"I was running something like create table as select 1;

First it logs why it cannot get stats:
{noformat}
2016-01-08T21:46:31,876 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: stats.StatsUtils (StatsUtils.java:getTableColumnStats(756)) - Failed to retrieve table statistics:
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Specified database/table does not exist : _dummy_database._dummy_table)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(Hive.java:3195) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:752) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:198) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
{noformat}

and returns null, then it fails with NPE:
{noformat}
2016-01-08T21:46:31,885 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: ql.Driver (SessionState.java:printError(1010)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(StatsUtils.java:1450)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:199)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
{noformat}

Only ""NullPointerException null"" is logged to CLI... :(",Sadness,-1
HIVE-13002,"Discovered in some q test run:
{noformat}
 TestCliDriver.testCliDriver_insert_values_orig_table:123->runTest:199 Unexpected exception java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$EntryIterator.next(HashMap.java:966)
	at java.util.HashMap$EntryIterator.next(HashMap.java:964)
	at org.apache.hadoop.hive.ql.metadata.Hive.dumpAndClearMetaCallTiming(Hive.java:3412)
	at org.apache.hadoop.hive.ql.Driver.dumpMetaCallTimingWithoutEx(Driver.java:574)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1722)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1342)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1113)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1101)
{noformat}",,
HIVE-13017,"The following query fails, when Azure Filesystem is used as default file system, and HDFS is used for intermediate data.

{noformat}
>>>  create temporary table s10k stored as orc as select * from studenttab10k;
>>>  create temporary table v10k as select * from votertab10k;
>>>  select registration 
from s10k s join v10k v 
on (s.name = v.name) join studentparttab30k p 
on (p.name = v.name) 
where s.age < 25 and v.age < 25 and p.age < 25;
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-5

Logs:

ERROR : /var/log/hive/hiveServer2.log
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)
Aborting command set because ""force"" is false and command failed: ""select registration 
from s10k s join v10k v 
on (s.name = v.name) join studentparttab30k p 
on (p.name = v.name) 
where s.age < 25 and v.age < 25 and p.age < 25;""
Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice
hiveServer2.log shows:
2016-02-02 18:04:34,182 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,199 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,212 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie
2016-02-02 18:04:34,213 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:checkConcurrency(168)) - Concurrency mode is disabled, not creating a lock manager
2016-02-02 18:04:34,219 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal
2016-02-02 18:04:34,219 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,225 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1390)) - Setting caller context to query id hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,226 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1393)) - Starting command(queryId=hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0): select registration
from s10k s join v10k v
on (s.name = v.name) join studentparttab30k p
on (p.name = v.name)
where s.age < 25 and v.age < 25 and p.age < 25
2016-02-02 18:04:34,228 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:<init>(90)) - Created ATS Hook
2016-02-02 18:04:34,229 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,237 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa
2016-02-02 18:04:34,238 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436274229 end=1454436274238 duration=9 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,239 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,240 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook start=1454436274239 end=1454436274240 duration=1 from=org.apache.hadoop.hive.ql.Driver>
Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,242 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
Total jobs = 1
2016-02-02 18:04:34,243 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Total jobs = 1
2016-02-02 18:04:34,245 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=TimeToSubmit start=1454436274199 end=1454436274245 duration=46 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,246 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,247 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=task.MAPREDLOCAL.Stage-5 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:34,258 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:launchTask(1718)) - Starting task [Stage-5:MAPREDLOCAL] in serial mode
2016-02-02 18:04:34,280 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(158)) - Generating plan file file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml
2016-02-02 18:04:34,288 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
2016-02-02 18:04:34,289 INFO  [HiveServer2-Background-Pool: Thread-517]: exec.Utilities (Utilities.java:serializePlan(1028)) - Serializing MapredLocalWork via kryo
2016-02-02 18:04:34,290 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPreHookEvent(158)) - Received pre-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
2016-02-02 18:04:34,358 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=serializePlan start=1454436274288 end=1454436274358 duration=70 from=org.apache.hadoop.hive.ql.exec.Utilities>
2016-02-02 18:04:34,737 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(287)) - Executing: /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop jar /usr/hdp/2.4.1.0-170/hive/lib/hive-common-1.2.1000.2.4.1.0-170.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml   -jobconffile file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10007/jobconf.xml
WARNING: Use ""yarn jar"" to launch YARN applications.
2016-02-02 18:04:37,450 INFO  [org.apache.ranger.audit.queue.AuditBatchQueue0]: provider.BaseAuditHandler (BaseAuditHandler.java:logStatus(312)) - Audit Status Log: name=hiveServer2.async.summary.batch.solr, interval=01:21.012 minutes, events=2, succcessCount=2, totalEvents=4, totalSuccessCount=4
Execution log at: /tmp/hive/hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0.log
2016-02-02 18:04:39,248 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie
2016-02-02 18:04:39,254 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal
2016-02-02 18:04:39,261 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa
2016-02-02 18:04:40     Starting to launch local task to process map join;      maximum memory = 477102080
Execution failed with exit status: 2
2016-02-02 18:04:43,728 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Execution failed with exit status: 2
Obtaining error information
2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Obtaining error information

Task failed!
Task ID:
  Stage-5

Logs:

2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) -
Task failed!
Task ID:
  Stage-5

Logs:

/var/log/hive/hiveServer2.log
2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - /var/log/hive/hiveServer2.log
2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(307)) - Execution failed with exit status: 2
2016-02-02 18:04:43,733 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:<init>(90)) - Created ATS Hook
2016-02-02 18:04:43,734 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:43,736 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436283734 end=1454436283736 duration=2 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:43,736 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPostHookEvent(193)) - Received post-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
2016-02-02 18:04:43,757 ERROR [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printError(932)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
2016-02-02 18:04:43,758 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1621)) - Resetting the caller context to HIVE_SSN_ID:916e3dbb-a10d-4888-a063-52fb058ea421
2016-02-02 18:04:43,759 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=Driver.execute start=1454436274219 end=1454436283759 duration=9540 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:43,760 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:43,761 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=releaseLocks start=1454436283760 end=1454436283761 duration=1 from=org.apache.hadoop.hive.ql.Driver>
2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)
        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
hive Configs can be viewed from http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/artifacts/tmpModifyConfDir_1454394513592/
Attachments
Drop files to attach, or browse.
Add LinkIssue Links
relates to
Bug - A problem which impairs or prevents the functions of the product. HIVE-739 webhcat tests failing in HDInsight secure cluster throwing NullPointerException	 Blocker - Blocks development and/or testing work, production could not run. RESOLVED
Activity
All
Comments
Work Log
History
Activity
Ascending order - Click to sort in descending order
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
The test passes via hive CLI and explain shows:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> explain select registration
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> from s10k s join v10k v
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (s.name = v.name) join studentparttab30k p
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (p.name = v.name)
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> where s.age < 25 and v.age < 25 and p.age < 25;
+--------------------------------------------------------------------------------------------------------------+--+
|                                                   Explain                                                    |
+--------------------------------------------------------------------------------------------------------------+--+
| STAGE DEPENDENCIES:                                                                                          |
|   Stage-5 is a root stage                                                                                    |
|   Stage-4 depends on stages: Stage-5                                                                         |
|   Stage-0 depends on stages: Stage-4                                                                         |
|                                                                                                              |
| STAGE PLANS:                                                                                                 |
|   Stage: Stage-5                                                                                             |
|     Map Reduce Local Work                                                                                    |
|       Alias -> Map Local Tables:                                                                             |
|         s                                                                                                    |
|           Fetch Operator                                                                                     |
|             limit: -1                                                                                        |
|         v                                                                                                    |
|           Fetch Operator                                                                                     |
|             limit: -1                                                                                        |
|       Alias -> Map Local Operator Tree:                                                                      |
|         s                                                                                                    |
|           TableScan                                                                                          |
|             alias: s                                                                                         |
|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |
|             Statistics: Num rows: 459 Data size: 47777 Basic stats: COMPLETE Column stats: NONE              |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |
|               Statistics: Num rows: 76 Data size: 7910 Basic stats: COMPLETE Column stats: NONE              |
|               HashTable Sink Operator                                                                        |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|         v                                                                                                    |
|           TableScan                                                                                          |
|             alias: v                                                                                         |
|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |
|             Statistics: Num rows: 1653 Data size: 337233 Basic stats: COMPLETE Column stats: NONE            |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |
|               Statistics: Num rows: 275 Data size: 56103 Basic stats: COMPLETE Column stats: NONE            |
|               HashTable Sink Operator                                                                        |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|                                                                                                              |
|   Stage: Stage-4                                                                                             |
|     Map Reduce                                                                                               |
|       Map Operator Tree:                                                                                     |
|           TableScan                                                                                          |
|             alias: p                                                                                         |
|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |
|             Statistics: Num rows: 30000 Data size: 627520 Basic stats: COMPLETE Column stats: COMPLETE       |
|             Filter Operator                                                                                  |
|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |
|               Statistics: Num rows: 10000 Data size: 1010000 Basic stats: COMPLETE Column stats: COMPLETE    |
|               Map Join Operator                                                                              |
|                 condition map:                                                                               |
|                      Inner Join 0 to 1                                                                       |
|                      Inner Join 1 to 2                                                                       |
|                 keys:                                                                                        |
|                   0 name (type: string)                                                                      |
|                   1 name (type: string)                                                                      |
|                   2 name (type: string)                                                                      |
|                 outputColumnNames: _col8                                                                     |
|                 Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE      |
|                 Select Operator                                                                              |
|                   expressions: _col8 (type: string)                                                          |
|                   outputColumnNames: _col0                                                                   |
|                   Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE    |
|                   File Output Operator                                                                       |
|                     compressed: false                                                                        |
|                     Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE  |
|                     table:                                                                                   |
|                         input format: org.apache.hadoop.mapred.TextInputFormat                               |
|                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat            |
|                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                            |
|       Local Work:                                                                                            |
|         Map Reduce Local Work                                                                                |
|                                                                                                              |
|   Stage: Stage-0                                                                                             |
|     Fetch Operator                                                                                           |
|       limit: -1                                                                                              |
|       Processor Tree:                                                                                        |
|         ListSink                                                                                             |
|                                                                                                              |
+--------------------------------------------------------------------------------------------------------------+--+
83 rows selected (2.473 seconds)
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
emptablemisc_8 also fails with the same error:
>>>  create temporary table temp1 as select * from votertab10k;
>>>  select * 
from studenttab10k s 
where s.name not in 
(select name from temp1);
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:1
INFO  : Submitting tokens for job: job_1454394534358_0164
INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 246 for hrt_qa)
INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/
INFO  : Starting Job = job_1454394534358_0164, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/
INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0164
INFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
INFO  : 2016-02-02 10:11:02,367 Stage-4 map = 0%,  reduce = 0%
INFO  : 2016-02-02 10:11:26,060 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 5.48 sec
INFO  : 2016-02-02 10:11:39,024 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.18 sec
INFO  : MapReduce Total cumulative CPU time: 11 seconds 180 msec
INFO  : Ended Job = job_1454394534358_0164
INFO  : Stage-9 is selected by condition resolver.
INFO  : Stage-1 is filtered out by condition resolver.
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-9

Logs:

ERROR : /var/log/hive/hiveServer2.log
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1454394534358_0169
INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 252 for hrt_qa)
INFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/
INFO  : Starting Job = job_1454394534358_0169, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/
INFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0169
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
INFO  : 2016-02-02 10:16:38,027 Stage-1 map = 0%,  reduce = 0%
INFO  : 2016-02-02 10:16:52,498 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.8 sec
INFO  : 2016-02-02 10:16:53,566 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.94 sec
INFO  : 2016-02-02 10:17:16,202 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.04 sec
INFO  : MapReduce Total cumulative CPU time: 17 seconds 40 msec
INFO  : Ended Job = job_1454394534358_0169
ERROR : Execution failed with exit status: 2
ERROR : Obtaining error information
ERROR : 
Task failed!
Task ID:
  Stage-8

Logs:

ERROR : /var/log/hive/hiveServer2.log
Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)
Aborting command set because ""force"" is false and command failed: ""select * 
from studenttab10k s 
where s.name not in 
(select name from temp1);""
Closing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice
Its app log can be viewed at http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/app-logs/application_1454394534358_0169.log
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
The query works without 'temporary table':
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> select registration from studenttab10k s join votertab10k v
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (s.name = v.name) join studentparttab30k p
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (p.name = v.name)
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> where s.age < 25 and v.age < 25 and p.age < 25;
Permalink Edit Delete 
tsaito Takahiko Saito added a comment - 3 days ago
More info about temporary table:
0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> describe formatted s10k;
+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+
|           col_name            |                                                                  data_type                                                                  |        comment        |
+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+
| # col_name                    | data_type                                                                                                                                   | comment               |
|                               | NULL                                                                                                                                        | NULL                  |
| name                          | string                                                                                                                                      |                       |
| age                           | int                                                                                                                                         |                       |
| gpa                           | double                                                                                                                                      |                       |
|                               | NULL                                                                                                                                        | NULL                  |
| # Detailed Table Information  | NULL                                                                                                                                        | NULL                  |
| Database:                     | default                                                                                                                                     | NULL                  |
| Owner:                        | hrt_qa                                                                                                                                      | NULL                  |
| CreateTime:                   | Tue Feb 02 23:02:31 UTC 2016                                                                                                                | NULL                  |
| LastAccessTime:               | UNKNOWN                                                                                                                                     | NULL                  |
| Protect Mode:                 | None                                                                                                                                        | NULL                  |
| Retention:                    | 0                                                                                                                                           | NULL                  |
| Location:                     | hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c  | NULL                  |
| Table Type:                   | MANAGED_TABLE                                                                                                                               | NULL                  |
|                               | NULL                                                                                                                                        | NULL                  |
| # Storage Information         | NULL                                                                                                                                        | NULL                  |
| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde                                                                                             ",,
HIVE-13128,"Nullscan provides uris of the form nullscan://null/ - which are added to the list of FileSystems for which Tez should obtain tokens.

{code}
2016-02-19T02:48:04,481 ERROR [main]: exec.Task (TezTask.java:execute(219)) - Failed to execute tez graph.
java.lang.IllegalArgumentException: java.net.UnknownHostException: null
  at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:406) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:291) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:302) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:524) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:508) ~[hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:107) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystemsInternal(TokenCache.java:86) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.common.security.TokenCache.obtainTokensForFileSystems(TokenCache.java:76) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.addFileSystemCredentialsFromURIs(TezClientUtils.java:338) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.setupDAGCredentials(TezClientUtils.java:369) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClientUtils.prepareAndCreateDAGPlan(TezClientUtils.java:704) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:522) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.tez.client.TezClient.submitDAG(TezClient.java:468) ~[tez-api-0.8.2.2.3.5.1-26.jar:0.8.2.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:466) ~[hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:187) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1858) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1600) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1373) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1196) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1184) [hive-exec-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:228) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:180) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:395) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:331) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:428) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:444) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:744) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:711) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:640) [hive-cli-2.0.0.2.3.5.1-26.jar:2.0.0.2.3.5.1-26]
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_45]
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_45]
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_45]
  at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_45]
  at org.apache.hadoop.util.RunJar.run(RunJar.java:221) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]
  at org.apache.hadoop.util.RunJar.main(RunJar.java:136) [hadoop-common-2.7.1.2.3.5.1-26.jar:?]
Caused by: java.net.UnknownHostException: null
  ... 37 more
{code}

This is while trying to obtain tokens for 
{code}2016-02-23T02:49:41,782 DEBUG [main]: tez.DagUtils (DagUtils.java:addCredentials(164)) - Marking URI as needing credentials: nullscan://null/default.studenttab10k/part_
{code}",Surprise,-1
HIVE-13174,"If you have a table with a bin column you're hs2/client logs are full of the stack traces below. These should either be made debug or we just log the message not the trace.
{code}
2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize
org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)
	at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)
	at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}","Sadness, Surprise",-1
HIVE-13261,"To repro
{code}
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

analyze table partitioned1 compute statistics for columns;
{code}

Error msg:
{code}
2016-03-10T14:55:43,205 ERROR [abc3eb8d-7432-47ae-b76f-54c8d7020312 main[]]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(177)) - NoSuchObjectException(message:Column c for which stats gathering is requested doesn't exist.)
        at org.apache.hadoop.hive.metastore.ObjectStore.writeMPartitionColumnStatistics(ObjectStore.java:6492)
        at org.apache.hadoop.hive.metastore.ObjectStore.updatePartitionColumnStatistics(ObjectStore.java:6574)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
{code}",,
HIVE-13361,"With HIVE-11807 buffer size estimation happens by default. This can have undesired effect wrt file concatenation. Consider the following table with files

{code}
testtable
  -- 000000_0 (created before HIVE-11807 which has buffer size 256KB)
  -- 000001_0 (created before HIVE-11807 which has buffer size 256KB)
  -- 000002_0 (created after HIVE-11807 with buffer size chosen as 128KB)
  -- 000003_0 (created after HIVE-11807 with buffer size chosen as 128KB)
{code}

If we perform ALTER TABLE .. CONCATENATE on the above table with HIVE-11807, then depending on the split arrangement 000000_0 and 000001_0 will be concatenated together to new merged file. But this new merged file will have 128KB buffer size (estimated buffer size and not requested buffer size). Since new ORC writer size does not honor the requested buffer size the new merged files will have smaller buffers than the required 256KB making the file unreadable. Following exception will be thrown when reading the table after concatenation
{code}
2016-03-24T16:26:33,974 ERROR [a9e27a9a-37cb-411d-9708-6c58a4ce34f2 main]: CliDriver (SessionState.java:printError(1049)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187
java.io.IOException: java.lang.IllegalArgumentException: Buffer size too small. size = 131072 needed = 153187
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:513)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:420)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:145)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1848)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:256)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:721)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:648)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}
",Fear,-1
HIVE-13743,"Data move codepath is broken with hive 2.1.0-SNAPSHOT with hadoop 2.8.0-snapshot.

{noformat}
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Path not found: /apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1
        at org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.getEZForPath(FSDirEncryptionZoneOp.java:178)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEZForPath(FSNamesystem.java:7336)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEZForPath(NameNodeRpcServer.java:1973)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEZForPath(ClientNamenodeProtocolServerSideTranslatorPB.java:1376)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:645)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2339)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2335)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1711)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2333)

        at org.apache.hadoop.ipc.Client.call(Client.java:1448)
        at org.apache.hadoop.ipc.Client.call(Client.java:1385)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy30.getEZForPath(Unknown Source)/apps/hive/warehouse/tpcds_bin_partitioned_orc_200.db/

...
...
...
2016-05-11T09:40:43,760 ERROR [main]: ql.Driver (:()) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Unable to move source hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/.hive-staging_hive_2016-05-11_09-40-42_489_5056654133706433454-1/-ext-10002 to destination hdfs://xyz:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_10000.db/date_dim1
{noformat}

https://github.com/apache/hive/blob/26b5c7b56a4f28ce3eabc0207566cce46b29b558/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L2836

hdfsEncryptionShim.isPathEncrypted(destf) in Hive could end up throwing FileNotFoundException as the destf is not present yet.  This causes moveFile to fail.
",Fear,-1
HIVE-13810,"When running using beeline (as a non hdfs user)
{noformat} CREATE  EXTERNAL TABLE Batters_txt(
       Player STRING ,
       Team STRING ,
       League STRING ,
       Year SMALLINT,
       Games DOUBLE,
       AB DOUBLE,
       R DOUBLE,
       H DOUBLE,
       Doubles DOUBLE,
       Triples DOUBLE,
       HR DOUBLE,
       RBI DOUBLE,
       SB DOUBLE,
       CS DOUBLE,
       BB DOUBLE,
       SO DOUBLE,
       IBB DOUBLE,
       HBP DOUBLE,
       SH DOUBLE,
       SF DOUBLE,
       GIDP DOUBLE
 )
 location '/user/tableau/Batters';
 drop table if exists Batters;
 CREATE TABLE Batters (
       Player STRING ,
       Team STRING ,
       League STRING ,
       Year SMALLINT,
       Games DOUBLE,
       AB DOUBLE,
       R DOUBLE,
       H DOUBLE,
       Doubles DOUBLE,
       Triples DOUBLE,
       HR DOUBLE,
       RBI DOUBLE,
       SB DOUBLE,
       CS DOUBLE,
       BB DOUBLE,
       SO DOUBLE,
       IBB DOUBLE,
       HBP DOUBLE,
       SH DOUBLE,
       SF DOUBLE,
       GIDP DOUBLE
       )
 STORED AS orc tblproperties (""orc.compress""=""SNAPPY"");
 insert overwrite table Batters select * from Batters_txt;
{noformat}

runs into the following error:
{code}
2016-05-18T19:59:00,883 ERROR [HiveServer2-Background-Pool: Thread-306]: operation.Operation (:()) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. org.apache.hadoop.security.AccessControlException: User does not belong to hdfs
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:818)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:472)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:644)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2273)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2267)

	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:387)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)
	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:90)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:290)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:303)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.security.AccessControlException: User does not belong to hdfs
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:818)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:472)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:644)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2273)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2267)

	at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2896)
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3151)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1803)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:347)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1857)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1312)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1076)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:235)
	... 11 more
Caused by: org.apache.hadoop.security.AccessControlException: User does not belong to hdfs
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:818)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:472)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:644)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2273)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2267)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.setOwner(DFSClient.java:2510)
	at org.apache.hadoop.hdfs.DistributedFileSystem$32.doCall(DistributedFileSystem.java:1609)
	at org.apache.hadoop.hdfs.DistributedFileSystem$32.doCall(DistributedFileSystem.java:1605)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.setOwner(DistributedFileSystem.java:1605)
	at org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:114)
	at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2875)
	at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2867)
	... 4 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): User does not belong to hdfs
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:818)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:472)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:644)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2273)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2267)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1531)
	at org.apache.hadoop.ipc.Client.call(Client.java:1481)
	at org.apache.hadoop.ipc.Client.call(Client.java:1386)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)
	at com.sun.proxy.$Proxy31.setOwner(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setOwner(ClientNamenodeProtocolTranslatorPB.java:417)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy32.setOwner(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.setOwner(DFSClient.java:2508)
	... 11 more
{code}",,
HIVE-13856,"{noformat}
2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended
 (SQLState=42000, ErrorCode=933)
2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)
	at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)
	at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing open_txns
MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended

	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
	at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
	at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
	at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
	at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
	at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)
	at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)
	at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
)
	at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.open_txns(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
I think the reason here is that
{code:title=metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java|borderStyle=solid}
  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {
...
        String query;
        String insertClause = ""insert into TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host) values "";
        StringBuilder valuesClause = new StringBuilder();

        for (long i = first; i < first + numTxns; i++) {
          txnIds.add(i);

          if (i > first &&
              (i - first) % conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE) == 0) {
            // wrap up the current query, and start a new one
            query = insertClause + valuesClause.toString();
            queries.add(query);

            valuesClause.setLength(0);
            valuesClause.append(""("").append(i).append("", 'o', "").append(now).append("", "").append(now)
                .append("", '"").append(rqst.getUser()).append(""', '"").append(rqst.getHostname())
                .append(""')"");

            continue;
          }

          if (i > first) {
            valuesClause.append("", "");
          }

          valuesClause.append(""("").append(i).append("", 'o', "").append(now).append("", "").append(now)
              .append("", '"").append(rqst.getUser()).append(""', '"").append(rqst.getHostname())
              .append(""')"");
        }

        query = insertClause + valuesClause.toString();
...
}
{code}
ends up building a query of the form
{code:sql}
INSERT INTO TXNS (...) VALUES (...), (...)
{code}
Oracle doesn't like this way of inserting multiple rows of data. Couple of ways the following [post|http://www.oratable.com/oracle-insert-all/] describe is either inserting each row individually or use the {{INSERT ALL}} semantics.",,
HIVE-13872,"TPC-DS Q13 produces a cross-product without CBO simplifying the query

{code}
Caused by: java.lang.RuntimeException: null STRING entry: batchIndex 0 projection column num 1
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.nullBytesReadError(VectorExtractRow.java:349)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRowColumn(VectorExtractRow.java:267)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRow(VectorExtractRow.java:343)
        at org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.process(VectorReduceSinkOperator.java:103)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:762)
        ... 18 more
{code}

Simplified query

{code}
set hive.cbo.enable=false;

-- explain

select count(1)  
 from store_sales
     ,customer_demographics
 where (
( 
  customer_demographics.cd_demo_sk = store_sales.ss_cdemo_sk
  and customer_demographics.cd_marital_status = 'M'
     )or
     (
   customer_demographics.cd_demo_sk = ss_cdemo_sk
  and customer_demographics.cd_marital_status = 'U'
     ))
;
{code}

{code}
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: customer_demographics
                  Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    sort order: 
                    Statistics: Num rows: 1920800 Data size: 717255532 Basic stats: COMPLETE Column stats: NONE
                    value expressions: cd_demo_sk (type: int), cd_marital_status (type: string)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
{code}",Surprise,-1
HIVE-14173,"hive.metastore.try.direct.sql is initially set to false in HMS hive-site.xml, then changed to true using set metaconf command in the middle of a session, running a query will be thrown NPE with error message is as following:
{code}
2016-07-06T17:44:41,489 ERROR [pool-5-thread-2]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(192)) - MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5741)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4771)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:4754)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy18.get_partitions_by_expr(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:12048)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:12032)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.<init>(ObjectStore.java:2667)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetListHelper.<init>(ObjectStore.java:2825)
	at org.apache.hadoop.hive.metastore.ObjectStore$4.<init>(ObjectStore.java:2410)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:2410)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:2400)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
	at com.sun.proxy.$Proxy17.getPartitionsByExpr(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:4749)
	... 20 more
{code}",,
HIVE-14292,"While creating a ACID table ran into the following error:
{noformat}
>>>  create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties('transactional'='true');
INFO  : Compiling command(queryId=hive_20160719105944_bfe65377-59fa-4e17-941e-1f86b8daca15): create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties('transactional'='true')
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20160719105944_bfe65377-59fa-4e17-941e-1f86b8daca15); Time taken: 0.111 seconds
Error: Error running query: java.lang.RuntimeException: Unable to lock 'CheckLock' due to: Duplicate entry 'CheckLock-0' for key 'PRIMARY' (SQLState=23000, ErrorCode=1062) (state=,code=0)
Aborting command set because ""force"" is false and command failed: ""create table acidcount1 (id int) 
clustered by (id) into 2 buckets 
stored as orc 
tblproperties('transactional'='true');""
{noformat}
Saw the following detailed stack in the server log:
{noformat}
2016-07-19T10:59:46,213 ERROR [HiveServer2-Background-Pool: Thread-463]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(196)) - java.lang.RuntimeException: Unable to lock 'CheckLock' due to: Duplicate entry 'CheckLock-0' for key 'PRIMARY' (SQLState=23000, ErrorCode=1062)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock(TxnHandler.java:3235)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:2309)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLockWithRetry(TxnHandler.java:1012)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:784)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5941)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy26.lock(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:2109)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
        at com.sun.proxy.$Proxy28.lock(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2259)
        at com.sun.proxy.$Proxy28.lock(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$SynchronizedMetaStoreClient.lock(DbTxnManager.java:740)
        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:103)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:341)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocksWithHeartbeatDelay(DbTxnManager.java:357)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:167)
        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:980)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1316)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1090)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242)
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'CheckLock-0' for key 'PRIMARY'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
        at com.mysql.jdbc.Util.getInstance(Util.java:360)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:971)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)
        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)
        at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock(TxnHandler.java:3231)
        ... 47 more
{noformat}",,
HIVE-14355,"When schema is evolved from any integer type to string then following exceptions are thrown in LLAP (Works fine in Tez). I guess this should happen even for other conversions.

{code}
hive> create table orc_integer(b bigint) stored as orc;
hive> insert into orc_integer values(100);
hive> select count(*) from orc_integer where b=100;
OK
1
hive> alter table orc_integer change column b b string;
hive> select count(*) from orc_integer where b=100;
// FAIL with following exception
{code}

{code:title=When vectorization is enabled}
2016-07-27T01:48:05,611  INFO [TezTaskRunner ()] vector.VectorReduceSinkOperator: RECORDS_OUT_INTERMEDIATE_Map_1:0,
2016-07-27T01:48:05,611 ERROR [TezTaskRunner ()] tez.TezProcessor: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:95)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:70)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:866)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
        ... 18 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
        at org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringGroupColEqualStringGroupScalarBase.evaluate(FilterStringGroupColEqualStringGroupScalarBase.java:42)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:110)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:774)
        ... 19 more
{code}

{code:title=When vectorization is disabled}
2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.ReduceSinkOperator: Using tag = -1
2016-07-27T01:52:43,328  INFO [TezTaskRunner (1469608604787_0002_26_00_000000_0)] exec.OperatorUtils: Setting output collector: RS[4] --> Reducer 2
2016-07-27T01:52:43,329 ERROR [TezTaskRunner (1469608604787_0002_26_00_000000_0)] io.BatchToRowReader: Error at row 0/1, column 0/1 org.apache.hadoop.hive.ql.exec.vector.LongColumnVector@7630e56a
java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextString(BatchToRowReader.java:334) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.nextValue(BatchToRowReader.java:602) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:149) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.BatchToRowReader.next(BatchToRowReader.java:78) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151) ~[tez-mapreduce-0.8.4.jar:0.8.4]
        at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116) ~[tez-mapreduce-0.8.4.jar:0.8.4]
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:62) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:393) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_91]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_91]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) ~[tez-runtime-internals-0.8.4.jar:0.8.4]
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) ~[tez-common-0.8.4.jar:0.8.4]
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) ~[hive-llap-server-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_91]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_91]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]
{code}

",Fear,-1
HIVE-14607,"Steps to repro: 
in TestTxnCommands2WithSplitUpdate remove the overridden method testOrcPPD().
Then run:
mvn test -Dtest=TestTxnCommands2WithSplitUpdate#testOrcPPD

it will fail with ArrayIndexOutOfBounds.  HIVE-14448 was supposed to have fixed it....


{noformat}
2016-08-22T15:54:17,654  INFO [main] mapreduce.JobSubmitter: Cleaning up the staging area file:/Users/ekoifman/dev/hiverwgit/ql/target/tmp/hadoop-tmp/mapred/staging/ekoifman99742506\
0/.staging/job_local997425060_0002
2016-08-22T15:54:17,663 ERROR [main] exec.Task: Job Submission failed with exception 'java.lang.RuntimeException(ORC split generation failed with exception: java.lang.ArrayIndexOutO\
fBoundsException: 1)'
java.lang.RuntimeException: ORC split generation failed with exception: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1670)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1756)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:370)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:488)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:329)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:321)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1297)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1294)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1294)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:417)
        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:141)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1983)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1674)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1410)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1134)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1122)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:1392)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcPPD(TestTxnCommands2.java:195)
        at org.apache.hadoop.hive.ql.TestTxnCommands2.testOrcPPD(TestTxnCommands2.java:157)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 1
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1664)
        ... 60 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSargColumnNames(OrcInputFormat.java:394)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:447)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.extractNeededColNames(OrcInputFormat.java:442)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$2900(OrcInputFormat.java:149)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1360)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2700(OrcInputFormat.java:1154)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1334)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1331)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1331)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1154)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}","Sadness, Surprise",-1
HIVE-14743,"The stack:
{noformat}
2016-09-13T09:38:49,972 ERROR [186b4545-65b5-4bfc-bc8e-3e14e251bb12 main] exec.Task: Job Submission failed with exception 'java.lang.ArrayIndexOutOfBoundsException(1)'
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.createFilterScan(HiveHBaseTableInputFormat.java:224)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplitsInternal(HiveHBaseTableInputFormat.java:492)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:449)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:370)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:466)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:356)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:546)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:329)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:320)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)
        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)

{noformat}

Repro:
{noformat}
CREATE TABLE HBASE_TABLE_TEST_1(
  cvalue string ,
  pk string,
 ccount int   )
ROW FORMAT SERDE
  'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
  'hbase.columns.mapping'='cf:val,:key,cf2:count',
  'hbase.scan.cache'='500',
  'hbase.scan.cacheblocks'='false',
  'serialization.format'='1')
TBLPROPERTIES (
  'hbase.table.name'='hbase_table_test_1',
  'serialization.null.format'=''  );


  CREATE VIEW VIEW_HBASE_TABLE_TEST_1 AS SELECT hbase_table_test_1.cvalue,hbase_table_test_1.pk,hbase_table_test_1.ccount FROM hbase_table_test_1 WHERE hbase_table_test_1.ccount IS NOT NULL;

CREATE TABLE HBASE_TABLE_TEST_2(
  cvalue string ,
    pk string ,
   ccount int  )
ROW FORMAT SERDE
  'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
  'hbase.columns.mapping'='cf:val,:key,cf2:count',
  'hbase.scan.cache'='500',
  'hbase.scan.cacheblocks'='false',
  'serialization.format'='1')
TBLPROPERTIES (
  'hbase.table.name'='hbase_table_test_2',
  'serialization.null.format'='');


CREATE VIEW VIEW_HBASE_TABLE_TEST_2 AS SELECT hbase_table_test_2.cvalue,hbase_table_test_2.pk,hbase_table_test_2.ccount FROM hbase_table_test_2 WHERE  hbase_table_test_2.pk >='3-0000h-0' AND hbase_table_test_2.pk <= '3-0000h-g' AND hbase_table_test_2.ccount IS NOT NULL;

set hive.auto.convert.join=false;

  SELECT  p.cvalue cvalue
FROM `VIEW_HBASE_TABLE_TEST_1` `p`
LEFT OUTER JOIN `VIEW_HBASE_TABLE_TEST_2` `A1`
ON `p`.cvalue = `A1`.cvalue
LEFT OUTER JOIN `VIEW_HBASE_TABLE_TEST_1` `A2`
ON `p`.cvalue = `A2`.cvalue;

{noformat}


",,
HIVE-14773,"Hive runs into a NPE when the query has a filter on a date column and the partitioned column 
eg: 

{code}
create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into date_dim partition(d_date_sk=2416945) values('1905-04-09');
insert into date_dim partition(d_date_sk=2416946) values('1905-04-10');
insert into date_dim partition(d_date_sk=2416947) values('1905-04-11');
analyze table date_dim partition(d_date_sk) compute statistics for columns;

explain select count(*) from date_dim where d_date > date ""1900-01-02"" and d_date_sk= 2416945;
{code}

Here d_date_sk is a partition column and d_date is of type date.

{code}
2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date
2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select ""COLUMN_NAME"", ""COLUMN_TYPE"", min(""LONG_LOW_VALUE""), max(""LONG_HIGH_VALUE""), min(""DOUBLE_LOW_VALUE""), max(""DOUBLE_HIGH_VALUE""), min(cast(""BIG_DECIMAL_LOW_VALUE"" as decimal)), max(cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)), sum(""NUM_NULLS""), max(""NUM_DISTINCTS""), max(""AVG_COL_LEN""), max(""MAX_COL_LEN""), sum(""NUM_TRUES""), sum(""NUM_FALSES""), avg((""LONG_HIGH_VALUE""-""LONG_LOW_VALUE"")/cast(""NUM_DISTINCTS"" as decimal)),avg((""DOUBLE_HIGH_VALUE""-""DOUBLE_LOW_VALUE"")/""NUM_DISTINCTS""),avg((cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)-cast(""BIG_DECIMAL_LOW_VALUE"" as decimal))/""NUM_DISTINCTS""),sum(""NUM_DISTINCTS"") from ""PART_COL_STATS"" where ""DB_NAME"" = ? and ""TABLE_NAME"" = ?  and ""COLUMN_NAME"" in (?) and ""PARTITION_NAME"" in (?) group by ""COLUMN_NAME"", ""COLUMN_TYPE""]
2016-09-16T08:27:06,526  INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = false
partsFound = 1
ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:<ColumnStatisticsData >)]
2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
        org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)
2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms
2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451)
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}",,
HIVE-15275,"Execute {{""beeline -f <file>""}} and the command will throw the following NPE exception.

{noformat}
2016-11-23T13:34:54,367 WARN [Thread-1] org.apache.hadoop.util.ShutdownHookManager - ShutdownHook '' failed, java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hive.beeline.BeeLine$1.run(BeeLine.java:1247) ~[hive-beeline-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54) [hadoop-common-2.7.3.jar:?]
{noformat}",,
HIVE-15282,"The index_auto_mult_tables and index_auto_mult_tables_compact q tests are failing from time to time with the following error:
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables

Failing for the past 1 build (Since Failed#16 )
Took 16 sec.
Error Message

Unexpected exception junit.framework.AssertionFailedError: Client Execution results failed with error code = 1
See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
 at junit.framework.Assert.fail(Assert.java:57)
 at org.apache.hadoop.hive.ql.QTestUtil.failedDiff(QTestUtil.java:2001)
 at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:194)
 at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables(TestCliDriver.java:142)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)


See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
Stacktrace

junit.framework.AssertionFailedError: Unexpected exception junit.framework.AssertionFailedError: Client Execution results failed with error code = 1
See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
	at junit.framework.Assert.fail(Assert.java:57)
	at org.apache.hadoop.hive.ql.QTestUtil.failedDiff(QTestUtil.java:2001)
	at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:194)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables(TestCliDriver.java:142)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)


See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
	at junit.framework.Assert.fail(Assert.java:57)
	at org.apache.hadoop.hive.ql.QTestUtil.failed(QTestUtil.java:2011)
	at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:198)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables(TestCliDriver.java:142)
Standard Output

Running: diff -a /home/hiveptest/hive-ptest-cloudera-slaves-0c70-6.vpc.cloudera.com-hiveptest-1/cdh-source/itests/qtest/../../itests/qtest/target/qfile-results/clientpositive/index_auto_mult_tables.q.out /home/hiveptest/hive-ptest-cloudera-slaves-0c70-6.vpc.cloudera.com-hiveptest-1/cdh-source/itests/qtest/../../ql/src/test/results/clientpositive/index_auto_mult_tables.q.out
216c216,218
<   Stage-1 depends on stages: Stage-3
---
>   Stage-1 depends on stages: Stage-3, Stage-5
>   Stage-6 is a root stage
>   Stage-5 depends on stages: Stage-6
224,225c226,227
<             alias: default__src_src_index__
<             filterExpr: (((((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) and (UDFToDouble(key) > 70.0)) and (UDFToDouble(key) < 90.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
---
>             alias: default__srcpart_srcpart_index__
>             filterExpr: (((((UDFToDouble(key) > 70.0) and (UDFToDouble(key) < 90.0)) and (UDFToDouble(key) > 80.0)) and (UDFToDouble(key) < 100.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
227c229
<               predicate: (((((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) and (UDFToDouble(key) > 70.0)) and (UDFToDouble(key) < 90.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
---
>               predicate: (((((UDFToDouble(key) > 70.0) and (UDFToDouble(key) < 90.0)) and (UDFToDouble(key) > 80.0)) and (UDFToDouble(key) < 100.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
316a319,358
>   Stage: Stage-6
>     Map Reduce
>       Map Operator Tree:
>           TableScan
>             alias: default__src_src_index__
>             filterExpr: (((((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) and (UDFToDouble(key) > 70.0)) and (UDFToDouble(key) < 90.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
>             Filter Operator
>               predicate: (((((UDFToDouble(key) > 80.0) and (UDFToDouble(key) < 100.0)) and (UDFToDouble(key) > 70.0)) and (UDFToDouble(key) < 90.0)) and (not EWAH_BITMAP_EMPTY(_bitmaps))) (type: boolean)
>               Select Operator
>                 expressions: _bucketname (type: string), _offset (type: bigint)
>                 outputColumnNames: _col0, _col1
>                 Group By Operator
>                   aggregations: collect_set(_col1)
>                   keys: _col0 (type: string)
>                   mode: hash
>                   outputColumnNames: _col0, _col1
>                   Reduce Output Operator
>                     key expressions: _col0 (type: string)
>                     sort order: +
>                     Map-reduce partition columns: _col0 (type: string)
>                     value expressions: _col1 (type: array<bigint>)
>       Reduce Operator Tree:
>         Group By Operator
>           aggregations: collect_set(VALUE._col0)
>           keys: KEY._col0 (type: string)
>           mode: mergepartial
>           outputColumnNames: _col0, _col1
>           File Output Operator
>             compressed: false
>             table:
>                 input format: org.apache.hadoop.mapred.TextInputFormat
>                 output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
>                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
> 
>   Stage: Stage-5
>     Move Operator
>       files:
>           hdfs directory: true
> #### A masked pattern was here ####
> 
325a368,372
> PREHOOK: Input: default@default__srcpart_srcpart_index__
> PREHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-08/hr=11
> PREHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-08/hr=12
> PREHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-09/hr=11
> PREHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-09/hr=12
335a383,387
> POSTHOOK: Input: default@default__srcpart_srcpart_index__
> POSTHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-08/hr=11
> POSTHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-08/hr=12
> POSTHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-09/hr=11
> POSTHOOK: Input: default@default__srcpart_srcpart_index__@ds=2008-04-09/hr=12
342a395,442
> 82	val_82
> 82	val_82
> 82	val_82
> 82	val_82
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 83	val_83
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 84	val_84
> 85	val_85
> 85	val_85
> 85	val_85
> 85	val_85
> 86	val_86
> 86	val_86
> 86	val_86
> 86	val_86
> 87	val_87
> 87	val_87
> 87	val_87
> 87	val_87
Standard Error

Begin query: index_auto_mult_tables.q
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Warning: fs.defaultFS is not set when running ""chgrp"" command.
Warning: fs.defaultFS is not set when running ""chmod"" command.
Exception: Client Execution results failed with error code = 1
See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
junit.framework.AssertionFailedError: Client Execution results failed with error code = 1
See ./ql/target/tmp/log/hive.log or ./itests/qtest/target/tmp/log/hive.log, or check ./ql/target/surefire-reports or ./itests/qtest/target/surefire-reports/ for specific test cases logs.
	at junit.framework.Assert.fail(Assert.java:57)
	at org.apache.hadoop.hive.ql.QTestUtil.failedDiff(QTestUtil.java:2001)
	at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:194)
	at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_index_auto_mult_tables(TestCliDriver.java:142)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Failed query: index_auto_mult_tables.q
{noformat}

From the output of the failing test, it seems that the index on the srcpart table is not used. 
The hive.log contains the following:
{noformat}
2016-11-07T02:47:45,992  INFO [6401ee51-9d53-4101-a14e-9067d0bc357d main] index.IndexWhereProcessor: checking index staleness...
2016-11-07T02:47:45,998  INFO [6401ee51-9d53-4101-a14e-9067d0bc357d main] index.IndexWhereProcessor: Index is stale on partition 'ds=2008-04-09/hr=11'. Modified time (1478515600000) for 'pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt' is higher than index creation time (1478515599000).
{noformat}

The staleness check fails for the index on the srcpart table for the ds=2008-04-09/hr=11 partition, so the index is really not used. The staleness check fails, because the modification time of the itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt file (11:46:40:0) is higher than the index creation time (11:46:39:0).

After some investigation, I found that this happens if the creation of the partition folder and moving the kv1.txt file happens when the second turns. So the folder is created at 11:46:39,961, but the MoveTask which moves the kv1.txt file to the folder starts at 11:46:39:961 and finishes at 11:46:40,012.

{noformat}
2016-11-07T02:46:39,961  INFO [9b9edc01-22c2-460e-b008-03878e74077e main] common.FileUtils: Creating directory if it doesn't exist: pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11
2016-11-07T02:46:39,973 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: {-chgrp,-R,hiveptest,pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09}
2016-11-07T02:46:39,981 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: Return value is :0
2016-11-07T02:46:39,981 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: {-chmod,-R,755,pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09}
2016-11-07T02:46:39,992 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: Return value is :0
2016-11-07T02:46:39,992 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] metadata.Hive: The source path is /home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/data/files/kv1.txt/ and the destination path is /home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt/
2016-11-07T02:46:40,001 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: {-chgrp,-R,hiveptest,pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt}
2016-11-07T02:46:40,006 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: Return value is :0
2016-11-07T02:46:40,006 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: {-chmod,-R,755,pfile:/home/hiveptest/54.215.115.117-hiveptest-0/apache-github-source-source/itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt}
2016-11-07T02:46:40,012 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] shims.HdfsUtils: Return value is :0
2016-11-07T02:46:40,012 DEBUG [9b9edc01-22c2-460e-b008-03878e74077e main] log.PerfLogger: </PERFLOG method=FileMoves start=1478515599961 end=1478515600012 duration=51 from=MoveTask>
{noformat}

In this case, the last modification time of the folder itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/ will be 11:46:39 and of the kv1.txt will be 11:46:40.

When the index is built in the DDLTask.alterIndex method, the modification time which is stored for each partition is the modification time of the folder:
{noformat}
if (baseTbl.isPartitioned()) {
  List<Partition> baseParts;
  if (alterIndex.getSpec() != null) {
    baseParts = db.getPartitions(baseTbl, alterIndex.getSpec());
  } else {
    baseParts = db.getPartitions(baseTbl);
  }
  if (baseParts != null) {
    for (Partition p : baseParts) {
      FileSystem fs = p.getDataLocation().getFileSystem(db.getConf());
      FileStatus fss = fs.getFileStatus(p.getDataLocation());
      basePartTs.put(p.getSpec(), fss.getModificationTime());
    }
  }
} else {
{noformat}

But when the staleness is checked in the IndexUtils.isIndexPartitionFresh method, it checks the modification time of the files in the partition folder:
{noformat}
  private static boolean isIndexPartitionFresh(Hive hive, Index index,
      Partition part) throws HiveException {
    LOG.info(""checking index staleness..."");
    try {
      String indexTs = index.getParameters().get(part.getSpec().toString());
      if (indexTs == null) {
        return false;
      }

      FileSystem partFs = part.getDataLocation().getFileSystem(hive.getConf());
      FileStatus[] parts = partFs.listStatus(part.getDataLocation(), FileUtils.HIDDEN_FILES_PATH_FILTER);
      for (FileStatus status : parts) {
        if (status.getModificationTime() > Long.parseLong(indexTs)) {
          LOG.info(""Index is stale on partition '"" + part.getName()
              + ""'. Modified time ("" + status.getModificationTime() + "") for '"" + status.getPath()
              + ""' is higher than index creation time ("" + indexTs + "")."");
          return false;
        }
      }
    } catch (IOException e) {
      throw new HiveException(""Failed to grab timestamp information from partition '"" + part.getName() + ""': "" + e.getMessage(), e);
    }
    return true;
  }
{noformat}

Because of the modification time of the itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11/kv1.txt file is higher (11:46:40), than the modification time of the itests/qtest/target/warehouse/srcpart/ds=2008-04-09/hr=11 folder (11:46:39), the check fails and the index is not used which leads to the failure of the q test.",Surprise,-1
HIVE-15309,"OrcAcidUtils.getLastFlushLength() should check for file existence first.  Currently causes unnecessary/confusing logging:
{noformat}
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/r\
rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolSe\
rverSideTranslatorPB.java:373)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientName\
nodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)

        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)
        at org.apache.hadoop.ipc.Client.call(Client.java:1496)
        at org.apache.hadoop.ipc.Client.call(Client.java:1396)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
        at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB\
.java:270)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)
        at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)
        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)
        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
{noformat}

Also,
{noformat}
2016-08-02 01:05:01,107 INFO  [org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService-0]: txn.TxnHandler (TxnHandler.java:timeOutLocks(2836)) - Deleted 9 ext locks from HIVE_LOCKS due to timeout (vs. 1 found. List: [738]) maxHeartbeatTime=1470099601000
{noformat}

Note that the msg says ""Deleted 9 ext locks...""  It actually delete 1 ext which has 9 internal components.  Need to follow up on this.

Also,
TxnHandler has
{noformat}
        LOG.info(quoteString(key) + "" locked by "" + quoteString(TxnHandler.hostname));
{noformat}
and a corresponding ""unlock"" msg which flood the metastore log.
","Sadness, Surprise",-1
HIVE-15542,"Observed the following stacktrace, when all the values are NULL in date column.
 
{noformat}
2017-01-04T19:10:37,779 ERROR [46f293ab-1516-429d-aaab-4d5818ef8b82 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:792)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:206)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11071)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10644)
        at org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.analyze(ColumnStatsSemanticAnalyzer.java:412)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:510)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1302)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1442)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1222)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}",,
HIVE-15647,"Here's a simple example with the foodmart database:

{code}
hive> explain select count(*) from
    > sales_fact_1997 join store on sales_fact_1997.store_id = store.store_id
    > where ((store.salad_bar)) and ((store_number) <=> (customer_id));
FAILED: NullPointerException null
{code}

This happens on trunk and on HDP 2.5.3 / Hive 2. If you use = the NPE doesn't happen. If you remove the boolean condition the NPE doesn't happen.

{code}
FAILED: NullPointerException null
2016-12-13T18:23:33,604 ERROR [c4b7242e-1252-4709-8adf-22f631af75e8 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateFilterProc.process(ConstantPropagateProcFactory.java:1047)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate$ConstantPropagateWalker.walk(ConstantPropagate.java:151)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ConstantPropagate.java:120)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:242)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10913)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:246)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:75)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:435)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:326)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1169)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1083)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}",,
HIVE-15731,"While returning a session to the pool, the interrupt status on the thread seems to be set, which causes the pool return to fail.
The session slot is useless at this point. A HS2 instance configured for a single session will stop running queries.

{code}
2017-01-25T01:21:02,803 ERROR [HiveServer2-Background-Pool: Thread-117]: exec.Task (TezTask.java:execute(210)) - Failed to execute tez graph.
java.lang.InterruptedException
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220) ~[?:1.8.0_77]
  at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) ~[?:1.8.0_77]
  at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:350) ~[?:1.8.0_77]
  at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.returnSession(TezSessionPoolManager.java:337) ~[hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:196) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1874) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1578) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1329) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1098) [hive-exec-2.1.jar:2.1]
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1091) [hive-exec-2.1.jar:2.1]
  at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.jar:2.1]
  at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.jar:2.1]
  at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) [hive-service-2.1.jar:2.1]
  at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_77]
  at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) [hadoop-common-2.7.3.2.6.0.0-389.jar:?]
  at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347) [hive-service-2.1.jar:2.1]
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
  at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
2017-01-25T01:21:02,803 INFO  [e208c68c-f502-4c9d-a7b1-f3e7dd8b9447 HiveServer2-Handler-Pool: Thread-99]: session.SessionState (SessionState.java:resetThreadName(428))
{code}
","Sadness, Surprise",-1
HIVE-15755,"Ran into this error message - ""Error while compiling statement: FAILED: NullPointerException null "" when I specified an incorrect tablename in the merge statement.
 
{code:java}
> create table src (col1 int,col2 int);
No rows affected (0.231 seconds)
> create table trgt (tcol1 int,tcol2 int);
No rows affected (0.182 seconds)
> insert into src values (1,232);
{code}

{code:java}
> merge into trgt using (select * from src) sub on sub.col1 = *invalidtablename.tcol1* when not matched then insert values (sub.col1,sub.col2);
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

> merge into trgt using (select * from src) sub on sub.col1 = *trgt.tcol1* when not matched then insert values (sub.col1,sub.col2);

INFO  : Session is already open
INFO  : Dag name: merge into trgt using ...(sub.col1,sub.col2)(Stage-1)
INFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896
INFO  : 

INFO  : Status: Running (Executing on YARN cluster with App id application_1485398058799_0129)

INFO  : Map 1: 0/1	Map 2: -/-	
INFO  : Map 1: 0(+1)/1	Map 2: -/-	
INFO  : Map 1: 0(+1)/1	Map 2: -/-	
INFO  : Map 1: 1/1	Map 2: -/-	
INFO  : Loading data to table tpch.trgt from hdfs://tesths2-merge-ks-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/trgt/.hive-staging_hive_2017-01-30_06-54-50_743_6276941178188398287-1/-ext-10000
INFO  : Table tpch.trgt stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]
No rows affected (7.709 seconds)
{code}
Hiveserver2 logs:
{code:java}
2017-01-30 19:34:09,972 INFO  [HiveServer2-Handler-Pool: Thread-70]: parse.ParseDriver (ParseDriver.java:parse(185)) - Parsing command: merge into trgt using (select * from src) sub on sub.col1 = target.tcol1 when not matched then insert values (sub.col1,sub.col2)
2017-01-30 19:34:09,975 INFO  [HiveServer2-Handler-Pool: Thread-70]: parse.ParseDriver (ParseDriver.java:parse(209)) - Parse Completed
2017-01-30 19:34:09,976 INFO  [HiveServer2-Handler-Pool: Thread-70]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=parse start=1485804849971 end=1485804849976 duration=5 from=org.apache.hadoop.hive.ql.Driver>
2017-01-30 19:34:09,976 INFO  [HiveServer2-Handler-Pool: Thread-70]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
2017-01-30 19:34:09,977 INFO  [HiveServer2-Handler-Pool: Thread-70]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(824)) - 13: get_table : db=tpch tbl=trgt
2017-01-30 19:34:09,977 INFO  [HiveServer2-Handler-Pool: Thread-70]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(393)) - ugi=hive     ip=unknown-ip-addr      cmd=get_table : db=tpch tbl=trgt
2017-01-30 19:34:10,031 ERROR [HiveServer2-Handler-Pool: Thread-70]: ql.Driver (SessionState.java:printError(980)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)
        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

",,
HIVE-15859,"Hive on Spark, failed with error:
{noformat}
2017-02-08 09:50:59,331 Stage-2_0: 1039(+2)/1041 Stage-3_0: 796(+456)/1520 Stage-4_0: 0/2021 Stage-5_0: 0/1009 Stage-6_0: 0/1
2017-02-08 09:51:00,335 Stage-2_0: 1040(+1)/1041 Stage-3_0: 914(+398)/1520 Stage-4_0: 0/2021 Stage-5_0: 0/1009 Stage-6_0: 0/1
2017-02-08 09:51:01,338 Stage-2_0: 1041/1041 Finished Stage-3_0: 961(+383)/1520 Stage-4_0: 0/2021 Stage-5_0: 0/1009 Stage-6_0: 0/1
Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
{noformat}

application log shows the driver commanded a shutdown with some unknown reason, but hive's log shows Driver could not get RPC header( Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead).


{noformat}
17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1169.0 in stage 3.0 (TID 2519)
17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared
17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped
17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml
17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.
17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1105.0 in stage 3.0 (TID 2511)
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk6/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-71da1dfc-99bd-4687-bc2f-33452db8de3d
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk2/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-7f134d81-e77e-4b92-bd99-0a51d0962c14
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk5/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-77a90d63-fb05-4bc6-8d5e-1562cc502e6c
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk4/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-91f8b91a-114d-4340-8560-d3cd085c1cd4
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a3c24f9e-8609-48f0-9d37-0de7ae06682a
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk7/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-f6120a43-2158-4780-927c-c5786b78f53e
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk3/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-e17931ad-9e8a-45da-86f8-9a0fdca0fad1
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-4de34175-f871-4c28-8ec0-d2fc0020c5c3
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1137.0 in stage 3.0 (TID 2515)
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 897.0 in stage 3.0 (TID 2417)
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1225.0 in stage 3.0 (TID 2526)
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 905.0 in stage 3.0 (TID 2423)
{noformat}

in hive's log,
{noformat}
2017-02-08T09:51:04,327 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.TaskSetManager: Finished task 971.0 in stage 3.0 (TID 2218) in 5948 ms on hsx-node8 (1338/1520)
2017-02-08T09:51:04,346 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (org.apache.hive.spark.client.RemoteDriver$DriverProtocol.handle(io.netty.channel.ChannelHandlerContext, org.apache.hive.spark.client.rpc.Rpc$MessageHeader)).
2017-02-08T09:51:04,346 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.RpcDispatcher: [DriverProtocol] Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead.
2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (null).
2017-02-08T09:51:04,347 INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (Connection reset by peer).
2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception
2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.
2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: at com.google.common.base.Preconditions.checkState(Preconditions.java:149)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.Rpc: Failed to send RPC, closing connection.
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: java.nio.channels.ClosedChannelException
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN client.RemoteDriver: Shutting down driver because RPC channel was closed.
2017-02-08T09:51:04,348 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO client.RemoteDriver: Shutting down remote driver.
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at com.google.common.base.Preconditions.checkState(Preconditions.java:149)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)
2017-02-08T09:51:04,349 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.DAGScheduler: Asked to cancel job 2
2017-02-08T09:51:04,350 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: java.lang.InterruptedException
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.lang.Object.wait(Native Method)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.lang.Object.wait(Object.java:502)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.JobWaiter.awaitResult(JobWaiter.scala:73)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.SimpleFutureAction.org$apache$spark$SimpleFutureAction$$awaitResult(FutureAction.scala:165)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:120)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:108)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at scala.concurrent.Await$.ready(package.scala:86)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:303)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at java.lang.Thread.run(Thread.java:745)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at com.google.common.base.Preconditions.checkState(Preconditions.java:149)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)
2017-02-08T09:51:04,351 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
2017-02-08T09:51:04,352 INFO [stderr-redir-1] client.SparkClientImpl: at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
2017-02-08T09:51:04,654 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65f40590-d87f-4701-b374-6b3b2a11538c
2017-02-08T09:52:04,346 WARN [b723c85d-2a7b-469e-bab1-9c165b25e656 main] impl.RemoteSparkJobStatus: Error getting stage info
java.util.concurrent.TimeoutException
at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.23.Final.jar:4.0.23.Final]
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:161) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:101) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1997) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1688) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1419) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:430) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~[spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2]
at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~[spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2]
2017-02-08T09:52:04,346 ERROR [b723c85d-2a7b-469e-bab1-9c165b25e656 main] status.SparkJobMonitor: Failed to monitor Job[ 2] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'
java.lang.IllegalStateException: RPC channel is closed.
at com.google.common.base.Preconditions.checkState(Preconditions.java:149) ~[guava-14.0.1.jar:?]
at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
{noformat}

also in container's log, I find Driver still request for executors:
{noformat}
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 77 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 76 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 75 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 74 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 73 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 71 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 70 executor(s).
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers
17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 50 executor(s).
17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers
17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.
17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers
17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.
17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. 192.168.1.1:42777
17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hsx-node1:42777
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/02/08 09:51:04 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1486453422616_0150
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called
{noformat}

found only one ERROR in yarn application log:

{noformat}
17/02/08 09:51:00 INFO executor.Executor: Finished task 1492.0 in stage 3.0 (TID 2168). 3294 bytes result sent to driver
17/02/08 09:51:00 INFO executor.Executor: Finished task 556.0 in stage 3.0 (TID 1587). 3312 bytes result sent to driver
17/02/08 09:51:00 INFO executor.Executor: Finished task 1412.0 in stage 3.0 (TID 2136). 3294 bytes result sent to driver
17/02/08 09:51:00 INFO executor.Executor: Finished task 1236.0 in stage 3.0 (TID 2007). 3294 bytes result sent to driver
17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared
17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped
17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.
17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a8167f0b-f3c3-458f-ad51-8a0f4bcda4f3
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-26cba445-66d2-4b78-a428-17881c92f0f6
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
{noformat}

this error occurs when several queries run at the same time with large data scale, in fact it would not occur when running the query separately, but it can frequently occur when running together again.",Surprise,-1
HIVE-15923,"This is the ORM error, direct SQL fails too before that, with a similar error.

{noformat}

2017-02-14T17:45:11,158 ERROR [09fdd887-0164-4f55-97e9-4ba147d962be main] metastore.ObjectStore:java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.ExprNodeConstantDefaultDesc cannot be cast to java.lang.Long
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaLongObjectInspector.get(JavaLongObjectInspector.java:40) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getDouble(PrimitiveObjectInspectorUtils.java:801) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]        at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter$DoubleConverter.convert(P
rimitiveObjectInspectorConverter.java:240) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:145) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween.evaluate(GenericUDFBetween.java:57) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.ppr.PartExprEvalUtils.evaluateExprOnPart(PartExprEvalUtils.java:126) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
{noformat}
",,
HIVE-16562,"HIVE-13555 adds support for nullif. I'm encountering issues with nullif on master (3.0.0-SNAPSHOT rdac3786d86462e4d08d62d23115e6b7a3e534f5d)

Cluster side jobs work fine but client side don't.

Consider these two tables:
e011_02:
Columns c1 = float, c2 = double
1.0	1.0
1.5	1.5
2.0	2.0

test:
Columns c1 = int, c2 = int
Data:
1	1
2	2

And this query:
select nullif(c1, c2) from e011_02;

With e011_02 I get:
{code}
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating NULLIF(c1,c2)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:165)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2177)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:253)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating NULLIF(c1,c2)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:93)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:442)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:434)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:147)
	... 13 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazy.LazyFloat cannot be cast to org.apache.hadoop.io.FloatWritable
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableFloatObjectInspector.get(WritableFloatObjectInspector.java:36)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.comparePrimitiveObjects(PrimitiveObjectInspectorUtils.java:412)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFNullif.evaluate(GenericUDFNullif.java:93)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	... 18 more
{code}

With 
select nullif(c1, c2) from test;

I get:
{code}
2017-05-01T03:32:19,905 ERROR [cbaf5380-5b06-4531-aeb9-524c62314a46 main] CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating NULLIF(c1,c2)
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating NULLIF(c1,c2)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:165)
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2177)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:253)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating NULLIF(c1,c2)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:93)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:442)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:434)
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:147)
	... 13 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazy.LazyInteger cannot be cast to org.apache.hadoop.io.IntWritable
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.comparePrimitiveObjects(PrimitiveObjectInspectorUtils.java:404)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFNullif.evaluate(GenericUDFNullif.java:93)
	at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	... 18 more
{code}

Now if I set hive.fetch.task.conversion=none; and force a cluster side job, everything works fine

/cc [~kgyrtkirk] in case you have any ideas",Surprise,-1
HIVE-16576,"Debug logs on HIVE side - 
{code}
2017-05-03T23:49:00,672 DEBUG [HttpClient-Netty-Worker-0] client.NettyHttpClient: [GET http://localhost:8082/druid/v2/datasources/cmv_basetable_druid/candidates?intervals=1900-01-01T00:00:00.000+05:53:20/3000-01-01T00:00:00.000+05:30] Got response: 500 Server Error
{code}

Druid exception stack trace - 
{code}
2017-05-03T18:56:58,928 WARN [qtp1651318806-158] org.eclipse.jetty.servlet.ServletHandler - /druid/v2/datasources/cmv_basetable_druid/candidates
java.lang.IllegalArgumentException: Invalid format: """"1900-01-01T00:00:00.000 05:53:20""
	at org.joda.time.format.DateTimeFormatter.parseDateTime(DateTimeFormatter.java:899) ~[joda-time-2.8.2.jar:2.8.2]
	at org.joda.time.convert.StringConverter.setInto(StringConverter.java:212) ~[joda-time-2.8.2.jar:2.8.2]
	at org.joda.time.base.BaseInterval.<init>(BaseInterval.java:200) ~[joda-time-2.8.2.jar:2.8.2]
	at org.joda.time.Interval.<init>(Interval.java:193) ~[joda-time-2.8.2.jar:2.8.2]
	at org.joda.time.Interval.parse(Interval.java:69) ~[joda-time-2.8.2.jar:2.8.2]
	at io.druid.server.ClientInfoResource.getQueryTargets(ClientInfoResource.java:320) ~[classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_92]
{code}

Note that intervals being sent as part of the HTTP request URL are not encoded properly when not using UTC timezone.  ",Sadness,-1
HIVE-16788,"This ODBC call is meant to allow you to determine FK relationships either from the PK side or from the FK side.

Hive only allows you to traverse from the FK side, trying it from the PK side leads to an NPE.

Example using the table ""customer"" from TPC-H with FKs defined in Hive:

{code}
=== Foreign Keys ===
Using table as foreign source
(u'HIVE', u'tpch_bin_flat_orc_2', u'nation', u'n_nationkey', u'HIVE', u'tpch_bin_flat_orc_2', u'customer', u'c_nationkey', 1, 0, 0, u'custome
r_c2', u'nation_c1', 0)
Not using table as foreign source
Got an error from the server for customer!
{code}

Compare: Postgres
{code}
=== Foreign Keys ===
Using table as foreign source
(u'vagrant', u'public', u'nation', u'n_nationkey', u'vagrant', u'public', u'customer', u'c_nationkey', 1, 3, 3, u'customer_c_nationkey_fkey', u'nation_pkey', 7)
Not using table as foreign source
(u'vagrant', u'public', u'customer', u'c_custkey', u'vagrant', u'public', u'orders', u'o_custkey', 1, 3, 3, u'orders_o_custkey_fkey', u'customer_pkey', 7)
{code}

Note that Postgres allows traversal from either way. The traceback you get in the HS2 logs is this:

{code}
2016-12-04T21:08:55,398 ERROR [8998ca98-9940-49f8-8833-7c6ebd8c96a2 HiveServer2-Handler-Pool: Thread-53] metastore.RetryingHMSHandler: MetaEx
ception(message:java.lang.NullPointerException)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5785)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_foreign_keys(HiveMetaStore.java:6474)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy25.get_foreign_keys(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getForeignKeys(HiveMetaStoreClient.java:1596)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
        at com.sun.proxy.$Proxy26.getForeignKeys(Unknown Source)
        at org.apache.hive.service.cli.operation.GetCrossReferenceOperation.runInternal(GetCrossReferenceOperation.java:128)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:324)
        at org.apache.hive.service.cli.session.HiveSessionImpl.getCrossReference(HiveSessionImpl.java:933)
        at org.apache.hive.service.cli.CLIService.getCrossReference(CLIService.java:411)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetCrossReference(ThriftCLIService.java:738)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetCrossReference.getResult(TCLIService.java:1617)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetCrossReference.getResult(TCLIService.java:1602)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.hive.common.util.HiveStringUtils.normalizeIdentifier(HiveStringUtils.java:919)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.<init>(ObjectStore.java:2722)
        at org.apache.hadoop.hive.metastore.ObjectStore$GetListHelper.<init>(ObjectStore.java:2863)
        at org.apache.hadoop.hive.metastore.ObjectStore$11.<init>(ObjectStore.java:8455)
        at org.apache.hadoop.hive.metastore.ObjectStore.getForeignKeysInternal(ObjectStore.java:8455)
        at org.apache.hadoop.hive.metastore.ObjectStore.getForeignKeys(ObjectStore.java:8445)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
        at com.sun.proxy.$Proxy24.getForeignKeys(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_foreign_keys(HiveMetaStore.java:6465)
        ... 28 more
{code}",Surprise,-1
HIVE-16877,"After HIVE-8839 in 1.1.0 support ""alter table ... cascade"" to cascade table changes to partitions as well.  But NPE thrown when issue query like ""alter table ... cascade"" onto non-partitioned table 

Sample Query:
{code}
create table test_cascade_npe (id int);
alter table test_cascade_npe add columns (name string ) cascade;
{code}

Exception stack:
{code}
2017-06-09T22:16:05,913 ERROR [main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:547)
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:489)
    at org.apache.hadoop.hive.ql.metadata.Partition.getName(Partition.java:198)
    at org.apache.hadoop.hive.ql.hooks.Entity.computeName(Entity.java:339)
    at org.apache.hadoop.hive.ql.hooks.Entity.<init>(Entity.java:208)
    at org.apache.hadoop.hive.ql.hooks.WriteEntity.<init>(WriteEntity.java:104)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1496)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1473)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableModifyCols(DDLSemanticAnalyzer.java:2685)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:284)
    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:474)
    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1245)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1387)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1174)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1164)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)

{code}
",Surprise,-1
HIVE-16973,"Had a report from a user that Kerberos+AccumuloStorageHandler+HS2 was broken. Looking into it, it seems like the bit-rot got pretty bad. You'll see something like the following:

{noformat}
Caused by: java.io.IOException: Failed to unwrap AuthenticationToken 
at org.apache.hadoop.hive.accumulo.HiveAccumuloHelper.unwrapAuthenticationToken(HiveAccumuloHelper.java:312) 
at org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.getSplits(HiveAccumuloTableInputFormat.java:122) 
{noformat}

It appears that some of the code-paths changed since when I first did my testing (or I just did poor testing) and the delegation token was never being fetched/serialized. There also are some issues with fetching the delegation token from Accumulo properly which were addressed in ACCUMULO-4665

I believe it would also be best to just update the dependency to use Accumulo 1.7 (drop 1.6 support) as it's lacking in this regard. These changes would otherwise get much more complicated with reflection -- Accumulo has moved on past 1.6, so let's do the same in Hive.","Sadness, Surprise",-1
HIVE-17007,"Stack:
{code}
2017-06-30T02:39:43,739 ERROR [HiveServer2-Background-Pool: Thread-2873]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(200)) - MetaException(message:java.lang.NullPointerException)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6066)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3993)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3944)
        at sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
        at com.sun.proxy.$Proxy32.alter_table_with_environment_context(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:397)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:325)
        at sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
        at com.sun.proxy.$Proxy33.alter_table_with_environmentContext(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor75.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2306)
        at com.sun.proxy.$Proxy33.alter_table_with_environmentContext(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:624)
        at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3490)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:383)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1905)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1607)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1354)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242)
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:348)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.cache.SharedCache.getCachedTableColStats(SharedCache.java:140)
        at org.apache.hadoop.hive.metastore.cache.CachedStore.getTableColumnStatistics(CachedStore.java:1409)
        at sun.reflect.GeneratedMethodAccessor165.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
        at com.sun.proxy.$Proxy28.getTableColumnStatistics(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:800)
        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:257)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3973)
        ... 43 more
{code}",,
HIVE-17063,"The default value of {{hive.exec.stagingdir}} which is a relative path, and also drop partition on a external table will not clear the real data. As a result, insert overwrite partition twice will happen to fail because of the target data to be moved has 
 already existed.

This happened when we reproduce partition data onto a external table. 

I see the target data will not be cleared only when {{immediately generated data}} is child of {{the target data directory}}, so my proposal is trying  to clear target file already existed finally whe doing rename  {{immediately generated data}} into {{the target data directory}}

Operation reproduced:
{code}
create external table insert_after_drop_partition(key string, val string) partitioned by (insertdate string);
from src insert overwrite table insert_after_drop_partition partition (insertdate='2008-01-01') select *;
alter table insert_after_drop_partition drop partition (insertdate='2008-01-01');
from src insert overwrite table insert_after_drop_partition partition (insertdate='2008-01-01') select *;
{code}

Stack trace:

{code}
2017-07-09T08:32:05,212 ERROR [f3bc51c8-2441-4689-b1c1-d60aef86c3aa main] exec.Task: Failed with exception java.io.IOException: rename for src path: pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/.hive-staging_hive_2017-07-09_08-32-03_840_4046825276907030554-1/-ext-10000/000000_0 to dest path:pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/000000_0 returned false
org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: rename for src path: pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/.hive-staging_hive_2017-07-09_08-32-03_840_4046825276907030554-1/-ext-10000/000000_0 to dest path:pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/000000_0 returned false
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2992)
        at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3248)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1532)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1461)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:498)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2073)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1744)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1453)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1171)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1161)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335)
        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1137)
        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1111)
        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:120)
        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_insert_after_drop_partition(TestCliDriver.java:103)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
Caused by: java.io.IOException: rename for src path: pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/.hive-staging_hive_2017-07-09_08-32-03_840_4046825276907030554-1/-ext-10000/000000_0 to dest path:pfile:/data/haihua/official/hive/itests/qtest/target/warehouse/insert_after_drop_partition/insertdate=2008-01-01/000000_0 returned false
        at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2972)
        at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:2962)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}


",,
HIVE-17275,"If dynamic partitioning is used to write the output of UNION or UNION ALL queries into ORC files with hive.merge.tezfiles=true, the merge step fails as follows:

{noformat}
2017-08-08T11:27:19,958 ERROR [e7b1f06d-d632-408a-9dff-f7ae042cd25a main] SessionState: Vertex failed, vertexName=File Merge, vertexId=vertex_1502216690354_0001_33_00, diagnostics=[Task failed, taskId=task_1502216690354_0001_33_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1502216690354_0001_33_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Multiple partitions for one merge mapper: hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/1 NOT EQUAL TO hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/2
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
  at org.apache.hadoop.hive.ql.exec.tez.MergeFileTezProcessor.run(MergeFileTezProcessor.java:42)
  at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
  at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)
  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
  at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
  at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Multiple partitions for one merge mapper: hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/1 NOT EQUAL TO hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/2
  at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.processRow(MergeFileRecordProcessor.java:225)
  at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.run(MergeFileRecordProcessor.java:154)
  at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
  ... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Multiple partitions for one merge mapper: hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/1 NOT EQUAL TO hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/2
  at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.processKeyValuePairs(OrcFileMergeOperator.java:169)
  at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.process(OrcFileMergeOperator.java:72)
  at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.processRow(MergeFileRecordProcessor.java:216)
  ... 16 more
Caused by: java.io.IOException: Multiple partitions for one merge mapper: hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/1 NOT EQUAL TO hdfs://localhost:39943/build/ql/test/data/warehouse/partunion1/.hive-staging_hive_2017-08-08_11-27-09_105_286405133968521828-1/-ext-10002/part1=2014/2
  at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.checkPartitionsMatch(AbstractFileMergeOperator.java:180)
  at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.fixTmpPath(AbstractFileMergeOperator.java:197)
  at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.processKeyValuePairs(OrcFileMergeOperator.java:96)
  ... 18 more
{noformat}",,
HIVE-17309,"When executor alter partition onto a table which existed not in current database, InvalidOperationException thrown.

SQL example:
{code}
use default;
ALTER TABLE anotherdb.test_table_for_alter_partition_nocurrentdb partition(ds='haihua001') CHANGE COLUMN a a_new BOOLEAN;
{code}

We see this code in {{DDLTask.java}} potential problem that not transfer the qualified table name with database name when {{db.alterPartitions}} called.
{code}
      if (allPartitions == null) {
        db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), alterTbl.getEnvironmentContext());
      } else {
        db.alterPartitions(tbl.getTableName(), allPartitions, alterTbl.getEnvironmentContext());
      }
{code}

stacktrace:
{code}

2017-07-19T11:06:39,639  INFO [main] metastore.HiveMetaStore: New partition values:[2017-07-14]
2017-07-19T11:06:39,654 ERROR [main] metastore.RetryingHMSHandler: InvalidOperationException(message:alter is not possible)
    at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:526)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3560)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
    at com.sun.proxy.$Proxy21.alter_partitions_with_environment_context(Unknown Source)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1486)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
    at com.sun.proxy.$Proxy22.alter_partitions(Unknown Source)
    at org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(Hive.java:712)
    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3338)
    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:368)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2166)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1837)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1713)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1174)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1164)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)

2017-07-19T11:06:39,669 ERROR [main] exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter partition. alter is not possible
    at org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(Hive.java:716)
    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3338)
    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:368)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2166)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1837)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1713)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1543)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1174)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1164)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: InvalidOperationException(message:alter is not possible)
    at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:526)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3560)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
    at com.sun.proxy.$Proxy21.alter_partitions_with_environment_context(Unknown Source)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1486)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
    at com.sun.proxy.$Proxy22.alter_partitions(Unknown Source)
    at org.apache.hadoop.hive.ql.metadata.Hive.alterPartitions(Hive.java:712)
    ... 22 more

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter partition. alter is not possible
2017-07-19T11:06:39,671 ERROR [main] ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter partition. alter is not possible

{code}

Fix proposal is transfer the qualified table name when {{db.alterPartitions}} called.",Fear,-1
HIVE-17368,"In setups where HMS is running as a remote process secured using Kerberos, and when {{DBTokenStore}} is configured as the token store, the HS2 Thrift API call {{GetDelegationToken}} fail with exception trace seen below. HS2 is not able to invoke HMS APIs needed to add/remove/renew tokens from the DB since it is possible that the user which is issue the {{GetDelegationToken}} is not kerberos enabled.

Eg. Oozie submits a job on behalf of user ""Joe"". When Oozie opens a session with HS2 it uses Oozie's principal and creates a proxy UGI with Hive. This principal can establish a transport authenticated using Kerberos. It stores the HMS delegation token string in the sessionConf and sessionToken. Now, lets say Oozie issues a {{GetDelegationToken}} which has {{Joe}} as the owner and {{oozie}} as the renewer in {{GetDelegationTokenReq}}. This API call cannot instantiate a HMSClient and open transport to HMS using the HMSToken string available in the sessionConf, since DBTokenStore uses server HiveConf instead of sessionConf. It tries to establish transport using Kerberos and it fails since user Joe is not Kerberos enabled.


I see the following exception trace in HS2 logs.
{noformat}
2017-08-21T18:07:19,644 ERROR [HiveServer2-Handler-Pool: Thread-61] transport.TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]
        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [?:1.8.0_121]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_121]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_121]
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
        at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]
        at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]
        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]
        at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) ~[?:1.8.0_121]
        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) ~[?:1.8.0_121]
        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) ~[?:1.8.0_121]
        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) ~[?:1.8.0_121]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) ~[?:1.8.0_121]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_121]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_121]
        ... 65 more
{noformat}

On HMS side I see a exception saying 

{noformat}
2017-08-17 11:45:13,655 ERROR org.apache.thrift.server.TThreadPoolServer: [pool-7-thread-34]: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: DIGEST-MD5: IO error acquiring password
{noformat}",,
HIVE-17602,"{code:sql}
hive> CREATE TABLE src (key STRING COMMENT 'default', value STRING COMMENT 'default') STORED AS TEXTFILE;

hive> explain select * from src where key > '4';
Failed with exception wrong number of arguments
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.ExplainTask
{code}

Error stack in hive.log
{noformat}
2017-09-25T21:18:59,591 ERROR [726b5e51-f470-4a79-be8c-95b82a6aa85d main] exec.Task: Failed with exception wrong number of arguments
java.lang.IllegalArgumentException: wrong number of arguments
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:896)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:774)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:668)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:797)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:668)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputList(ExplainTask.java:635)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:968)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:668)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputMap(ExplainTask.java:569)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:954)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:668)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputPlan(ExplainTask.java:1052)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.outputStagePlans(ExplainTask.java:1197)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(ExplainTask.java:275)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.getJSONPlan(ExplainTask.java:220)
	at org.apache.hadoop.hive.ql.exec.ExplainTask.execute(ExplainTask.java:368)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:204)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2190)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1832)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1549)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1304)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1294)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:409)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:827)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:765)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:692)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:234)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{noformat}",,
HIVE-17774,"{noformat}
2017-09-26 10:36:01,979 INFO  [...]: compactor.CompactorMR (CompactorMR.java:launchCompactionJob(295)) - 
Submitting MINOR compaction job ....
 (current delta dirs count=0, obsolete delta dirs count=0. TxnIdRange[9223372036854775807,-9223372036854775808]
...
2017-09-26 10:36:02,350 INFO  [...]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:0
...
2017-09-26 10:36:08,637 INFO  [...]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - 
Job job_1503950256860_15982 failed with state FAILED due to: No of maps and reduces are 0 job_1503950256860_15982
Job commit failed: java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)
	at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)
	at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)
	at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)
	at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)
	at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

Looks like the MR job should not have been attempted in this case.","Sadness, Surprise",-1
HIVE-17829,"Stack
{code}
2017-10-09T09:39:54,804 ERROR [HiveServer2-Background-Pool: Thread-95]: metadata.Table (Table.java:getColsInternal(642)) - Unable to get field from serde: org.apache.hadoop.hive.hbase.HBaseSerDe
java.lang.ArrayIndexOutOfBoundsException: 1
        at java.util.Arrays$ArrayList.get(Arrays.java:3841) ~[?:1.8.0_77]
        at org.apache.hadoop.hive.serde2.BaseStructObjectInspector.init(BaseStructObjectInspector.java:104) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.init(LazySimpleStructObjectInspector.java:97) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.<init>(LazySimpleStructObjectInspector.java:77) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.getLazySimpleStructObjectInspector(LazyObjectInspectorFactory.java:115) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.hbase.HBaseLazyObjectFactory.createLazyHBaseStructInspector(HBaseLazyObjectFactory.java:79) ~[hive-hbase-handler-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.hbase.HBaseSerDe.initialize(HBaseSerDe.java:127) ~[hive-hbase-handler-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:531) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:424) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:411) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:279) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:261) ~[hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:639) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:622) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:833) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4228) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:347) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1905) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1607) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1354) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1123) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1116) [hive-exec-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) [hive-service-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_77]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866) [hadoop-common-2.7.3.2.6.2.0-205.jar:?]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:348) [hive-service-2.1.0.2.6.2.0-205.jar:2.1.0.2.6.2.0-205]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
{code}

Steps to Repro:

{code}
Create Hbase Table:
========================
create 'hbase_avro_table', 'test_col_fam', 'test_col'

Create Hive Table:
=========================
CREATE EXTERNAL TABLE test_hbase_avro2
ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe'
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
""hbase.columns.mapping"" = "":key,test_col_fam:test_col"",
""test_col_fam.test_col.serialization.type"" = ""avro"",
""test_col_fam.test_col.avro.schema.url"" = ""hdfs://rpathak-h1.openstacklocal:8020/user/hive/schema.avsc"")
TBLPROPERTIES (
""hbase.table.name"" = ""hbase_avro_table"",
""hbase.mapred.output.outputtable"" = ""hbase_avro_table"",
""hbase.struct.autogenerate""=""true"",
""avro.schema.literal""='{
""type"": ""record"",
""name"": ""test_hbase_avro"",
""fields"": [
{ ""name"":""test_col"", ""type"":""string""}
]
}');
{code}

The same query works with Hive 1.2.1",,
HIVE-17900,"{noformat}
2017-10-16 09:01:51,255 ERROR [haddl0007.mycenterpointenergy.com-51]: ql.Driver (SessionState.java:printError(993)) - FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement
org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)
        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)
        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)

2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=compile start=1508162511253 end=1508162511255 duration=2 from=org.apache.hadoop.hive.ql.Driver>
2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: ql.Driver (Driver.java:compile(559)) - We are resetting the hadoop caller context to
2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=releaseLocks start=1508162511255 end=1508162511255 duration=0 from=org.apache.hadoop.hive.ql.Driver>
2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(183)) - Closing tez session default? false
2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: tez.TezSessionState (TezSessionState.java:close(294)) - Closing Tez Session
2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: client.TezClient (TezClient.java:stop(518)) - Shutting down Tez Session, sessionName=HIVE-ae652f03-72c7-4ca8-a2d8-05dcc7392f4f, applicationId=application_1507779664083_0159
2017-10-16 09:01:51,279 ERROR [haddl0007.mycenterpointenergy.com-51]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:3723,dbname:mobiusad,tableName:zces_img_data_small_pt,partName:month=201608/dates=9,state:^@,type:MAJOR,properties:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking failed to avoid repeated failures, java.io.IOException: Could not update stats for table mobiusad.zces_img_data_small_pt/month=201608/dates=9 due to: (40000,FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement,42000line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement)
        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:296)
        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)
        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)
{noformat}
",,
HIVE-18001,"
{code}
hive> show create table inventory;
OK
CREATE TABLE `inventory`(
  `inv_item_sk` bigint,
  `inv_warehouse_sk` bigint,
  `inv_quantity_on_hand` int)
PARTITIONED BY (
  `inv_date_sk` bigint)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
LOCATION
  'hdfs://ctr-e134-1499953498516-233086-01-000002.hwx.site:8020/apps/hive/warehouse/tpcds_bin_partitioned_orc_1000.db/inventory'
TBLPROPERTIES (
  'transient_lastDdlTime'='1508284425')
Time taken: 0.25 seconds, Fetched: 16 row(s)

hive> alter table inventory add constraint pk_in primary key (inv_date_sk, inv_item_sk, inv_warehouse_sk) disable novalidate rely;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidObjectException(message:Parent column not found: inv_date_sk)

{code}

Exception from the log
{code}
2017-11-07T18:17:50,516 ERROR [d4ed6f97-20ea-4bc8-a046-b0646f483a20 main] exec.DDLTask: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: InvalidObjectException(message:Parent column not found: inv_date_sk)
        at org.apache.hadoop.hive.ql.metadata.Hive.addPrimaryKey(Hive.java:4668) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.DDLTask.addConstraints(DDLTask.java:4356) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:413) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:206) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2276) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1906) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1623) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1362) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1352) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:187) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:409) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:827) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:765) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:692) ~[hive-cli-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233) ~[hadoop-common-2.7.3.2.6.2.0-205.jar:?]
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148) ~[hadoop-common-2.7.3.2.6.2.0-205.jar:?]
Caused by: org.apache.hadoop.hive.metastore.api.InvalidObjectException: Parent column not found: inv_date_sk
        at org.apache.hadoop.hive.metastore.ObjectStore.addPrimaryKeys(ObjectStore.java:4190) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.ObjectStore.addPrimaryKeys(ObjectStore.java:4163) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at com.sun.proxy.$Proxy39.addPrimaryKeys(Unknown Source) ~[?:?]
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_primary_key(HiveMetaStore.java:1718) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at com.sun.proxy.$Proxy42.add_primary_key(Unknown Source) ~[?:?]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.addPrimaryKey(HiveMetaStoreClient.java:819) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at com.sun.proxy.$Proxy43.addPrimaryKey(Unknown Source) ~[?:?]
        at org.apache.hadoop.hive.ql.metadata.Hive.addPrimaryKey(Hive.java:4666) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        ... 21 more
2017-11-07T18:17:50,517  INFO [d4ed6f97-20ea-4bc8-a046-b0646f483a20 main] hooks.ATSHook: Created ATS Hook
2017-11-07T18:17:50,519 ERROR [d4ed6f97-20ea-4bc8-a046-b0646f483a20 main] ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidObjectException(message:Parent column not found: inv_date_sk)
2017-11-07T18:17:50,519  INFO [d4ed6f97-20ea-4bc8-a046-b0646f483a20 main] ql.Driver: Completed executing command(queryId=root_20171107181750_b5b6298f-57aa-4457-9848-368882dbf020); Time taken: 0.086 seconds
{code}",,
HIVE-18046,"The materialized view impl breaks old metastore sql write access, by complaining that the new table creation does not set this column up.

{code}
  `IS_REWRITE_ENABLED` bit(1) NOT NULL,
{code}

{{NOT NULL DEFAULT 0}} would allow old metastore direct sql compatibility (not thrift).

{code}
2017-11-09T07:11:58,331 ERROR [HiveServer2-Background-Pool: Thread-2354] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MTable@249dbf1"" using statement ""INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?)"" failed : Field 'IS_REWRITE_ENABLED' doesn't have a default value
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:720)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:740)
at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1038)
{code}",Anger,-1
HIVE-18090,"steps to recreate the issue. assuming two users 
* test
* another 

create two jceks files for each user and place them on hdfs with access to that file only allowed to the user. hdfs locations with permissions 
{code}
-rwx------   1 another another        492 2017-11-16 13:06 /user/another/another.jceks
-rwx------   1 test test        489 2017-11-16 13:05 /user/test/test.jceks
{code}

password used to create 
* /user/another/another.jceks -- another
* /user/test/test.jceks -- test

on core-site.xml 
{code}
    <property>
        <name>hadoop.proxyuser.[superuser].hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.[superuser].groups</name>
        <value>*</value>
    </property>
{code}
and restart hdfs.
enable ACID on HS2 (change the required properties).additional changes on  hiveserver2 configs 
{code}
* hive.metastore.warehouse.dir=file:///tmp/hive/test-warehouse
* hive.server2.enable.doAs=true
* remove javax.jdo.option.ConnectionPassword property from hive-site.xml
{code}
start hiveserver2

connect to the server using beeline using any user:
{code}
create table a (i int, b string);
insert into a values (0 , '0'), (1 , '1'), (2 , '2'), (3 , '3'), (4 , '4'), (5 , '5'), (6 , '6'), (7 , '7'), (8 , '8'), (9 , '9'), (10 , '10'), (11 , '11'), (12 , '12'), (13 , '13'), (14 , '14'), (15 , '15'), (16 , '16'), (17 , '17'), (18 , '18'), (19 , '19'), (20 , '20'), (21 , '21'), (22 , '22'), (23 , '23'), (24 , '24'), (25 , '25'), (26 , '26'), (27 , '27'), (28 , '28'), (29 , '29'), (30 , '30'), (31 , '31'), (32 , '32'), (33 , '33'), (34 , '34'), (35 , '35'), (36 , '36'), (37 , '37'), (38 , '38'), (39 , '39'), (40 , '40'), (41 , '41'), (42 , '42'), (43 , '43'), (44 , '44'), (45 , '45'), (46 , '46'), (47 , '47'), (48 , '48'), (49 , '49'), (50 , '50'), (51 , '51'), (52 , '52'), (53 , '53'), (54 , '54'), (55 , '55'), (56 , '56'), (57 , '57'), (58 , '58'), (59 , '59'), (60 , '60'), (61 , '61'), (62 , '62'), (63 , '63'), (64 , '64'), (65 , '65'), (66 , '66'), (67 , '67'), (68 , '68'), (69 , '69'), (70 , '70'), (71 , '71'), (72 , '72'), (73 , '73'), (74 , '74'), (75 , '75'), (76 , '76'), (77 , '77'), (78 , '78'), (79 , '79'), (80 , '80'), (81 , '81'), (82 , '82'), (83 , '83'), (84 , '84'), (85 , '85'), (86 , '86'), (87 , '87'), (88 , '88'), (89 , '89'), (90 , '90'), (91 , '91'), (92 , '92'), (93 , '93'), (94 , '94'), (95 , '95'), (96 , '96'), (97 , '97'), (98 , '98'), (99 , '99');
{code}

exit beeline and connect with user another 
{code}
./beeline -u ""jdbc:hive2://localhost:10000/default?hive.strict.checks.cartesian.product=false;hive.txn.timeout=4s;hive.txn.heartbeat.threadpool.size=1;hadoop.security.credential.provider.path=jceks://hdfs/user/another/another.jceks;ssl.server.keystore.keypassword=another"" -n another

create table another_a_acid (i int, b string) clustered by (i) into 8 buckets stored as orc tblproperties('transactional'='true');

insert overwrite table another_a_acid select a2.i, a3.b from a a1 join a a2 join a a3 on 1=1;
{code}

open another beeline session with user test:
{code}
./beeline -u ""jdbc:hive2://localhost:10000/default?hive.strict.checks.cartesian.product=false;hive.txn.timeout=4s;hive.txn.heartbeat.threadpool.size=1;hadoop.security.credential.provider.path=jceks://hdfs/user/test/test.jceks;ssl.server.keystore.keypassword=test"" -n test

create table a_acid (i int, b string) clustered by (i) into 8 buckets stored as orc tblproperties('transactional'='true');

insert overwrite table a_acid select a2.i, a3.b from a a1 join a a2 join a a3 on 1=1;
{code}

fails with exception 
{code}
2017-11-17T12:15:52,664 DEBUG [Heartbeater-1] retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException: Permission denied: user=test, access=EXECUTE, inode=""/user/another/another.jceks"":another:another:drwx------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1498) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ipc.Client.call(Client.java:1398) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at com.sun.proxy.$Proxy30.getFileInfo(Unknown Source) ~[?:?]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818) ~[hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at com.sun.proxy.$Proxy31.getFileInfo(Unknown Source) [?:?]
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438) [hadoop-hdfs-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.keystoreExists(JavaKeyStoreProvider.java:65) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:105) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:49) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:41) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:100) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:61) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1992) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1972) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.metastore.conf.MetastoreConf.getPassword(MetastoreConf.java:1334) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:571) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:312) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) [hadoop-common-2.7.3.2.6.1.0-SNAPSHOT.jar:?]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:677) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:643) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:637) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:544) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7516) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:169) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:77) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at sun.reflect.GeneratedConstructorAccessor148.newInstance(Unknown Source) [?:1.8.0_112]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_112]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1445) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4051) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4103) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4083) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:158) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:610) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:878) [hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_112]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_112]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_112]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-11-17T12:15:52,670 ERROR [Heartbeater-1] metastore.RetryingHMSHandler: java.lang.RuntimeException: Error getting metastore password: Configuration problem with provider path.
	at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:577)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:312)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:59)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:677)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:643)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:637)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:544)
	at sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:80)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7516)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:169)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:77)
	at sun.reflect.GeneratedConstructorAccessor148.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1445)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4051)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4103)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4083)
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:158)
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:610)
	at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:878)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Configuration problem with provider path.
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:2012)
	at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1972)
	at org.apache.hadoop.hive.metastore.conf.MetastoreConf.getPassword(MetastoreConf.java:1334)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDataSourceProps(ObjectStore.java:571)
	... 39 more
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=test, access=EXECUTE, inode=""/user/another/another.jceks"":another:another:drwx------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2167)
	at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)
	at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1438)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1447)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.keystoreExists(JavaKeyStoreProvider.java:65)
	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:105)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:49)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:41)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:100)
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:61)
	at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1992)
	... 42 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=test, access=EXECUTE, inode=""/user/another/another.jceks"":another:another:drwx------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:259)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1955)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:109)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4111)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
	at org.apache.hadoop.ipc.Client.call(Client.java:1498)
	at org.apache.hadoop.ipc.Client.call(Client.java:1398)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy30.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:818)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)
	at com.sun.proxy.$Proxy31.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2165)
	... 54 more

{code}

above will only help in recreating the issue, if the _insert overwrite_ query takes longer than _hive.txn.timeout / 2 = 4 / 2 = 2seconds_",,
HIVE-18148,"The stack trace is:
{noformat}
2017-11-27T10:32:38,752 ERROR [e6c8aab5-ddd2-461d-b185-a7597c3e7519 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.physical.SparkDynamicPartitionPruningResolver$SparkDynamicPartitionPruningDispatcher.dispatch(SparkDynamicPartitionPruningResolver.java:100)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)
        at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)
        at org.apache.hadoop.hive.ql.optimizer.physical.SparkDynamicPartitionPruningResolver.resolve(SparkDynamicPartitionPruningResolver.java:74)
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.optimizeTaskPlan(SparkCompiler.java:568)
{noformat}
At this stage, there shouldn't be a DPP sink whose target map work is null. The root cause seems to be a malformed operator tree generated by SplitOpTreeForDPP.","Sadness, Surprise",-1
HIVE-18250,"{code}
 create table t1 (a int);
explain select t1.a as a1, min(t1.a) as a from t1 group by t1.a;
{code}

CBO gets turned off with:
{code}
WARN [2e80e34e-dc46-49cf-88bf-2c24c0262d41 main] parse.RowResolver: Found duplicate column alias in RR: null.a => {null, a1, _col0: int} adding null.a => {null, null, _col1: int}
2017-12-07T15:27:47,651 ERROR [2e80e34e-dc46-49cf-88bf-2c24c0262d41 main] parse.CalcitePlanner: CBO failed, skipping CBO.
org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException: Cannot add column to RR: null.a => _col1: int due to duplication, see previous warnings
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genSelectLogicalPlan(CalcitePlanner.java:3985) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genLogicalPlan(CalcitePlanner.java:4313) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1392) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1322) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
{code}
After that non-CBO path completes the query.",,
HIVE-18360,"{noformat}
2018-01-03T01:09:23,822 ERROR [HiveServer2-Background-Pool: Thread-409]: exec.Task (:()) - Failed to execute tez graph.
java.lang.NullPointerException: null
at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.ensureLocalResources(TezSessionState.java:600) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:264) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.openInternal(TezSessionPoolSession.java:127) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:223) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:352) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:188) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2257) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1909) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1640) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1385) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1378) ~[hive-exec-3.0.0.3.0.0.0-663.jar:3.0.0.3.0.0.0-663]
{noformat}",,
HIVE-18393,"TimeStamp, Decimal, Double, Float, BigInt, Int, SmallInt, Tinyint and Boolean when read as String, Varchar or Char should return the correct data.  Now this results in error for parquet tables.

Test Case:

{code}
drop table if exists testAltCol;
create table testAltCol
(cId   	    TINYINT,
 cTimeStamp TIMESTAMP,
 cDecimal   DECIMAL(38,18),
 cDouble    DOUBLE,
 cFloat	    FLOAT,
 cBigInt    BIGINT,
 cInt	    INT,
 cSmallInt  SMALLINT,
 cTinyint   TINYINT,
 cBoolean   BOOLEAN);

insert into testAltCol values
(1,
 '2017-11-07 09:02:49.999999999',
 12345678901234567890.123456789012345678,
 1.79e308,
 3.4e38,
 1234567890123456789,
 1234567890,
 12345,
 123,
 TRUE);

insert into testAltCol values
(2,
 '1400-01-01 01:01:01.000000001',
 1.1,
 2.2,
 3.3,
 1,
 2,
 3,
 4,
 FALSE);

insert into testAltCol values
(3,
 '1400-01-01 01:01:01.000000001',
 10.1,
 20.2,
 30.3,
 1234567890123456789,
 1234567890,
 12345,
 123,
 TRUE);

select cId, cTimeStamp from testAltCol order by cId;
select cId, cDecimal, cDouble, cFloat from testAltCol order by cId;
select cId, cBigInt, cInt, cSmallInt, cTinyint from testAltCol order by cId;
select cId, cBoolean from testAltCol order by cId;

drop table if exists testAltColP;
create table testAltColP stored as parquet as select * from testAltCol;

select cId, cTimeStamp from testAltColP order by cId;
select cId, cDecimal, cDouble, cFloat from testAltColP order by cId;
select cId, cBigInt, cInt, cSmallInt, cTinyint from testAltColP order by cId;
select cId, cBoolean from testAltColP order by cId;

alter table testAltColP replace columns
(cId  	    TINYINT,
 cTimeStamp STRING,
 cDecimal   STRING,
 cDouble    STRING,
 cFloat	    STRING,
 cBigInt    STRING,
 cInt	    STRING,
 cSmallInt  STRING,
 cTinyint   STRING,
 cBoolean   STRING);

select cId, cTimeStamp from testAltColP order by cId;
select cId, cDecimal, cDouble, cFloat from testAltColP order by cId;
select cId, cBigInt, cInt, cSmallInt, cTinyint from testAltColP order by cId;
select cId, cBoolean from testAltColP order by cId;

alter table testAltColP replace columns
(cId  	    TINYINT,
 cTimeStamp VARCHAR(100),
 cDecimal   VARCHAR(100),
 cDouble    VARCHAR(100),
 cFloat	    VARCHAR(100),
 cBigInt    VARCHAR(100),
 cInt	    VARCHAR(100),
 cSmallInt  VARCHAR(100),
 cTinyint   VARCHAR(100),
 cBoolean   VARCHAR(100));

select cId, cTimeStamp from testAltColP order by cId;
select cId, cDecimal, cDouble, cFloat from testAltColP order by cId;
select cId, cBigInt, cInt, cSmallInt, cTinyint from testAltColP order by cId;
select cId, cBoolean from testAltColP order by cId;

alter table testAltColP replace columns
(cId  	    TINYINT,
 cTimeStamp CHAR(100),
 cDecimal   CHAR(100),
 cDouble    CHAR(100),
 cFloat	    CHAR(100),
 cBigInt    CHAR(100),
 cInt	    CHAR(100),
 cSmallInt  CHAR(100),
 cTinyint   CHAR(100),
 cBoolean   CHAR(100));

select cId, cTimeStamp from testAltColP order by cId;
select cId, cDecimal, cDouble, cFloat from testAltColP order by cId;
select cId, cBigInt, cInt, cSmallInt, cTinyint from testAltColP order by cId;
select cId, cBoolean from testAltColP order by cId;
drop table if exists testAltColP;
{code}

{code}
Error:
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask

Excerpt for log:
2018-01-05T15:54:05,756 ERROR [LocalJobRunner Map Task Executor #0] mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.UnsupportedOperationException: Cannot inspect org.apache.hadoop.hive.serde2.io.TimestampWritable
	at org.apache.hadoop.hive.ql.io.parquet.serde.primitive.ParquetStringInspector.getPrimitiveJavaObject(ParquetStringInspector.java:77)
{code}",Surprise,-1
HIVE-18413,"exposed by: HIVE-18359

in case of vectorization, the summary row object was left as is (presumed null earlier); which may cause it to be inconsistent isNull conditions in .VectorHashKeyWrapperBatch

issue happens only if:

* vectorizable groupby
* groupping set contains empty
* non-trivial empty; mapper is run
* groupping key is select ; with a type which is backed by a bytea; ex:string

{code}
set hive.vectorized.execution.enabled=true;
create table tx2 (a integer,b integer,c integer,d double,u string,bi binary) stored as orc;

insert into tx2 values
(1,2,3,1.1,'x','b'),
(3,2,3,1.1,'y','b');

select  sum(a),
        u,
        bi,
        'asd',
        grouping(bi),
        'NULL,1' as expected
from    tx2
where   a=2
group by a,u,bi grouping sets ( u, (), bi);
{code}

causes:

{code}
Caused by: java.lang.NullPointerException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:173)
        at org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignRowColumn(VectorHashKeyWrapperBatch.java:1065)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeSingleRow(VectorGroupByOperator.java:1134)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$800(VectorGroupByOperator.java:74)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.close(VectorGroupByOperator.java:862)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(VectorGroupByOperator.java:1176)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:705)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383)
        ... 16 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1515531021543_0001_12_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
2018-01-09T12:50:30,611 DEBUG [01fdcefd-40b0-45a6-8e5b-b1cd14241088 main] ql.Driver: Shutting down query 
{code}",Fear,-1
HIVE-18417,"{noformat}
2018-01-09T00:11:23,010 ERROR [HiveServer2-Background-Pool: Thread-208]: exec.Task (TezTask.java:execute(277)) - Failed to execute tez graph.
java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.cleanupScratchDir(TezSessionState.java:659) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.close(TezSessionState.java:637) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.close(TezSessionPoolSession.java:114) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.destroy(TezSessionPoolManager.java:364) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.destroy(TezSessionPoolSession.java:208) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:539) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
{noformat}

{noformat}
java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.ensureLocalResources(TezSessionState.java:558) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:355) ~[hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:188) [hive-exec-3.0.0.3.0.0.0-688.jar:3.0.0.3.0.0.0-688]

{noformat}",,
HIVE-18494,"{code:java}
018-01-19T02:12:34,457 WARN [pool-10-thread-1] metastore.ObjectStore: Falling back to ORM path due to direct SQL failure (this is not an error): java.lang.String cannot be cast to [Ljava.lang.Object; at

org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTables(MetaStoreDirectSql.java:412) at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1336)

2018-01-19T02:12:34,457 DEBUG [pool-10-thread-1] metastore.ObjectStore: Full DirectSQL callstack for debugging (note: this is not an error)

java.lang.ClassCastException: java.lang.String cannot be cast to [Ljava.lang.Object;

    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTables(MetaStoreDirectSql.java:412) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]

    at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1336) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]

    at org.apache.hadoop.hive.metastore.ObjectStore$2.getSqlResult(ObjectStore.java:1332) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]

    at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3200) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]

    at org.apache.hadoop.hive.metastore.ObjectStore.getTablesInternal(ObjectStore.java:1344) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]

    at org.apache.hadoop.hive.metastore.ObjectStore.getTables(ObjectStore.java:1323) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]
{code}

The DataNucleus layer returns List<String> for this case, when exactly one column has been selected.

And MetastoreDirectSQL is disabled for all further queries.",,
HIVE-18574,"Removing netty from Tez libs causes

{code}
2018-01-29T18:28:49,995 ERROR [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exception
java.lang.NoClassDefFoundError: org/jboss/netty/channel/group/ChannelGroup
	at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.serviceStart(LlapDaemon.java:410) 
{code}",,
HIVE-18595,"{code}

2018-01-31T12:59:45,464 ERROR [10e97c86-7f90-406b-a8fa-38be5d3529cc main] ql.Driver: FAILED: SemanticException [Error 10014]: Line 3:456 Wrong arguments ''yyyy-MM-dd HH:mm:ss'': The function UNIX_TIMESTAMP takes only string/date/timestamp types
org.apache.hadoop.hive.ql.parse.SemanticException: Line 3:456 Wrong arguments ''yyyy-MM-dd HH:mm:ss'': The function UNIX_TIMESTAMP takes only string/date/timestamp types
 at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1394)
 at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
 at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
 at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
 at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76)
 at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
 at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:235)
 at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:181)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:11847)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:11780)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genGBLogicalPlan(CalcitePlanner.java:3140)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.genLogicalPlan(CalcitePlanner.java:4330)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1407)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1354)
 at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:118)
 at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:1052)
 at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:154)
 at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:111)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1159)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:1175)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:422)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11393)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:304)
 at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:268)
 at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:163)
 at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:268)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:639)
 at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1504)
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1632)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1395)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1382)
 at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:240)
 at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:343)
 at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1331)
 at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1305)
 at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:173)
 at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:104)
 at org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver(TestMiniDruidCliDriver.java:59)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:92)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.junit.runners.Suite.runChild(Suite.java:127)
 at org.junit.runners.Suite.runChild(Suite.java:26)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:73)
 at org.junit.rules.RunRules.evaluate(RunRules.java:20)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:369)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:275)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:239)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:160)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:373)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:334)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:119)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:407)
Caused by: org.apache.hadoop.hive.ql.exec.UDFArgumentException: The function UNIX_TIMESTAMP takes only string/date/timestamp types
 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initializeInput(GenericUDFToUnixTimeStamp.java:110)
 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.initializeInput(GenericUDFUnixTimeStamp.java:43)
 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.initialize(GenericUDFToUnixTimeStamp.java:67)
 at org.apache.hadoop.hive.ql.udf.generic.GenericUDF.initializeAndFoldConstants(GenericUDF.java:147)
 at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:259)
 at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1132)
 at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1386)
 ... 76 more



{code}",,
HIVE-18606,"{noformat}
@Test
public void testCtasEmpty() throws Exception {
  MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);
  runStatementOnDriver(""create table myctas stored as ORC as"" +
      "" select a, b from "" + Table.NONACIDORCTBL);
  List<String> rs = runStatementOnDriver(""select ROW__ID, a, b, INPUT__FILE__NAME"" +
      "" from myctas order by ROW__ID"");
}
{noformat}
{noformat}
2018-02-01T19:08:52,813 INFO  [HiveServer2-Background-Pool: Thread-463]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(822)) - 114: Done cleaning up thread local RawStore
2018-02-01T19:08:52,813 INFO  [HiveServer2-Background-Pool: Thread-463]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(305)) - ugi=hive ip=unknown-ip-addr      cmd=Done cleaning up thread local RawStore
2018-02-01T19:08:52,815 ERROR [HiveServer2-Background-Pool: Thread-463]: exec.Task (SessionState.java:printError(1228)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.moveAcidFiles(Hive.java:3816)
        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:298)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2267)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1919)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1651)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1395)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1388)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:253)
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:92)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:345)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:358)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

2018-02-01T19:08:52,815 ERROR [HiveServer2-Background-Pool: Thread-463]: ql.Driver (SessionState.java:printError(1228)) - FAILED: Execution Error, return code 1 from {noformat}
",,
HIVE-18886,"At 200+ sessions on a single HS2, the DbLock impl fails to propagate mysql exceptions

{code}
2018-03-06T22:55:16,197 ERROR [HiveServer2-Background-Pool: Thread-12867]: ql.Driver (:()) - FAILED: Error in acquiring locks: null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.DatabaseProduct.isDeadlock(DatabaseProduct.java:56)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable(TxnHandler.java:2459)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:499)
{code}

{code}
    return e instanceof SQLTransactionRollbackException
        || ((dbProduct == MYSQL || dbProduct == POSTGRES || dbProduct == SQLSERVER)
            && e.getSQLState().equals(""40001""))
        || (dbProduct == POSTGRES && e.getSQLState().equals(""40P01""))
        || (dbProduct == ORACLE && (e.getMessage().contains(""deadlock detected"")
            || e.getMessage().contains(""can't serialize access for this transaction"")));
{code}",,
HIVE-18944,"groupingSetsPosition is set to -1 in case there are no grouping sets; however DPP calls the constructor with 0 

this could potentially trigger an unwanted emittion of a summary row
{code}
2018-03-13T05:58:16,226 ERROR [TezTR-881987_1_5_1_1_0] tez.TezProcessor: java.lang.RuntimeException: Hive Runtime Error while closing operators: 0
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:407)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:284)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.setLongValue(VectorHashKeyWrapperBatch.java:994)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.close(VectorGroupByOperator.java:461)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(VectorGroupByOperator.java:1179)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:722)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:746)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:746)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383)
{code}
",Surprise,-1
HIVE-19155,"If you try to insert data around the daylight saving time hour the query fails with following exception
{code}
2018-04-10T11:24:58,836 ERROR [065fdaa2-85f9-4e49-adaf-3dc14d51be90 main] exec.DDLTask: Failed
org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hive.druid.io.druid.java.util.common.UOE: Cannot add overlapping segments [2015-03-08T05:00:00.000Z/2015-03-09T05:00:00.000Z and 2015-03-09T04:00:00.000Z/2015-03-10T04:00:00.000Z] with the same version [2018-04-10T11:24:48.388-07:00]
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:914) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:919) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4831) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:394) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2443) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2114) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1797) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1538) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1532) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:204) [hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) [hive-cli-3.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) [hive-cli-3.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) [hive-cli-3.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) [hive-cli-3.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1455) [hive-it-util-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1429) [hive-it-util-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:177) [hive-it-util-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:104) [hive-it-util-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver(TestMiniDruidCliDriver.java:59) [test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_92]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_92]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_92]
{code}

You can reproduce this using the following DDL 
{code}
create database druid_test;
use druid_test;

create table test_table(`timecolumn` timestamp, `userid` string, `num_l` float);

insert into test_table values ('2015-03-08 00:00:00', 'i1-start', 4);
insert into test_table values ('2015-03-08 23:59:59', 'i1-end', 1);

insert into test_table values ('2015-03-09 00:00:00', 'i2-start', 4);
insert into test_table values ('2015-03-09 23:59:59', 'i2-end', 1);

insert into test_table values ('2015-03-10 00:00:00', 'i3-start', 2);
insert into test_table values ('2015-03-10 23:59:59', 'i3-end', 2);

CREATE TABLE druid_table
STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'
TBLPROPERTIES (""druid.segment.granularity"" = ""DAY"")
AS
select cast(`timecolumn` as timestamp with local time zone) as `__time`, `userid`, `num_l` FROM test_table;
{code}

The fix is to always adjust the Druid segments identifiers to UTC.
",,
HIVE-19316,"The stack trace:
{noformat}
2018-04-26T20:17:37,674 ERROR [pool-7-thread-11] metastore.RetryingHMSHandler: java.lang.ClassCastException: org.apache.hadoop.hive.metastore.api.LongColumnStatsData cannot be cast to org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector
        at org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.merge(LongColumnStatsMerger.java:30)
        at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.mergeColStats(MetaStoreUtils.java:1052)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.set_aggr_stats_for(HiveMetaStore.java:7202)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy26.set_aggr_stats_for(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:16795)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:16779)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}",,
HIVE-19646,"Exception in proto logging hook on secure cluster.

{code}
2018-05-18T04:48:01,136 ERROR [Hive Hook Proto Log Writer 0]: hooks.HiveProtoLoggingHook (:()) - Error writing proto message for query hive_20180518043717_ca3ab4df-6cab-4920-aa44-2340ae246ad2, eventType: QUERY_SUBMITTED:
java.io.IOException: Filesystem closed
 at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:472) ~[hadoop-hdfs-client-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1652) ~[hadoop-hdfs-client-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1569) ~[hadoop-hdfs-client-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1566) ~[hadoop-hdfs-client-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1581) ~[hadoop-hdfs-client-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) ~[hadoop-common-3.0.0.3.0.0.0-1298.jar:?]
 at org.apache.hadoop.hive.ql.hooks.DatePartitionedLogger.getPathForDate(DatePartitionedLogger.java:89) ~[hive-exec-3.0.0.3.0.0.0-1298.jar:3.0.0.3.0.0.0-1298]
 at org.apache.hadoop.hive.ql.hooks.DatePartitionedLogger.getWriter(DatePartitionedLogger.java:73) ~[hive-exec-3.0.0.3.0.0.0-1298.jar:3.0.0.3.0.0.0-1298]
 at org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook$EventLogger.writeEvent(HiveProtoLoggingHook.java:283) ~[hive-exec-3.0.0.3.0.0.0-1298.jar:3.0.0.3.0.0.0-1298]
 at org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook$EventLogger.lambda$generateEvent$1(HiveProtoLoggingHook.java:274) ~[hive-exec-3.0.0.3.0.0.0-1298.jar:3.0.0.3.0.0.0-1298]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]
{code}",,
HIVE-19771,"Otherwise we may throw an Exception.

{noformat}
2018-05-26T00:30:22,335 DEBUG [HiveServer2-Background-Pool: Thread-631]: stats.StatsUtils (:()) - Estimated average row size: 372
2018-05-26T00:30:22,352 DEBUG [HiveServer2-Background-Pool: Thread-631]: calcite.RelOptHiveTable (:()) - Stats for column a in table basetable_rebuild stored in cache
2018-05-26T00:30:22,352 DEBUG [HiveServer2-Background-Pool: Thread-631]: calcite.RelOptHiveTable (:()) -  colName: a colType: int countDistincts: 4 numNulls: 1 avgColLen: 4.0 numTrues: 0 numFalses: 0 Range: [ min: -9223372036854775808 max: 9223372036854775807 ] isPrimaryKey: false isEstimated: true
2018-05-26T00:30:22,352 DEBUG [HiveServer2-Background-Pool: Thread-631]: calcite.RelOptHiveTable (:()) - Stats for column b in table basetable_rebuild stored in cache
2018-05-26T00:30:22,352 DEBUG [HiveServer2-Background-Pool: Thread-631]: calcite.RelOptHiveTable (:()) -  colName: b colType: varchar(256) countDistincts: 4 numNulls: 1 avgColLen: 256.0 numTrues: 0 numFalses: 0 isPrimaryKey: false isEstimated: true
2018-05-26T00:30:22,352 ERROR [HiveServer2-Background-Pool: Thread-631]: calcite.RelOptHiveTable (:()) - No Stats for default@basetable_rebuild, Columns: a, b
java.lang.RuntimeException: No Stats for default@basetable_rebuild, Columns: a, b
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.updateColStats(RelOptHiveTable.java:586) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.getColStat(RelOptHiveTable.java:606) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.getColStat(RelOptHiveTable.java:592) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan.getColStat(HiveTableScan.java:155) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:78) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.getDistinctRowCount(HiveRelMdDistinctRowCount.java:65) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at GeneratedMetadataHandler_DistinctRowCount.getDistinctRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_DistinctRowCount.getDistinctRowCount(Unknown Source) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getDistinctRowCount(RelMetadataQuery.java:781) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:207) ~[calcite-core-1.16.0.jar:1.16.0]
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:235) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.rel.externalize.RelWriterImpl.explain_(RelWriterImpl.java:100) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.rel.externalize.RelWriterImpl.done(RelWriterImpl.java:156) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.rel.AbstractRelNode.explain(AbstractRelNode.java:312) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.plan.RelOptUtil.toString(RelOptUtil.java:1991) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1898) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction.apply(CalcitePlanner.java:1613) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.calcite.tools.Frameworks$1.apply(Frameworks.java:118) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.prepare.CalcitePrepareImpl.perform(CalcitePrepareImpl.java:1052) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.tools.Frameworks.withPrepare(Frameworks.java:154) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.calcite.tools.Frameworks.withPlanner(Frameworks.java:111) ~[calcite-core-1.16.0.jar:1.16.0]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.logicalPlan(CalcitePlanner.java:1418) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genLogicalPlan(CalcitePlanner.java:369) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.parseQuery(HiveMaterializedViewsRegistry.java:416) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.addMaterializedView(HiveMaterializedViewsRegistry.java:225) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.createMaterializedView(HiveMaterializedViewsRegistry.java:188) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.MaterializedViewTask.execute(MaterializedViewTask.java:61) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2479) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2150) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1826) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1567) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1561) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.0.0.jar:3.0.0-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:221) ~[hive-service-3.0.0.jar:3.0.0]
        at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.0.0.jar:3.0.0]
{noformat}",,
HIVE-19917,"The actual issues is fixed by HIVE-19861.
This is a follow up to add a test case.

Issue:
{noformat}
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Can not create a Path from a null string
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:940) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:945) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.exec.DDLTask.createTableLike(DDLTask.java:5099) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:433) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:195) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:106) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:288) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:658) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1813) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1760) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1755) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:194) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:257) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:243) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562) ~[hive-service-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.IllegalArgumentException: Can not create a Path from a null string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:164) ~[hadoop-common-3.0.0.3.0.0.0-1485.jar:?]
	at org.apache.hadoop.fs.Path.<init>(Path.java:180) ~[hadoop-common-3.0.0.3.0.0.0-1485.jar:?]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.createTempTable(SessionHiveMetaStoreClient.java:459) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:117) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:831) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:816) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at sun.reflect.GeneratedMethodAccessor124.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at com.sun.proxy.$Proxy55.createTable(Unknown Source) ~[?:?]
	at sun.reflect.GeneratedMethodAccessor124.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2768) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	at com.sun.proxy.$Proxy55.createTable(Unknown Source) ~[?:?]
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:929) ~[hive-exec-3.0.0.3.0.0.0-1485.jar:3.0.0.3.0.0.0-1485]
	... 27 more
2018-06-14T17:53:32,112 ERROR [07758225-f4e7-4fc2-a9e5-c6ed19e9fcfd HiveServer2-Handler-Pool: Thread-143]: metadata.Hive (:()) - Table tpch.tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 not found: hive.tpch.tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39 table not found
2018-06-14T17:53:32,113 ERROR [07758225-f4e7-4fc2-a9e5-c6ed19e9fcfd HiveServer2-Handler-Pool: Thread-143]: ql.Driver (:()) - FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39
org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:198)
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:106)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:288)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:658)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1813)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1760)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1755)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:194)
	at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:257)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:243)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:647)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found tbl_export_05da5215_6695_420e_99e9_24a9bb5d1a39
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1141)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1092)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1079)
	at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeAcidExport(UpdateDeleteSemanticAnalyzer.java:196)
	... 23 more
{noformat}
",,
HIVE-19935,"I'm getting this error with WM feature quite frequently. It causes AM containers to shut down and a new one created to replace it.
{noformat}
018-06-18T19:06:49,969 INFO [Thread-250] monitoring.RenderStrategy$LogToFileFunction: Map 1: 313(+270)/641
2018-06-18T19:06:49,988 INFO [NotificationEventPoll 0] metastore.HiveMetaStore: 4: get_config_value: name=metastore.batch.retrieve.max defaultValue=50
2018-06-18T19:06:49,988 INFO [NotificationEventPoll 0] HiveMetaStore.audit: ugi=hive ip=unknown-ip-addr cmd=get_config_value: name=metastore.batch.retrieve.max defaultValue=50
2018-06-18T19:06:50,204 INFO [pool-29-thread-1] tez.TriggerValidatorRunnable: Query: hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4. Trigger { name: alluxio_medium, expression: ALLUXIO_BYTES_READ >
6442450944, action: MOVE TO medium } violated. Current value: 7184667126. Applying action.
2018-06-18T19:06:50,205 INFO [pool-29-thread-1] tez.WorkloadManager: Queued move session: 49be39e5-875c-4cfe-8601-7fe84dd57e0c moving from default to medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Handling move session event: 49be39e5-875c-4cfe-8601-7fe84dd57e0c moving from default to medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Subscribed to counters: [S3A_BYTES_READ, BYTES_READ, ALLUXIO_BYTES_READ]
2018-06-18T19:06:50,205 INFO [pool-29-thread-1] tez.KillMoveTriggerActionHandler: Moved session 49be39e5-875c-4cfe-8601-7fe84dd57e0c to pool medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.GuaranteedTasksAllocator: Updating 49be39e5-875c-4cfe-8601-7fe84dd57e0c with 144 guaranteed tasks
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WmEvent: Added WMEvent: EventType: MOVE EventStartTimestamp: 1529348810205 elapsedTime: 0 wmTezSessionInfo:SessionId: 49be39e5-875c-4cfe-8601-7fe
84dd57e0c Pool: medium Cluster %: 30.0
2018-06-18T19:06:50,234 INFO [StateChangeNotificationHandler] impl.ZkRegistryBase$InstanceStateChangeListener: CHILD_UPDATED for zknode /user-hive/llap/workers/worker-0000001571
2018-06-18T19:06:50,235 INFO [StateChangeNotificationHandler] tez.TezSessionPool: AM for 49be39e5-875c-4cfe-8601-7fe84dd57e0c, v.1571 has updated; updating [sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, qu
eueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277120ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killR
eason=null] with an endpoint at 32769
2018-06-18T19:06:50,235 INFO [StateChangeNotificationHandler] tez.TezSessionState: Ignoring an outdated info update 1571: TezAmInstance [49be39e5-875c-4cfe-8601-7fe84dd57e0c, host=ip-10-8-121-231.data.bazaar
voice.com, rpcPort=33365, pluginPort=32769, token=null]
2018-06-18T19:06:50,323 ERROR [TaskCommunicator # 4] tez.GuaranteedTasksAllocator: Failed to update guaranteed tasks count for the session sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=
hive, doAs=false, isOpen=true, isDefault=true, expires in 586277032ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=null
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.checkAndSendGuaranteedStateUpdate(LlapTaskSchedulerService.java:596)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateGuaranteedCount(LlapTaskSchedulerService.java:581)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateQuery(LlapTaskSchedulerService.java:3041)
at org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.updateQuery(LlapPluginServerImpl.java:57)
at org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos$LlapPluginProtocol$2.callBlockingMethod(LlapPluginProtocolProtos.java:835)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at com.sun.proxy.$Proxy111.updateQuery(Unknown Source) ~[?:?]
at org.apache.hadoop.hive.llap.impl.LlapPluginProtocolClientImpl.updateQuery(LlapPluginProtocolClientImpl.java:42) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl$SendUpdateQueryCallable.call(LlapPluginEndpointClientImpl.java:128) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl$SendUpdateQueryCallable.call(LlapPluginEndpointClientImpl.java:105) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) [guava-19.0.jar:?]
at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) [guava-19.0.jar:?]
at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) [guava-19.0.jar:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.hadoop.ipc.RemoteException: java.lang.NullPointerException
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.checkAndSendGuaranteedStateUpdate(LlapTaskSchedulerService.java:596)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateGuaranteedCount(LlapTaskSchedulerService.java:581)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateQuery(LlapTaskSchedulerService.java:3041)
at org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.updateQuery(LlapPluginServerImpl.java:57)
at org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos$LlapPluginProtocol$2.callBlockingMethod(LlapPluginProtocolProtos.java:835)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.Client.call(Client.java:1437) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.Client.call(Client.java:1347) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
... 11 more
2018-06-18T19:06:50,323 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,323 INFO [Workload management master] tez.WorkloadManager: Update failed for sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277032ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=null
2018-06-18T19:06:50,324 INFO [Workload management master] tez.WorkloadManager: Replacing sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277031ms, WM state poolName=null, clusterFraction=null, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=Failed to update resource allocation with a new session
2018-06-18T19:06:50,325 INFO [Workload management worker 0] tez.WorkloadManager: Created new interactive session object b89aaebf-aeaa-4b76-974a-7047592a186b
2018-06-18T19:06:50,325 INFO [Workload management worker 0] tez.TezSessionState: Closing Tez Session
2018-06-18T19:06:50,325 INFO [Workload management worker 0] client.TezClient: Shutting down Tez Session, sessionName=HIVE-49be39e5-875c-4cfe-8601-7fe84dd57e0c, applicationId=application_1528322657674_0427
2018-06-18T19:06:50,353 INFO [Workload management worker 0] tez.TezSessionState: Attemting to clean up resources for 49be39e5-875c-4cfe-8601-7fe84dd57e0c: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/49be39e5-875c-4cfe-8601-7fe84dd57e0c-resources; 0 additional files, 2 localized resources
2018-06-18T19:06:50,354 INFO [Workload management worker 0] tez.TezSessionState: User of session id b89aaebf-aeaa-4b76-974a-7047592a186b is hive
2018-06-18T19:06:50,356 INFO [Workload management worker 0] tez.DagUtils: Localizing resource because it does not exist: file:/usr/hdp/current/hive-server2-hive2/lib/hive-hcatalog-core.jar to dest: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/hive-hcatalog-core.jar
2018-06-18T19:06:50,467 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1529348810466 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/hive-hcatalog-core.jar
2018-06-18T19:06:50,467 INFO [Workload management worker 0] tez.DagUtils: Localizing resource because it does not exist: file:/usr/hdp/3.0.0.0-1064/hive2/auxlib/alluxio-core-client-runtime.jar to dest: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/alluxio-core-client-runtime.jar
2018-06-18T19:06:50,504 INFO [Thread-250] monitoring.RenderStrategy$LogToFileFunction: Map 1: 361(+0)/641
2018-06-18T19:06:50,785 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1529348810784 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/alluxio-core-client-runtime.jar
2018-06-18T19:06:50,785 INFO [Workload management worker 0] tez.TezSessionState: Created new resources: null
2018-06-18T19:06:50,786 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,788 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,791 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,792 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767036 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-llap-tez-3.1.0-SNAPSHOT-1530a700e8bc86603cf11cd8d0a2518f21a7a7c2ac09eb3af7782017ec05c4d3.jar
2018-06-18T19:06:50,792 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,793 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,794 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,796 INFO [Thread-250] SessionState: Status: Killed
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Dag received [DAG_TERMINATE, DAG_KILL] in RUNNING state.
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Received message to shutdown AM from hive (auth:SIMPLE) at 10.8.101.64
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Vertex killed, vertexName=Map 1, vertexId=vertex_1528322657674_0427_1_00, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:280, Vertex vertex_1528322657674_0427_1_00 [Map 1] killed/failed due to:DAG_TERMINATED]
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: DAG did not succeed due to DAG_KILL. failedVertices:0 killedVertices:1
2018-06-18T19:06:50,797 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,797 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,797 INFO [Workload management master] tez.WorkloadManager: Returning sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=false, isDefault=true, expires in 586276558ms, WM state poolName=null, clusterFraction=null, queryId=null, killReason=Failed to update resource allocation{noformat}",Sadness,-1
HIVE-20209,"Run the following command:
{code:java}
repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true');
{code}
See this in hs2.log:
{code:java}
2018-07-10T18:07:32,308 INFO [HiveServer2-Handler-Pool: Thread-14380]: conf.HiveConf (HiveConf.java:getLogIdVar(5061)) - Using the default value passed in for log id: f1e13736-3f10-4abf-a29b-683b534dfa4c
2018-07-10T18:07:32,309 INFO [HiveServer2-Handler-Pool: Thread-14380]: session.SessionState (:()) - Updating thread name to f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380
2018-07-10T18:07:32,311 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: operation.OperationManager (:()) - Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=16eb1d07-e125-490c-8ab8-90192bfd459b]
2018-07-10T18:07:32,314 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Compiling command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b): repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true')
2018-07-10T18:07:32,317 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,317 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,319 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,319 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,439 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Semantic Analysis Completed (retrial = false)
2018-07-10T18:07:32,440 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:dump_dir, type:string, comment:from deserializer), FieldSchema(name:last_repl_id, type:string, comment:from deserializer)], properties:null)
2018-07-10T18:07:32,443 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: exec.ListSinkOperator (:()) - Initializing operator LIST_SINK[0]
2018-07-10T18:07:32,446 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Completed compiling command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b); Time taken: 0.132 seconds
2018-07-10T18:07:32,447 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: conf.HiveConf (HiveConf.java:getLogIdVar(5061)) - Using the default value passed in for log id: f1e13736-3f10-4abf-a29b-683b534dfa4c
2018-07-10T18:07:32,448 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: session.SessionState (:()) - Resetting thread name to HiveServer2-Handler-Pool: Thread-14380
2018-07-10T18:07:32,451 INFO [HiveServer2-Background-Pool: Thread-15161]: reexec.ReExecDriver (:()) - Execution #1 of query
2018-07-10T18:07:32,452 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbTxnManager (:()) - Setting lock request transaction to txnid:30327 for queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b
2018-07-10T18:07:32,454 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbLockManager (:()) - Requesting: queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b LockRequest(component:[LockComponent(type:SHARED_READ, level:DB, dbname:default, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:hwxdemo, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:information_schema, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:sys, operationType:SELECT)], txnid:30327, user:hive, hostname:hwx-demo-2.field.hortonworks.com, agentInfo:hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b)
2018-07-10T18:07:32,497 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbLockManager (:()) - Response to queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b LockResponse(lockid:30305, state:ACQUIRED)
2018-07-10T18:07:32,501 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Executing command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b): repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true')
2018-07-10T18:07:32,503 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Starting task [Stage-0:REPL_DUMP] in serial mode
2018-07-10T18:07:32,506 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,509 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,509 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,510 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,510 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,536 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,538 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,539 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,542 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,542 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,565 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::START: \{""dbName"":""*"",""dumpType"":""INCREMENTAL"",""estimatedNumEvents"":0,""dumpStartTime"":1531246052}
2018-07-10T18:07:32,577 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,578 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60759 COMMIT_TXN message : \{""txnid"":30321,""timestamp"":1531246017,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,603 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60759"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""1/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,604 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60760 COMMIT_TXN message : \{""txnid"":30322,""timestamp"":1531246021,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,628 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60760"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""2/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,629 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60761 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246022,""fromTxnId"":30323,""toTxnId"":30323,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,653 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60761"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""3/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,654 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60762 COMMIT_TXN message : \{""txnid"":30323,""timestamp"":1531246028,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,679 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60762"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""4/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,680 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60763 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246033,""fromTxnId"":30324,""toTxnId"":30324,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,702 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60763"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""5/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,702 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60764 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246038,""fromTxnId"":30325,""toTxnId"":30325,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,724 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60764"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""6/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,725 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60765 COMMIT_TXN message : \{""txnid"":30324,""timestamp"":1531246038,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,747 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60765"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""7/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,748 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60766 COMMIT_TXN message : \{""txnid"":30325,""timestamp"":1531246043,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,768 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60766"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""8/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,769 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60767 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246043,""fromTxnId"":30326,""toTxnId"":30326,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,791 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60767"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""9/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,791 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60768 COMMIT_TXN message : \{""txnid"":30326,""timestamp"":1531246049,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,816 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60768"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""10/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,817 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60769 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246052,""fromTxnId"":30327,""toTxnId"":30327,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,841 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60769"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""11/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,841 WARN [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - MetaStoreClient lost connection. Attempting to reconnect (1 of 24) after 5s. getNextNotification
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
 at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:178) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:106) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:70) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_next_notification(ThriftHiveMetastore.java:5547) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_next_notification(ThriftHiveMetastore.java:5539) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getNextNotification(HiveMetaStoreClient.java:2697) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[?:?]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at com.sun.proxy.$Proxy60.getNextNotification(Unknown Source) ~[?:?]
 at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[?:?]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at com.sun.proxy.$Proxy60.getNextNotification(Unknown Source) ~[?:?]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getNextNotificationEvents(EventUtils.java:94) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$NotificationEventIterator.fetchNextBatch(EventUtils.java:146) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$NotificationEventIterator.hasNext(EventUtils.java:176) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.incrementalDump(ReplDumpTask.java:170) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.execute(ReplDumpTask.java:121) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2668) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2339) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2015) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1713) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1707) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_112]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_112]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) ~[hadoop-common-3.1.0.3.0.0.0-1605.jar:?]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
 at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-10T18:07:33,157 INFO [org.apache.ranger.audit.queue.AuditBatchQueue1]: destination.HDFSAuditDestination (:()) - Flushing HDFS audit. Event Size:4
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient trying reconnect as hive (auth:SIMPLE)
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:37,843 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:37,864 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::END: \{""dbName"":""*"",""dumpType"":""INCREMENTAL"",""actualNumEvents"":11,""dumpEndTime"":1531246057,""dumpDir"":""/user/hive/repl/c72dc152-c252-4454-9b0e-cf8c335ab07c"",""lastReplId"":""60769""}
2018-07-10T18:07:37,864 INFO [HiveServer2-Background-Pool: Thread-15161]: repl.ReplDumpTask (:()) - Done dumping events, preparing to return /user/hive/repl/c72dc152-c252-4454-9b0e-cf8c335ab07c,60769
2018-07-10T18:07:37,919 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Completed executing command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b); Time taken: 5.419 seconds
2018-07-10T18:07:37,919 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - OK
{code}
Metastore connection failed 1st attempt, but success after reconnect. That adds 5s for every repl dump command and likely to leak connection.

Similarly, Hive.close() also causes
{code:java}
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream{code}
","Fear, Surprise",-1
HIVE-20281,"HIVE-18201 seems to trigger a latent bug in SW optimizer. Test {{subquery_in_having}} fails with:
{code}
2018-07-31T08:42:57,328 DEBUG [b68f20cc-54d5-466d-b512-1540b3a43396 main] optimizer.SharedWorkOptimizer: After SharedWorkExtendedOptimizer:
TS[0]-SEL[1]-MAPJOIN[131]-FIL[12]-SEL[13]-GBY[14]-RS[15]-GBY[16]-SEL[17]-MAPJOIN[136]-MAPJOIN[137]-FIL[103]-SEL[104]-FS[105]
     -FIL[113]-SEL[20]-RS[44]-MAPJOIN[133]-SEL[47]-GBY[48]-RS[49]-GBY[50]-SEL[51]-GBY[55]-RS[98]-MAPJOIN[136]
                                                          -RS[88]-GBY[89]-SEL[120]-FIL[116]-SEL[91]-GBY[93]-RS[94]-GBY[95]-SEL[96]-RS[101]-MAPJOIN[137]
TS[2]-FIL[112]-GBY[5]-RS[6]-GBY[7]-SEL[8]-RS[10]-MAPJOIN[131]
                                         -RS[31]-MAPJOIN[132]-FIL[33]-SEL[34]-GBY[35]-RS[36]-GBY[37]-SEL[38]-GBY[42]-MAPJOIN[133]
TS[21]-FIL[114]-SEL[22]-MAPJOIN[132]
2018-07-31T08:42:57,329 ERROR [b68f20cc-54d5-466d-b512-1540b3a43396 main] ql.Driver: FAILED: SemanticException Error in shared work optimizer: operator cache contentsand actual plan differ
org.apache.hadoop.hive.ql.parse.SemanticException: Error in shared work optimizer: operator cache contentsand actual plan differ
        at org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.transform(SharedWorkOptimizer.java:524)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:185)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:146)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12361)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:356)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:284)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:165)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:284)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:663)
{code}

",Surprise,-1
HIVE-20502,"Enabling {{hive.stats.fetch.column.stats}} makes this test fail during:

{code}
EXPLAIN
SELECT a.*, b.* FROM T1_n151 a RIGHT OUTER JOIN T2_n88 b ON a.key = b.key
{code}

Seems like joinKeys is null at [this point|https://github.com/apache/hive/blob/48f92c31dee3983f573f2e66baaa213a0196f1ba/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java#L2169]

Exception:

{code}
2018-09-04T23:47:02,398 DEBUG [fef236ce-e62e-4c20-b0c0-3b15d2b336f7 main] annotation.StatsRulesProcFactory: STATS-JOIN[15]: detects none/multiple PK parents.
2018-09-04T23:47:02,409 ERROR [fef236ce-e62e-4c20-b0c0-3b15d2b336f7 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.isJoinKey(StatsRulesProcFactory.java:2169)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.updateNumNulls(StatsRulesProcFactory.java:2210)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.updateColStats(StatsRulesProcFactory.java:2276)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.process(StatsRulesProcFactory.java:1785)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
{code}

",,
HIVE-20610,"Using /tmpdirectory creates exceptions for tests like dropTable :
{code:java}
2018-09-19T06:42:04,818  INFO [main] metastore.HiveMetaStore: 0: drop_table : tbl=hive.default.droptbl
2018-09-19T06:42:04,819  INFO [main] HiveMetaStore.audit: ugi=hiveptest	ip=unknown-ip-addr	cmd=drop_table : tbl=hive.default.droptbl	
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/.ICE-unix]: it still exists.
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/.XIM-unix]: it still exists.
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/.X11-unix]: it still exists.
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/hsperfdata_root]: it still exists.
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/.font-unix]: it still exists.
2018-09-19T06:42:05,072  WARN [main] fs.FileUtil: Failed to delete file or dir [/tmp/.Test-unix]: it still exists.
2018-09-19T06:42:05,072 ERROR [main] utils.FileUtils: Failed to delete file:/tmp
2018-09-19T06:42:05,072 ERROR [main] utils.MetaStoreUtils: Got exception: org.apache.hadoop.hive.metastore.api.MetaException Unable to delete directory: file:/tmp
org.apache.hadoop.hive.metastore.api.MetaException: Unable to delete directory: file:/tmp
	at org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.deleteDir(HiveMetaStoreFsImpl.java:45) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.Warehouse.deleteDir(Warehouse.java:365) [hive-standalone-metastore-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.Warehouse.deleteDir(Warehouse.java:353) [hive-standalone-metastore-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.deleteTableData(HiveMetaStore.java:2562) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:2523) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:2685) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_102]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_102]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at com.sun.proxy.$Proxy33.drop_table_with_environment_context(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:3204) [hive-standalone-metastore-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:1492) [hive-standalone-metastore-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:1432) [hive-standalone-metastore-common-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
	at org.apache.hive.hcatalog.listener.TestDbNotificationListener.dropTable(TestDbNotificationListener.java:522) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_102]{code}


",,
HIVE-20627,"When multiple async queries are executed from same session, it leads to multiple async query execution DAGs share the same Hive object which is set by caller for all threads. In case of loading dynamic partitions, it creates MoveTask whichre-creates the Hive objectandcloses the shared Hive object which causes metastore connection issues forother async execution thread who still access it. This is also seen ifReplDumpTask and ReplLoadTask are part of the DAG.

*Call Stack:*
{code:java}
2018-09-16T04:38:04,280 ERROR [load-dynamic-partitions-7]: metadata.Hive (Hive.java:call(2436)) - Exception when loading partition with parameters partPath=hdfs://mycluster/warehouse/tablespace/managed/hive/tbl_3bcvvdubni/.hive-staging_hive_2018-09-16_04-35-50_708_7776079613819042057-1147/-ext-10000/age=55, table=tbl_3bcvvdubni, partSpec={age=55}, loadFileType=KEEP_EXISTING, listBucketingLevel=0, isAcid=true, hasFollowingStatsTask=true
org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore
at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidWriteIds(DbTxnManager.java:714) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableValidWriteIdListWithTxnList(AcidUtils.java:1791) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableSnapshot(AcidUtils.java:1756) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.io.AcidUtils.getTableSnapshot(AcidUtils.java:1714) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1976) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive$5.call(Hive.java:2415) [hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.ql.metadata.Hive$5.call(Hive.java:2406) [hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.thrift.protocol.TProtocolException: Required field 'validTxnList' is unset! Struct:GetValidWriteIdsRequest(fullTableNames:[default.tbl_3bcvvdubni], validTxnList:null)
at org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.validate(GetValidWriteIdsRequest.java:396) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args.validate(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args$get_valid_write_ids_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args$get_valid_write_ids_argsStandardScheme.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_valid_write_ids_args.write(ThriftHiveMetastore.java) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:71) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_valid_write_ids(ThriftHiveMetastore.java:5443) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_valid_write_ids(ThriftHiveMetastore.java:5435) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidWriteIds(HiveMetaStoreClient.java:2589) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at com.sun.proxy.$Proxy57.getValidWriteIds(Unknown Source) ~[?:?]
at sun.reflect.GeneratedMethodAccessor125.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2934) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
at com.sun.proxy.$Proxy57.getValidWriteIds(Unknown Source) ~[?:?]
at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidWriteIds(DbTxnManager.java:712) ~[hive-exec-3.1.0.3.0.1.0-184.jar:3.1.0.3.0.1.0-184]
... 10 more{code}
*Root cause:*
 For Async query execution from SQLOperation.runInternal, we set the Thread local Hive object for all the child threads as parentHive (parentSession.getSessionHive())
{code:java}
@Override
 public void run() {
 PrivilegedExceptionAction<Object> doAsAction = new PrivilegedExceptionAction<Object>() {
 @Override
 public Object run() throws HiveSQLException {
 Hive.set(parentHive); // Setting parentHive for all async operations.
 // TODO: can this result in cross-thread reuse of session state?
 SessionState.setCurrentSessionState(parentSessionState);
 PerfLogger.setPerfLogger(parentPerfLogger);
 LogUtils.registerLoggingContext(queryState.getConf());
 try {
 if (asyncPrepare) {
 prepare(queryState);
 }
 runQuery();
 } catch (HiveSQLException e) {
 // TODO: why do we invent our own error path op top of the one from Future.get?
 setOperationException(e);
 LOG.error(""Error running hive query: "", e);
 } finally {
 LogUtils.unregisterLoggingContext();
 }
 return null;
 }
 };
{code}
Now, when async execution in progress and if one of the thread re-creates the Hive object, it closes the parentHive object first which impacts other threads using it and hence conf object it refers too gets cleaned up and hence we get null for VALID_TXNS_KEY value.
{code:java}
private static Hive create(HiveConf c, boolean needsRefresh, Hive db, boolean doRegisterAllFns)
 throws HiveException {
 if (db != null) {
 LOG.debug(""Creating new db. db = "" + db + "", needsRefresh = "" + needsRefresh +
 "", db.isCurrentUserOwner = "" + db.isCurrentUserOwner());
 db.close();
 }
 closeCurrent();
 if (c == null) {
 c = createHiveConf();
 }
 c.set(""fs.scheme.class"", ""dfs"");
 Hive newdb = new Hive(c, doRegisterAllFns);
 hiveDB.set(newdb);
 return newdb;
 }
{code}
*Fix:*
 We shouldn't clean the old Hive object if it is shared by multiple threads. Shall use a flag to know this.

*Memory leak issue:*
 Memory leak is found if one of the threads from Hive.loadDynamicPartitions throw exception. rawStoreMap is used to store rawStore objects which has to be cleaned. In this case, it is populated only in success flow but if there are exceptions, it is not and hence there is a leak.
{code:java}
futures.add(pool.submit(new Callable<Void>() {
 @Override
 public Void call() throws Exception {
 try {
 // move file would require session details (needCopy() invokes SessionState.get)
 SessionState.setCurrentSessionState(parentSession);
 LOG.info(""New loading path = "" + partPath + "" with partSpec "" + fullPartSpec);

// load the partition
 Partition newPartition = loadPartition(partPath, tbl, fullPartSpec, loadFileType,
 true, false, numLB > 0, false, isAcid, hasFollowingStatsTask, writeId, stmtId,
 isInsertOverwrite);
 partitionsMap.put(fullPartSpec, newPartition);

if (inPlaceEligible) {
 synchronized (ps) {
 InPlaceUpdate.rePositionCursor(ps);
 partitionsLoaded.incrementAndGet();
 InPlaceUpdate.reprintLine(ps, ""Loaded : "" + partitionsLoaded.get() + ""/""
 + partsToLoad + "" partitions."");
 }
 }
 // Add embedded rawstore, so we can cleanup later to avoid memory leak
 if (getMSC().isLocalMetaStore()) {
 if (!rawStoreMap.containsKey(Thread.currentThread().getId())) {
 rawStoreMap.put(Thread.currentThread().getId(), HiveMetaStore.HMSHandler.getRawStore());
 }
 }
 return null;
 } catch (Exception t) {
 }
{code}",Fear,-1
HIVE-20652,"Test case attached. The following query fail:
{code}
SELECT * FROM ext_auth1 JOIN ext_auth2 ON ext_auth1.ikey = ext_auth2.ikey
{code}
Error message:
{code}
2018-09-28T00:36:23,860 DEBUG [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Query to execute is [SELECT *
FROM (SELECT *
FROM ""SIMPLE_DERBY_TABLE1""
WHERE ""ikey"" IS NOT NULL) AS ""t""
INNER JOIN (SELECT *
FROM ""SIMPLE_DERBY_TABLE2""
WHERE ""ikey"" IS NOT NULL) AS ""t0"" ON ""t"".""ikey"" = ""t0"".""ikey"" {LIMIT 1}]
2018-09-28T00:36:23,864 ERROR [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Error while trying to get column names.
java.sql.SQLSyntaxErrorException: Table/View 'SIMPLE_DERBY_TABLE2' does not exist.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement42.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.jdbc.Driver42.newEmbedPreparedStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.commons.dbcp.DelegatingConnection.prepareStatement(DelegatingConnection.java:281) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.prepareStatement(PoolingDataSource.java:313) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getColumnNames(GenericJdbcDatabaseAccessor.java:74) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.storage.jdbc.JdbcSerDe.initialize(JdbcSerDe.java:78) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:540) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:90) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:77) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:295) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:277) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genTablePlan(SemanticAnalyzer.java:11100) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11468) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11427) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:525) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12319) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:356) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:669) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1872) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1819) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1814) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) [hive-cli-4.0.0-SNAPSHOT.jar:?]
{code}
Hive is pushing the join into jdbc driver though the table refer to different data source.

",Surprise,-1
HIVE-20817,"CREATE TABLE JdbcBasicRead ( empno int, desg string,empname string,doj timestamp,Salary float,mgrid smallint, deptno tinyint ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH '/tmp/art_jdbc/hive/input/input_7columns.txt' OVERWRITE INTO TABLE JdbcBasicRead;

Sample Data.

7369,M,SMITH,1980-12-17 17:07:29.234234,5000.00,7902,20
7499,X,ALLEN,1981-02-20 17:07:29.234234,1250.00,7698,30
7521,X,WARD,1981-02-22 17:07:29.234234,01600.57,7698,40
7566,M,JONES,1981-04-02 17:07:29.234234,02975.65,7839,10
7654,X,MARTIN,1981-09-28 17:07:29.234234,01250.00,7698,20
7698,M,BLAKE,1981-05-01 17:07:29.234234,2850.98,7839,30
7782,M,CLARK,1981-06-09 17:07:29.234234,02450.00,7839,20


Select statement: SELECT empno, desg, empname, doj, salary, mgrid, deptno FROM JdbcBasicWrite

{code}
2018-09-25T07:11:03,222 WARN [HiveServer2-Handler-Pool: Thread-83]: thrift.ThriftCLIService (:()) - Error fetching results:
org.apache.hive.service.cli.HiveSQLException: java.lang.ClassCastException: org.apache.hadoop.hive.common.type.Timestamp cannot be cast to java.sql.Timestamp
at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:469) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:328) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:910) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source) ~[?:?]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_112]
at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_112]
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at com.sun.proxy.$Proxy46.fetchResults(Unknown Source) ~[?:?]
at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:564) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:786) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.common.type.Timestamp cannot be cast to java.sql.Timestamp
at org.apache.hive.service.cli.ColumnValue.toTColumnValue(ColumnValue.java:203) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.RowBasedSet.addRow(RowBasedSet.java:60) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.RowBasedSet.addRow(RowBasedSet.java:32) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.operation.SQLOperation.prepareFromRow(SQLOperation.java:517) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.operation.SQLOperation.decode(SQLOperation.java:509) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:463) ~[hive-service-3.1.0.3.0.1.0-187.jar:3.1.0.3.0.1.0-187]
... 24 more
2018-09-25T07:11:03,232 INFO [HiveServer2-Handler-Pool: Thread-83]: thrift.ThriftCLIService (:()) - Session disconnected without closing properly.
{code}",,
HIVE-20829,"{code}
2018-10-29T06:37:14,982 ERROR [HiveServer2-Background-Pool: Thread-44466]: operation.Operation (:()) - Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:228) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:318) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.3.0-150.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:338) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:240) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:210) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2707) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2378) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2054) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1752) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1746) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	... 13 more
{code}",,
HIVE-20830,"{code}
2018-10-29T10:10:16,325 ERROR [b4bf5eb2-a986-4aae-908e-93b9908acd32 HiveServer2-HttpHandler-Pool: Thread-124]: dao.GenericJdbcDatabaseAccessor (:()) - Caught exception while trying to execute query
java.lang.IllegalArgumentException: null
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:108) ~[guava-19.0.jar:?]
	at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.addBoundaryToQuery(GenericJdbcDatabaseAccessor.java:238) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getRecordIterator(GenericJdbcDatabaseAccessor.java:161) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.JdbcRecordReader.next(JdbcRecordReader.java:58) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.JdbcRecordReader.next(JdbcRecordReader.java:35) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:569) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:509) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2734) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:469) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:328) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:910) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:564) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:790) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.server.TServlet.doPost(TServlet.java:83) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:208) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707) ~[javax.servlet-api-3.1.0.jar:3.1.0]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar:3.1.0]
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.Server.handle(Server.java:534) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:251) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]
2018-10-29T10:10:16,325 ERROR [b4bf5eb2-a986-4aae-908e-93b9908acd32 HiveServer2-HttpHandler-Pool: Thread-124]: jdbc.JdbcRecordReader (:()) - An error occurred while reading the next record from DB.
org.apache.hive.storage.jdbc.exception.HiveJdbcDatabaseAccessException: Caught exception while trying to execute query
	at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getRecordIterator(GenericJdbcDatabaseAccessor.java:177) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.JdbcRecordReader.next(JdbcRecordReader.java:58) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.JdbcRecordReader.next(JdbcRecordReader.java:35) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:569) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:509) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2734) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:469) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:328) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:910) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:564) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:790) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.thrift.server.TServlet.doPost(TServlet.java:83) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:208) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707) ~[javax.servlet-api-3.1.0.jar:3.1.0]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar:3.1.0]
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.Server.handle(Server.java:534) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:251) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) ~[jetty-io-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) ~[jetty-runner-9.3.20.v20170531.jar:9.3.20.v20170531]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]
Caused by: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:108) ~[guava-19.0.jar:?]
	at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.addBoundaryToQuery(GenericJdbcDatabaseAccessor.java:238) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getRecordIterator(GenericJdbcDatabaseAccessor.java:161) ~[hive-jdbc-handler-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-99]
	... 45 more
{code}",,
HIVE-20839,"Occurs in some cases in the non-CBO optimized queries, either if CBO is disabled or has failed due to error.

{noformat}
2018-10-11T04:40:22,724 ERROR [TezTR-85144_8944_1085_28_996_2 (1539092085144_8944_1085_28_000996_2)] tez.ReduceRecordProcessor: Hit error while closing operators - failing tree
2018-10-11T04:40:22,724 ERROR [TezTR-85144_8944_1085_28_996_2 (1539092085144_8944_1085_28_000996_2)] tez.TezProcessor: java.lang.RuntimeException: cannot find field _col304 from [0:_col0, 1:_col1, 2:_col2, 3:_col3, 4:_col4, 5:_col5, 6:_col6, 7:_col7, 8:_col8, 9:_col9, 10:_col10, 11:_col11, 12:_col12, 13:_col13, 14:_col15, 15:_col16, 16:_col17, 17:_col18, 18:_col19, 19:_col20, 20:_col21, 21:_col22, 22:_col23, 23:_col24, 24:_col25, 25:_col26, 26:_col27, 27:_col28, 28:_col29, 29:_col30, 30:_col31, 31:_col32, 32:_col33, 33:_col34, 34:_col35, 35:_col36, 36:_col37, 37:_col38, 38:_col39, 39:_col40, 40:_col41, 41:_col42, 42:_col43, 43:_col44, 44:_col45, 45:_col46, 46:_col47, 47:_col48, 48:_col49, 49:_col50, 50:_col51, 51:_col52, 52:_col53, 53:_col54, 54:_col55, 55:_col56, 56:_col57, 57:_col58, 58:_col59, 59:_col60, 60:_col61, 61:_col62, 62:_col63, 63:_col64, 64:_col65, 65:_col66, 66:_col67, 67:_col68, 68:_col70, 69:_col72, 70:_col73, 71:_col74, 72:_col75, 73:_col76, 74:_col77, 75:_col78, 76:_col79, 77:_col80, 78:_col81, 79:_col82, 80:_col83, 81:_col84, 82:_col85, 83:_col86, 84:_col87, 85:_col88, 86:_col89, 87:_col90, 88:_col91, 89:_col92, 90:_col93, 91:_col94, 92:_col95, 93:_col96, 94:_col97, 95:_col98, 96:_col99, 97:_col100, 98:_col101, 99:_col102, 100:_col103, 101:_col104, 102:_col105, 103:_col106, 104:_col107, 105:_col108, 106:_col109, 107:_col110, 108:_col111, 109:_col112, 110:_col113, 111:_col114, 112:_col115, 113:_col116, 114:_col117, 115:_col118, 116:_col119, 117:_col120, 118:_col121, 119:_col122, 120:_col123, 121:_col124, 122:_col125, 123:_col126, 124:_col127, 125:_col128, 126:_col129, 127:_col130, 128:_col131, 129:_col132, 130:_col133, 131:_col134, 132:_col135, 133:_col136, 134:_col137, 135:_col138, 136:_col139, 137:_col140, 138:_col141, 139:_col142, 140:_col143, 141:_col144, 142:_col145, 143:_col146, 144:_col147, 145:_col148, 146:_col149, 147:_col150, 148:_col151, 149:_col152, 150:_col153, 151:_col154, 152:_col155, 153:_col156, 154:_col157, 155:_col158, 156:_col159, 157:_col160, 158:_col161, 159:_col162, 160:_col163, 161:_col164, 162:_col165, 163:_col166, 164:_col167, 165:_col168, 166:_col169, 167:_col170, 168:_col171, 169:_col318]
at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:485)
at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:153)
at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:80)
at org.apache.hadoop.hive.ql.exec.JoinUtil.getObjectInspectorsFromEvaluators(JoinUtil.java:91)
at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:74)
at org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp(MapJoinOperator.java:144)
at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:374)
at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:195)
at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:188)
at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:172)
at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
{noformat}",,
HIVE-4403,"While working on BIGTOP-885, I saw that Hive was giving a bunch of warnings related to overriding final parameters in job.conf. This was on a pseudo distributed cluster. FWIW, I didn't see this happen on a fully-distributed cluster. Perhaps, Hive's job.conf is overriding some final parameters it shouldn't.

Here is what the warnings looked like:
{code}
2013-04-19 14:20:32,304 WARN  [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval;  Ignoring.
2013-04-19 14:20:32,367 WARN  [main] conf.Configuration (Configuration.java:loadProperty(2032)) - file:/tmp/root/hive_2013-04-19_14-20-30_159_5701876916688815815/-local-10002/jobconf.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts;  Ignoring.
{code}

To reproduce, run a query like:
{code}
CREATE TABLE u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
{code}
Load some data into u_data, here is some sample data:
https://github.com/apache/bigtop/blob/master/bigtop-tests/test-artifacts/hive/src/main/resources/seed_data_files/ml-data/u.data

Run a simple query on that data (on YARN/MR2)
{code}
INSERT OVERWRITE DIRECTORY '/tmp/count'
SELECT COUNT(1) FROM u_data
{code}","Fear, Surprise",-1
HIVE-7114,"When starting the HiveServer2 we are seeing an extra Tez AM launched.

This is where it is getting created .
{noformat}
2014-05-09 23:11:22,261 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:addAdminUsers(588)) - No user is added in admin role, since config is empty
java.lang.Exception: Opening session
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)
        at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)
        at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)
        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)
        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)
        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)
        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}",Surprise,-1
MAPREDUCE-2463,"If ""mapreduce.jobtracker.jobhistory.location"" is configured as HDFS location then either during initialization of Job Tracker (while moving old job history files) or after completion of the job, history files are not moving to done and giving following exception.

{code:xml} 
2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.
java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)
	at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

{code} 
",,
MAPREDUCE-2942,"This is failing right after the MAPREDUCE-2655 commit, but Jenkins did report a success when that patch was submitted.

{code}
Standard Output

2011-09-07 07:12:52,785 INFO  ipc.Server (Server.java:run(349)) - Starting Socket Reader #1 for port 33000
2011-09-07 07:12:52,787 INFO  ipc.Server (WritableRpcEngine.java:registerProtocolAndImpl(399)) - ProtocolImpl=org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger$MyTestRPCServer protocolClass=org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger$MyTestRPCServer version=1
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(642)) - IPC Server Responder: starting
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(473)) - IPC Server listener on 33000: starting
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(1459)) - IPC Server handler 0 on 33000: starting
2011-09-07 07:12:52,798 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 33000, call: ping(), rpc version=2, client version=1, methodsFingerPrint=-1968962669 from 67.195.138.31:33806, error: 
java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.ipc.TestRPC$TestProtocol
	at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)
{code}",Surprise,-1
MAPREDUCE-3005,"The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on [~karams]'s sort runs on a big cluster.
{code}
2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)
        at java.lang.Thread.run(Thread.java:619)
{code}",,
MAPREDUCE-3030,"{code:title=Node Manager Logs|borderStyle=solid}
2011-09-19 13:39:29,816 INFO  webapp.WebApps (WebApps.java:start(162)) - Registered webapp guice modules
2011-09-19 13:39:29,817 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is started.
2011-09-19 13:39:29,818 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:Dispatcher is started.
2011-09-19 13:39:29,819 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:start(133)) - Configured ContainerManager Address is 10.18.52.124:45454
2011-09-19 13:39:29,819 INFO  ipc.YarnRPC (YarnRPC.java:create(47)) - Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2011-09-19 13:39:29,822 INFO  ipc.HadoopYarnRPC (HadoopYarnProtoRPC.java:getProxy(49)) - Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.server.api.ResourceTracker
2011-09-19 13:39:29,862 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(165)) - Connected to ResourceManager at 0.0.0.0:8025
2011-09-19 13:39:30,369 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(189)) - Registered with ResourceManager as 10.18.52.124:45454 with total resource of memory: 8192, 
2011-09-19 13:39:30,369 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is started.
2011-09-19 13:39:30,371 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is started.
{code}



{code:title=Resource Manager Logs|borderStyle=solid}
2011-09-19 14:01:03,238 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:04,240 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:05,242 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:06,244 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:07,246 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:08,247 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
{code}

Node Manager is registered with Resource manager and the for every heartbeat, it is printing the above message.",,
MAPREDUCE-3058,"While running GridMixV3, one of the jobs got stuck for 15 hrs. After clicking on the Job-page, found one of its reduces to be stuck. Looking at syslog of the stuck reducer, found this:
Task-logs' head:

{code}
2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
{code}

Task-logs' tail:
{code}
2011-09-19 18:06:49,818 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink as <DATANODE1>
2011-09-19 18:06:49,818 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1405370709-<NAMENODE>-1316452621953:blk_-7004355226367468317_79871 in pipeline  <DATANODE2>,  <DATANODE1>: bad datanode  <DATANODE1>
2011-09-19 18:06:49,818 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol: lastAckedSeqno = 26870
2011-09-19 18:06:49,820 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #454
2011-09-19 18:06:49,826 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <<NAMENODE> from gridperf got value #454
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.ipc.RPC: Call: getAdditionalDatanode 8
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Connecting to datanode <DATANODE2>
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Send buf size 131071
2011-09-19 18:06:49,833 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)
2011-09-19 18:06:49,837 WARN org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)

2011-09-19 18:06:49,837 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #455
2011-09-19 18:06:49,839 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #455
2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.RPC: Call: statusUpdate 3
2011-09-19 18:06:49,840 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task
2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #456
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf got value #456
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.RPC: Call: delete 18
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #457
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #457
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.RPC: Call: reportDiagnosticInfo 1
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: refCount=1
2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source UgiMetrics
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder$1
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=UgiMetrics
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.source.JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Stats
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Control
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
{code}

Which means that tasks is supposed to have stopped within 20 secs, whereas the process itself is stuck for more than 15 hours. From AM log, also found that this task was sending its update regularly. ps -ef | grep java was also showing that process is still alive.
",Surprise,-1
MAPREDUCE-3070,"After stopping NM gracefully then starting NM, NM registration fails with RM with Duplicate registration from the node! error.


{noformat} 
2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager
org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)
Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	... 2 more
Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)
	at $Proxy13.registerNodeManager(Unknown Source)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)
	... 3 more
{noformat} 
",Surprise,-1
MAPREDUCE-3241,"When we run the TraceBuilder, we get this exception. Output of the TraceBuilder doesn't contain the map and reduce task information.

{code}
2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist
java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type
        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)
        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)
        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)

{code}",,
MAPREDUCE-3306,"Seeing this in NM logs when trying to run jobs.
{code}
2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED
2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..
java.util.NoSuchElementException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)
        at java.util.HashMap$ValueIterator.next(HashMap.java:822)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:662)
{code}",,
MAPREDUCE-3333,"[~Karams] just found this. The usual sort job on a 350 node cluster hung due to OutOfMemory and eventually failed after an hour instead of the usual odd 20 minutes.
{code}
2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002
_01_001434 : java.lang.reflect.UndeclaredThrowableException
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)
        at $Proxy20.startContainer(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)
        ... 4 more
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)
        at org.apache.hadoop.ipc.Client.call(Client.java:1089)
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)
        ... 6 more
Caused by: java.io.IOException: Couldn't set up IO streams
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)
        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)
        at org.apache.hadoop.ipc.Client.call(Client.java:1065)
        ... 7 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:597)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)
        ... 10 more
{code}",Surprise,-1
MAPREDUCE-3463,"Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml. Started yarn 4 Node cluster.
First Ran Randowriter/Sort/Sort-validate successfully
Then again sort, when job was 50% complete
Login node running AppMaster, and killed AppMaster with kill -9
On Client side failed with following:
{code}
11/11/23 10:57:27 INFO mapreduce.Job:  map 58% reduce 8%
11/11/23 10:57:27 INFO mapred.ClientServiceDelegate: Failed to contact AM/History for job job_1322040898409_0005 retrying..
11/11/23 10:57:28 INFO mapreduce.Job:  map 0% reduce 0%
11/11/23 10:57:37 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=UNDEFINED. Redirecting to job history server
11/11/23 10:57:37 INFO client.ClientTokenSelector: Looking for a token with service <RM Host>:Port
11/11/23 10:57:37 INFO client.ClientTokenSelector: Token kind is YARN_CLIENT_TOKEN and the token's service name is <New AM Host>:Port
11/11/23 10:57:38 WARN mapred.ClientServiceDelegate: Error from remote end: Unknown job job_1322040898409_0005
RemoteTrace: 
 at Local Trace: 
	org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)
	at $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)
	at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)
	at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)
	at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)
	at org.apache.hadoop.examples.Sort.run(Sort.java:181)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at org.apache.hadoop.examples.Sort.main(Sort.java:192)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
{code}

On lookig RM logs found second AM was also lauched, it was saying -:
{code}
011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1322040898409_0005_000002 State change from RUNNING to FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Processing event for application_1322040898409_0005 of type ATTEMPT_FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1322040898409_0005 State change from RUNNING to FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application appattempt_1322040898409_0005_000002 is done. finalState=FINISHED
{code}

Now looking at AM logs and found Second AM was shutdown gracefully due to :-
{code}
2011-11-23 10:57:37,640 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService: Sending assigned event to attempt_1322040898409_0005_m_000000_0
2011-11-23 10:57:37,641 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..
java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port
        at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
2011-11-23 10:57:37,642 INFO [CompositeServiceShutdownHook for org.apache.hadoop.mapreduce.v2.app.MRAppMaster] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler
{code}",Joy,1
MAPREDUCE-3531,"Filling this Jira a bit late
Started 350 cluster
sbummited large sleep job.
Foud that job was not running as RM has not allocated resouces to it.
{code}
2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <NMHost>:48490 clusterResources: memory: 3225600
2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event
type NODE_UPDATE to the scheduler
java.lang.IllegalArgumentException: Invalid key to HMAC computation
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)
        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)
        atorg.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.security.InvalidKeyException: Secret key expected
        at com.sun.crypto.provider.HmacCore.a(DashoA13*..)
        at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)
        at javax.crypto.Mac.init(DashoA13*..)
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)
        ... 14 more
{code}
As this stack is from 30 Nov checkou line number may be different",,
MAPREDUCE-3532,"I tried following -:
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.webapp.address=0.0.0.0:0
yarn.nodemanager.localizer.address=0.0.0.0:0
mapreduce.shuffle.port=0

When 0 is provided as number in yarn.nodemanager.webapp.address. 
NM instantiate WebServer as 0 piort e.g.
{code}
2011-12-08 11:33:02,467 INFO org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:0
{code}

After that WebServer pick up some random port e.g.
{code}
2011-12-08 11:33:02,562 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 36272
2011-12-08 11:33:02,562 INFO org.mortbay.log: jetty-6.1.26
2011-12-08 11:33:02,831 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:36272
2011-12-08 11:33:02,831 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /node started at 36272
{code}

And NM WebServer responds correctly but
 RM's cluster/Nodes page shows the following -:
{code}
/Rack RUNNING NM:57963 NM:0 Healthy 8-Dec-2011 11:33:01 Healthy 8 12 GB 0 KB
{code}
Whereas NM:0 is not clickable.
Seems even NM's webserver pick random port but it never gets updated and so NM report 0 as HTTP port to RM causing NM Hyperlinks un-clickable
But verified that MR job runs successfully with random.
",Surprise,-1
MAPREDUCE-3649,"When calling job end notification for oozie the AM fails with the following trace:

{noformat}
2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed
java.net.UnknownServiceException: no content-type
	at java.net.URLConnection.getContentHandler(URLConnection.java:1192)
	at java.net.URLConnection.getContent(URLConnection.java:689)
	at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)
	at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
{noformat}",,
MAPREDUCE-3916,"Seem like yarn proxyserver is not operational when running out of the 0.23.1 RC2 tarball.

# Setting yarn.web-proxy.address to match yarn.resourcemanager.address doesn't disable the proxyserver (althought not setting yarn.web-proxy.address at all correctly disable it and produces a message: org.apache.hadoop.yarn.YarnException: yarn.web-proxy.address is not set so the proxy will not run). This contradicts the documentation provided for yarn.web-proxy.address in yarn-default.xml

# Setting yarn.web-proxy.address and running the service results in the following:

{noformat}
$ ./sbin/yarn-daemon.sh start proxyserver 
starting proxyserver, logging to /tmp/hadoop-0.23.1/logs/yarn-rvs-proxyserver-ahmed-laptop.out
/usr/java/64/jdk1.6.0_22/bin/java -Dproc_proxyserver -Xmx1000m -Dhadoop.log.dir=/tmp/hadoop-0.23.1/logs -Dyarn.log.dir=/tmp/hadoop-0.23.1/logs -Dhadoop.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.home.dir= -Dyarn.id.str=rvs -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -Djava.library.path=/tmp/hadoop-0.23.1/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/tmp/hadoop-0.23.1/logs -Dyarn.log.dir=/tmp/hadoop-0.23.1/logs -Dhadoop.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.log.file=yarn-rvs-proxyserver-ahmed-laptop.log -Dyarn.home.dir=/tmp/hadoop-0.23.1 -Dhadoop.root.logger=INFO,DRFA -Dyarn.root.logger=INFO,DRFA -Djava.library.path=/tmp/hadoop-0.23.1/lib/native -classpath /tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/etc/hadoop:/tmp/hadoop-0.23.1/share/hadoop/common/lib/*:/tmp/hadoop-0.23.1/share/hadoop/common/*:/tmp/hadoop-0.23.1/share/hadoop/hdfs:/tmp/hadoop-0.23.1/share/hadoop/hdfs/lib/*:/tmp/hadoop-0.23.1/share/hadoop/hdfs/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/lib/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/*:/tmp/hadoop-0.23.1/share/hadoop/mapreduce/lib/* org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer
{noformat}

with the following message found in the logs:

{noformat}
2012-02-24 09:26:31,099 FATAL org.apache.hadoop.yarn.server.webproxy.WebAppProxy: Could not start proxy web server
java.io.FileNotFoundException: webapps/proxy not found in CLASSPATH
        at org.apache.hadoop.http.HttpServer.getWebAppsPath(HttpServer.java:532)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:224)
        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:164)
        at org.apache.hadoop.yarn.server.webproxy.WebAppProxy.start(WebAppProxy.java:85)
        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer.main(WebAppProxyServer.java:76)
{noformat}",Sadness,-1
MAPREDUCE-3931,"[~karams] reported this offline. Seems that tasks are randomly failing during gridmix runs:
{code}
2012-02-24 21:03:34,912 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1330116323296_0140_m_003868_0: RemoteTrace:
java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875
       at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)
       at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)
       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)
       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:396)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)
       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
 at LocalTrace:
       org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875
       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)
       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)
       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)
       at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)
       at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)
       at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:396)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)
{code}",,
MAPREDUCE-3932,"[~karams] reported this offline. One reduce task gets preempted because of zero headRoom and crashes the AM.
{code}
2012-02-23 11:30:15,956 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544
2012-02-23 11:30:16,959 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 44544
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:23 AssignedMaps:0 AssignedReduces:0 completedMaps:4 completedReduces:0 containersAllocated:4 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 3
2012-02-23 11:30:16,965 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000006 to attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000007 to attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned to reduce
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1329995034628_0983_01_000008 to attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Assign: PendingReduces:377 ScheduledMaps:6 ScheduledReduces:20 AssignedMaps:0 AssignedReduces:3 completedMaps:4 completedReduces:0 containersAllocated:7 containersReleased:0 hostLocalAssigned:0 rackLocalAssigned:4 availableResources(headroom):memory: 0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down all scheduled reduces:20
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Going to preempt 2
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Preempting attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule...
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: completedMapPercent 0.4 totalMemLimit:4608 finalMapMemLimit:2765 finalReduceMemLimit:1843 netScheduledMapMem:9216 netScheduledReduceMem:4608
2012-02-23 11:30:16,966 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Ramping down 0
2012-02-23 11:30:16,968 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host6 to /$rack6
2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,976 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host1 to /$rack1
2012-02-23 11:30:16,977 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,981 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved $host9 to /$rack9
2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED
2012-02-23 11:30:16,982 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000002_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP
2012-02-23 11:30:16,983 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from ASSIGNED to KILL_CONTAINER_CLEANUP
2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,983 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,983 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000000_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:16,984 INFO [ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for taskAttempt attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,987 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for taskAttempt attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,988 INFO [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1329995034628_0983_r_000001_0
2012-02-23 11:30:16,988 ERROR [ContainerLauncher #9] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container was killed before it was launched
2012-02-23 11:30:17,061 INFO [ContainerLauncher #8] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000000_0 : 53990
2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000001_0 TaskAttempt Transitioned from KILL_CONTAINER_CLEANUP to KILL_TASK_CLEANUP
2012-02-23 11:30:17,077 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1329995034628_0983_r_000001_0: Container was killed before it was launched
2012-02-23 11:30:17,078 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1329995034628_0983_r_000001_0
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
	at java.lang.Thread.run(Thread.java:619)
2012-02-23 11:30:17,080 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1329995034628_0983_r_000000_0] using containerId: [container_1329995034628_0983_01_000006 on NM: [$host6:51529]
2012-02-23 11:30:17,081 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1329995034628_0983_r_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING
2012-02-23 11:30:17,207 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1329995034628_0983_r_000002_0 : 47960
2012-02-23 11:30:17,207 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1329995034628_0983_r_000002_0
2012-02-23 11:30:17,215 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1329995034628_0983Job Transitioned from RUNNING to ERROR
2012-02-23 11:30:17,216 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)
	at java.lang.Thread.run(Thread.java:619)

{code}",,
MAPREDUCE-4008,"{code:xml}
2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default
org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)
	at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)
	at $Proxy6.postStart(Unknown Source)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)
Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)
	... 19 more
2012-03-14 15:22:23,090 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean ""null""
javax.management.RuntimeOperationsException: Exception occurred trying to register the MBean
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)
	at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)
	at $Proxy6.postStart(Unknown Source)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)
Caused by: java.lang.IllegalArgumentException: No object name specified
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)
	... 20 more
{code}",,
MAPREDUCE-4048,"{code:xml}
2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED
java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        .......
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
        at com.google.common.base.Joiner.toString(Joiner.java:317)
        at com.google.common.base.Joiner.appendTo(Joiner.java:97)
        at com.google.common.base.Joiner.appendTo(Joiner.java:127)
        at com.google.common.base.Joiner.join(Joiner.java:158)
        at com.google.common.base.Joiner.join(Joiner.java:166)
        at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)
        ... 36 more
{code}",,
MAPREDUCE-4224,"{code:xml}
2012-05-04 15:18:47,180 WARN  [main] util.MBeans (MBeans.java:getMBeanName(95)) - Error creating MBean object name: Hadoop:service=ResourceManager,name=RMNMInfo
org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=RMNMInfo already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)
	at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:93)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)
	at org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo.<init>(RMNMInfo.java:59)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:225)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler.setUp(TestFifoScheduler.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=RMNMInfo already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)
	... 30 more
{code}",,
MAPREDUCE-4262,"{code:xml}
2012-05-16 18:04:25,844 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Connected to ResourceManager at /xx.xx.xx.xx:8025
2012-05-16 18:04:26,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 0 time(s).
2012-05-16 18:04:27,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 1 time(s).
2012-05-16 18:04:28,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 2 time(s).
2012-05-16 18:04:29,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 3 time(s).
2012-05-16 18:04:30,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: host-xx-xx-xx-xx/xx.xx.xx.xx:8025. Already tried 4 time(s).
{code}",,
MAPREDUCE-4467,"TestMRJobs.testSleepJob fails randomly due to synchronization error in IndexCache:

{code}
2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error: 
java.lang.IllegalMonitorStateException
	at java.lang.Object.wait(Native Method)
	at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)
	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

A related issue is MAPREDUCE-4384. The change introduced there removed ""synchronized"" keyword and hence ""info.wait()"" call fails. Tbis needs to be wrapped into a ""synchronized"" block.",Sadness,-1
MAPREDUCE-4657,"When Shell command execution is interrupted then WindowsResourceCalculatorPlugin has NPE.
code}
2012-08-31 13:01:00,140 ERROR [Thread-771] util.WindowsResourceCalculatorPlugin(69): java.io.IOException: java.lang.InterruptedException^M
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:424)^M
        at org.apache.hadoop.util.Shell.run(Shell.java:336)^M
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:540)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getSystemInfoInfoFromShell(WindowsResourceCalculatorPlugin.java:66)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:81)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
^M
2012-08-31 13:01:00,140 ERROR [Thread-771] mapred.TaskTracker(1766): Caught exception: java.lang.NullPointerException^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:83)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
{code}",,
MAPREDUCE-4741,"The ApplicationMaster is logging WARN and ERROR messages during normal shutdown, and some users are misinterpreting these as serious problems.  For example:

{noformat}
2012-10-02 13:58:50,247 ERROR [ContainerLauncher Event Handler] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Returning, interrupted : java.lang.InterruptedException
[...]
2012-10-02 13:58:50,248 ERROR [Thread-47] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Returning, interrupted : java.lang.InterruptedException
2012-10-02 13:58:50,248 WARN [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Allocated thread interrupted. Returning.
[...]
2012-10-02 13:58:50,367 ERROR [TaskCleaner Event Handler] org.apache.hadoop.mapreduce.v2.app.taskclean.TaskCleanerImpl: Returning, interrupted : java.lang.InterruptedException
{noformat}

Warnings or errors should not be logged if everything is working as intended.",Surprise,-1
MAPREDUCE-4825,"TestMRApp.testJobError is causing AsyncDispatcher to exit with System.exit due to an exception being thrown.  From the console output from testJobError:

{noformat}
2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000
2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread
java.lang.IllegalArgumentException: Illegal job state: ERROR
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:662)
2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye..
{noformat}
",,
MAPREDUCE-4848,"Recently saw an AM that failed and tried to recover, but the subsequent attempt quickly exited with its own failure during recovery:

{noformat}
2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext
	at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)
	at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:619)
2012-12-05 02:33:36,752 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
{noformat}

The RM then launched a third AM attempt which succeeded. The third attempt saw basically no progress after parsing the history file from the second attempt and ran the job again from scratch.","Joy, Sadness, Surprise","1, -1"
MAPREDUCE-4913,"testMRAppMasterMissingStaging will sometimes cause the JVM to exit due to this error from AsyncDispatcher:

{noformat}
2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread
java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
        at java.lang.Thread.run(Thread.java:662)
2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye..
{noformat}

This can cause a build to fail since the test process exits without unregistering from surefire which treats it as a build error rather than a test failure.",Fear,-1
MAPREDUCE-5349,"The two unit tests fails due to MiniMRCluster use test class fullname in branch-2, instead of simple name as in trunk, to construct the MiniMRCluster identifier. Full name in the identifier almost always leads to a command script path with length larger than 260 characters which will generate an exception {{DefaultContainerExecutor.launchContainer()}} when launching the container script.

The exception looks like the follows:
{noformat}
2013-06-24 09:45:03,060 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(262)) - Failed to launch container.
java.io.IOException: Cannot launch container using script at path C:/Users/chuanliu/AppData/Local/Temp/1/1372092295656/org.apache.hadoop.mapred.ClusterMapReduceTestCaseConfigurableMiniMRCluster_1106798455-localDir-nm-0_1/usercache/chuanliu/appcache/application_1372092193505_0001/container_1372092193505_0001_01_000001/default_container_executor.cmd, because it exceeds the maximum supported path length of 260 characters.  Consider configuring shorter directories in yarn.nodemanager.local-dirs.
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:159)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:257)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}",,
MAPREDUCE-5358,"{code:xml}
2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:662)
{code}

{code:xml}
2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:662)
{code}",,
MAPREDUCE-5414,"Test case org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt fails once in a while when i run all of them together.
{code:xml} 
Running org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt
Tests run: 9, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 7.893 sec <<< FAILURE!
Results :

Tests in error:
  testLaunchFailedWhileKilling(org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt)
  testContainerCleanedWhileRunning(org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt)
  testContainerCleanedWhileCommitting(org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt)
  testDoubleTooManyFetchFailure(org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt)

Tests run: 9, Failures: 0, Errors: 4, Skipped: 0
{code}
But if i run a single test case,taking testContainerCleanedWhileRunning for example,it will fail without doubt.
{code:xml} 
 <testcase time=""0.057"" classname=""org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt"" name=""testContainerCleanedWhileRunning"">
    <error type=""java.lang.NullPointerException"">java.lang.NullPointerException
        at org.apache.hadoop.security.token.Token.write(Token.java:216)
        at org.apache.hadoop.mapred.ShuffleHandler.serializeServiceData(ShuffleHandler.java:205)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:695)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:751)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1309)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1282)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1009)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt.testContainerCleanedWhileRunning(TestTaskAttempt.java:410)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
</error>
    <system-out>2013-07-24 10:32:27,664 INFO  [main] util.RackResolver (RackResolver.java:coreResolve(100)) - Resolved 127.0.0.1 to /default-rack
2013-07-24 10:32:27,665 INFO  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:handle(1020)) - attempt_1_0002_m_000001_1 TaskAttempt Transitioned from NEW to UNASSIGNED
2013-07-24 10:32:27,666 INFO  [main] util.RackResolver (RackResolver.java:coreResolve(100)) - Resolved 127.0.0.1 to /default-rack
2013-07-24 10:32:27,668 INFO  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:createCommonContainerLaunchContext(636)) - Job jar is not present. Not adding any jar to the list of resources.
2013-07-24 10:32:27,669 INFO  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:createCommonContainerLaunchContext(653)) - The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/root/.staging/job_1_0001/job.xml
2013-07-24 10:32:27,669 INFO  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:createCommonContainerLaunchContext(675)) - Size of containertokens_dob is 1
2013-07-24 10:32:27,670 INFO  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:createCommonContainerLaunchContext(685)) - Putting shuffle token in serviceData
2013-07-24 10:32:27,671 WARN  [main] impl.TaskAttemptImpl (TaskAttemptImpl.java:createCommonContainerLaunchContext(688)) - Cannot locate shuffle secret in credentials. Using job token as shuffle secret.
</system-out>
{code}",Surprise,-1
MAPREDUCE-5724,"Starting JHS without HDFS running fails with the following error:

{code}
STARTUP_MSG:   build = git://git.apache.org/hadoop-common.git -r ad74e8850b99e03b0b6435b04f5b3e9995bc3956; compiled by 'tucu' on 2014-01-14T22:40Z
STARTUP_MSG:   java = 1.7.0_45
************************************************************/
2014-01-14 16:47:40,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2014-01-14 16:47:40,883 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-01-14 16:47:41,101 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: JobHistory Init
2014-01-14 16:47:41,710 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)
Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1359)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)
	at org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)
	at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)
	at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)
	at org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)
	at org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)
	at org.apache.hadoop.ipc.Client.call(Client.java:1377)
	... 28 more
2014-01-14 16:47:41,713 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistory failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)
Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1359)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)
	at org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)
	at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)
	at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)
	at org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)
	at org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)
	at org.apache.hadoop.ipc.Client.call(Client.java:1377)
	... 28 more
2014-01-14 16:47:41,714 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Stopping JobHistory
2014-01-14 16:47:41,714 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]
{code}
",,
MAPREDUCE-5744,"We ran into a situation where tasks are not getting assigned because RMContainerAllocator$AssignedRequests.preemptReduce() fails repeatedly with the following exception:

{code}
2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.
java.lang.IllegalArgumentException: Comparison method violates its general contract!
     at java.util.TimSort.mergeLo(TimSort.java:747)
     at java.util.TimSort.mergeAt(TimSort.java:483)
     at java.util.TimSort.mergeCollapse(TimSort.java:408)
     at java.util.TimSort.sort(TimSort.java:214)
     at java.util.TimSort.sort(TimSort.java:173)
     at java.util.Arrays.sort(Arrays.java:659)
     at java.util.Collections.sort(Collections.java:217)
     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)
     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)
     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)
     at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)
     at java.lang.Thread.run(Thread.java:744)
{code}

It is because the comparator that's defined in this method does not abide by the contract, specifically if p == 0.

Comparator.compare(): http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html#compare(T, T)",Sadness,-1
MAPREDUCE-5763,"{code}
2014-02-20 12:08:45,141 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: The Auxilurary Service named 'mapreduce_shuffle' in the configuration is for class class org.apache.hadoop.mapred.ShuffleHandler which has a name of 'httpshuffle'. Because these are not the same tools trying to send ServiceData and read Service Meta Data may have issues unless the refer to the name in the config.
2014-02-20 12:08:45,142 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service httpshuffle, ""mapreduce_shuffle""
{code}

I'm seeing this in my NodeManager logs,  even though things work fine.  A WARN is being caused by some sort of mismatch between the name of the service (in terms of org.apache.hadoop.service.Service.getName()) and the name of the auxiliary service.",Surprise,-1
MAPREDUCE-5837,"When the MRAppMaster determines whether the job should run in the uber mode, it call {{Class.forName()}} to check whether the class is derived from {{ChainMapper}}:

{code}
 try {
      String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);
      if (mapClassName != null) {
        Class<?> mapClass = Class.forName(mapClassName);
        if (ChainMapper.class.isAssignableFrom(mapClass))
          isChainJob = true;
      }
    } catch (ClassNotFoundException cnfe) {
      // don't care; assume it's not derived from ChainMapper
    }
{code}

The problem here is that {{Class.forName()}} can also throw {{NoClassDefError}}. It happens when the additional dependent jar is unavailable to the MRAppMaster. For example, the MRAppMaster complains about a MR job on Scala:

{noformat}
2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster
java.lang.NoClassDefFoundError: scala/Function1
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:190)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)
Caused by: java.lang.ClassNotFoundException: scala.Function1
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 22 more
{noformat}
 
The proposed fix is to catch {{NoClassDefError}} at the corresponding places.","Sadness, Surprise",-1
MAPREDUCE-5884,"When the owner of a token tries to explicitly cancel the token, it gets the following error/exception
{noformat} 
2014-04-14 20:07:35,744 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:<someuser>/<machine_name>.linkedin.com@<realm>.LINKEDIN.COM (auth:KERBEROS) cause:org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token
2014-04-14 20:07:35,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 10020, call org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB.cancelDelegationToken from 172.20.158.61:49042 Call#4 Retry#0: error: org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token
org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)
        at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)
        at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)
        at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)

{noformat}


Details:
AbstractDelegationTokenSecretManager.cacelToken() gets the owner as full principal name where as the canceller is the short name.
The potential code snippets:
{code}
String owner = id.getUser().getUserName(); 
    Text renewer = id.getRenewer();
    HadoopKerberosName cancelerKrbName = new HadoopKerberosName(canceller);
    String cancelerShortName = cancelerKrbName.getShortName();
    if (!canceller.equals(owner)
        && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName
            .equals(renewer.toString()))) {
      throw new AccessControlException(canceller
          + "" is not authorized to cancel the token"");
    }
{code}

The code shows 'owner' gets the full principal name. Where as the value of 'canceller' depends on who is calling it. 
In some cases, it is the short name. REF: HistoryClientService.java
{code}
String user = UserGroupInformation.getCurrentUser().getShortUserName();
        jhsDTSecretManager.cancelToken(token, user);
{code}
 
In other cases, the value could be full principal name. REF: FSNamesystem.java.
{code}
String canceller = getRemoteUser().getUserName();
      DelegationTokenIdentifier id = dtSecretManager
        .cancelToken(token, canceller);
{code}

Possible resolution:
--------------------------
Option 1: in cancelToken() method, compare with both : short name and full principal name.
Pros: Easy. Have to change in one place.
Cons: Someone can argue that it is hacky!
 
Option 2:
All the caller sends the consistent value as 'canceller' : either short name or full principal name.

Pros: Cleaner.
Cons: A lot of code changes and potential bug injections.

I'm open for both options.
Please give your opinion.

Btw, how it is working now in most cases?  The short name and the full principal name are usually the same for end-users.


","Fear, Surprise",-1
MAPREDUCE-5912,"{code}
@@ -1098,8 +1120,8 @@ private long calculateOutputSize() throws IOException {
     if (isMapTask() && conf.getNumReduceTasks() > 0) {
       try {
         Path mapOutput =  mapOutputFile.getOutputFile();
-        FileSystem localFS = FileSystem.getLocal(conf);
-        return localFS.getFileStatus(mapOutput).getLen();
+        FileSystem fs = mapOutput.getFileSystem(conf);
+        return fs.getFileStatus(mapOutput).getLen();
       } catch (IOException e) {
         LOG.warn (""Could not find output size "" , e);
       }
{code}

causes Windows local output files to be routed through HDFS:

{code}
2014-06-02 00:14:53,891 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.
       at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)
       at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)
       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)
       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)
       at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
       at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)
       at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)
       at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)
       at org.apache.hadoop.mapred.Task.done(Task.java:1048)
{code}
",,
MAPREDUCE-5931,"This is a minor issue per se. I had a typo in my script specifying a negative number of reducers for the SleepJob. It results in the exception that is far from the root cause, and appeared as a serious issue with the map-side sort.

{noformat}
2014-06-17 21:42:48,072 INFO [main] org.apache.hadoop.mapred.MapTask: Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@972141f
java.lang.NullPointerException
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1447)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:700)
	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1990)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:774)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:173)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
2014-06-17 21:42:48,075 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:330)
	at org.apache.hadoop.mapred.SpillRecord.<init>(SpillRecord.java:51)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1824)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1484)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:700)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:173)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
{noformat}",Surprise,-1
MAPREDUCE-5952,"The javadoc comment for {{renameMapOutputForReduce}} incorrectly refers to a single map output directory, whereas this depends on LOCAL_DIRS.
mapOutIndex should be set to subMapOutputFile.getOutputIndexFile()

{code}
2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.          TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist
  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)                      
  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)                          
  at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)    
  at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)    
  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)         
  at java.util.concurrent.FutureTask.run(FutureTask.java:138)                       
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
  at java.lang.Thread.run(Thread.java:695)         
{code}",Sadness,-1
MAPREDUCE-6002,"With MAPREDUCE-5900, preempted MR task should not be treat as failed. 
But it is still possible a MR task fail and report to AM when preemption take effect and the AM hasn't received completed container from RM yet. It will cause the task attempt marked failed instead of preempted.

An example is FileSystem has shutdown hook, it will close all FileSystem instance, if at the same time, the FileSystem is in-use (like reading split details from HDFS), MR task will fail and report the fatal error to MR AM. An exception will be raised:
{code}
2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)
	at java.io.DataInputStream.readByte(DataInputStream.java:265)
	at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)
	at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)
	at org.apache.hadoop.io.Text.readString(Text.java:464)
	at org.apache.hadoop.io.Text.readString(Text.java:457)
	at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
{code}

We should prevent this, because it is possible other exceptions happen when shutting down, we shouldn't report any of such exceptions to AM.","Fear, Surprise",-1
MAPREDUCE-6091,"If you query the job status of a job that rolled off the RM view via YARNRunner.getJobStatus(), it fails with an ApplicationNotFoundException. For example,

{noformat}
2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)
	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)

	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)
	at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)
	at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)
	at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)
	at java.lang.Thread.run(Thread.java:662)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)
{noformat}

Prior to 2.1.0, it used to be able to fall back onto the job history server and get the status.

This appears to be introduced by YARN-873. YARN-873 changed ClientRMService to throw an ApplicationNotFoundException on an unknown app id (from returning null). But MR's ClientServiceDelegate was never modified to change its behavior.","Sadness, Surprise",-1
MAPREDUCE-6213,"When DNS failed for a time, all MapReduce jobs which completed during that time got failed. Log as below:
{noformat}
2015-01-08 01:26:44,968 ERROR [Thread-856] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Exception while unregistering 
java.lang.NullPointerException
    at org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil.getApplicationWebURLOnJHSWithoutScheme(MRWebAppUtil.java:135)
    at org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(MRWebAppUtil.java:150)
    at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.doUnregistration(RMCommunicator.java:212)
    at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.unregister(RMCommunicator.java:182)
    at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.serviceStop(RMCommunicator.java:255)
    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.serviceStop(RMContainerAllocator.java:257)
    at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
    at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter.serviceStop(MRAppMaster.java:823)
    at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
    at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
    at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
    at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)
    at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1471)
    at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1085)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)
    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)
{noformat}",,
MAPREDUCE-6259,"-1 job submit time cause IllegalArgumentException when parse the Job history file name and JOB_INIT_FAILED cause -1 job submit time in JobIndexInfo.
We found the following job history file name which cause IllegalArgumentException when parse the job status in the job history file name.
{code}
job_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist
{code}
The stack trace for the IllegalArgumentException is
{code}
2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED
java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0
	at java.lang.Enum.valueOf(Enum.java:236)
	at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)
	at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)
	at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)
	at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)
	at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)
	at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)
	at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}


when IOException happened in JobImpl#setup, the Job submit time in JobHistoryEventHandler#MetaInfo#JobIndexInfo will not be changed and the Job submit time will be its [initial value -1|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java#L1185].
{code}
      this.jobIndexInfo =
          new JobIndexInfo(-1, -1, user, jobName, jobId, -1, -1, null,
                           queueName);
{code}

The following is the sequences to get -1 job submit time:
1. 
a job is created at MRAppMaster#serviceStart and  the new job is at state JobStateInternal.NEW after created
{code}
    job = createJob(getConfig(), forcedState, shutDownMessage);
{code}

2.
JobEventType.JOB_INIT is sent to JobImpl from MRAppMaster#serviceStart
{code}
      JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);
      // Send init to the job (this does NOT trigger job execution)
      // This is a synchronous call, not an event through dispatcher. We want
      // job-init to be done completely here.
      jobEventDispatcher.handle(initJobEvent);
{code}

3.
after JobImpl received JobEventType.JOB_INIT, it call InitTransition#transition
{code}
          .addTransition
              (JobStateInternal.NEW,
              EnumSet.of(JobStateInternal.INITED, JobStateInternal.NEW),
              JobEventType.JOB_INIT,
              new InitTransition())
{code}

4.
then the exception happen from setup(job) in InitTransition#transition before JobSubmittedEvent is handled.
JobSubmittedEvent will update the job submit time. Due to the exception, the submit time is still the initial value -1.
This is the code InitTransition#transition
{code}
public JobStateInternal transition(JobImpl job, JobEvent event) {
      job.metrics.submittedJob(job);
      job.metrics.preparingJob(job);
      if (job.newApiCommitter) {
        job.jobContext = new JobContextImpl(job.conf, job.oldJobId);
      } else {
        job.jobContext = new org.apache.hadoop.mapred.JobContextImpl(job.conf, job.oldJobId);
      }
      try {
        setup(job);
        job.fs = job.getFileSystem(job.conf);
        //log to job history
        JobSubmittedEvent jse = new JobSubmittedEvent(job.oldJobId,
              job.conf.get(MRJobConfig.JOB_NAME, ""test""), 
            job.conf.get(MRJobConfig.USER_NAME, ""mapred""),
            job.appSubmitTime,
            job.remoteJobConfFile.toString(),
            job.jobACLs, job.queueName,
            job.conf.get(MRJobConfig.WORKFLOW_ID, """"),
            job.conf.get(MRJobConfig.WORKFLOW_NAME, """"),
            job.conf.get(MRJobConfig.WORKFLOW_NODE_NAME, """"),
            getWorkflowAdjacencies(job.conf),
            job.conf.get(MRJobConfig.WORKFLOW_TAGS, """"));
        job.eventHandler.handle(new JobHistoryEvent(job.jobId, jse));
        //TODO JH Verify jobACLs, UserName via UGI?

        TaskSplitMetaInfo[] taskSplitMetaInfo = createSplits(job, job.jobId);
        job.numMapTasks = taskSplitMetaInfo.length;
        job.numReduceTasks = job.conf.getInt(MRJobConfig.NUM_REDUCES, 0);

        if (job.numMapTasks == 0 && job.numReduceTasks == 0) {
          job.addDiagnostic(""No of maps and reduces are 0 "" + job.jobId);
        } else if (job.numMapTasks == 0) {
          job.reduceWeight = 0.9f;
        } else if (job.numReduceTasks == 0) {
          job.mapWeight = 0.9f;
        } else {
          job.mapWeight = job.reduceWeight = 0.45f;
        }

        checkTaskLimits();

        long inputLength = 0;
        for (int i = 0; i < job.numMapTasks; ++i) {
          inputLength += taskSplitMetaInfo[i].getInputDataLength();
        }

        job.makeUberDecision(inputLength);
        
        job.taskAttemptCompletionEvents =
            new ArrayList<TaskAttemptCompletionEvent>(
                job.numMapTasks + job.numReduceTasks + 10);
        job.mapAttemptCompletionEvents =
            new ArrayList<TaskCompletionEvent>(job.numMapTasks + 10);
        job.taskCompletionIdxToMapCompletionIdx = new ArrayList<Integer>(
            job.numMapTasks + job.numReduceTasks + 10);

        job.allowedMapFailuresPercent =
            job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT, 0);
        job.allowedReduceFailuresPercent =
            job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT, 0);

        // create the Tasks but don't start them yet
        createMapTasks(job, inputLength, taskSplitMetaInfo);
        createReduceTasks(job);

        job.metrics.endPreparingJob(job);
        return JobStateInternal.INITED;
      } catch (Exception e) {
        LOG.warn(""Job init failed"", e);
        job.metrics.endPreparingJob(job);
        job.addDiagnostic(""Job init failed : ""
            + StringUtils.stringifyException(e));
        // Leave job in the NEW state. The MR AM will detect that the state is
        // not INITED and send a JOB_INIT_FAILED event.
        return JobStateInternal.NEW;
      }
    }
{code}

This is the code JobImpl#setup
{code}
    protected void setup(JobImpl job) throws IOException {

      String oldJobIDString = job.oldJobId.toString();
      String user = 
        UserGroupInformation.getCurrentUser().getShortUserName();
      Path path = MRApps.getStagingAreaDir(job.conf, user);
      if(LOG.isDebugEnabled()) {
        LOG.debug(""startJobs: parent="" + path + "" child="" + oldJobIDString);
      }

      job.remoteJobSubmitDir =
          FileSystem.get(job.conf).makeQualified(
              new Path(path, oldJobIDString));
      job.remoteJobConfFile =
          new Path(job.remoteJobSubmitDir, MRJobConfig.JOB_CONF_FILE);

      // Prepare the TaskAttemptListener server for authentication of Containers
      // TaskAttemptListener gets the information via jobTokenSecretManager.
      JobTokenIdentifier identifier =
          new JobTokenIdentifier(new Text(oldJobIDString));
      job.jobToken =
          new Token<JobTokenIdentifier>(identifier, job.jobTokenSecretManager);
      job.jobToken.setService(identifier.getJobId());
      // Add it to the jobTokenSecretManager so that TaskAttemptListener server
      // can authenticate containers(tasks)
      job.jobTokenSecretManager.addTokenForJob(oldJobIDString, job.jobToken);
      LOG.info(""Adding job token for "" + oldJobIDString
          + "" to jobTokenSecretManager"");

      // If the job client did not setup the shuffle secret then reuse
      // the job token secret for the shuffle.
      if (TokenCache.getShuffleSecretKey(job.jobCredentials) == null) {
        LOG.warn(""Shuffle secret key missing from job credentials.""
            + "" Using job token secret as shuffle secret."");
        TokenCache.setShuffleSecretKey(job.jobToken.getPassword(),
            job.jobCredentials);
      }
    }
{code}

5.
Due to the IOException from  JobImpl#setup, the new job is still at state JobStateInternal.NEW
{code}
      } catch (Exception e) {
        LOG.warn(""Job init failed"", e);
        job.metrics.endPreparingJob(job);
        job.addDiagnostic(""Job init failed : ""
            + StringUtils.stringifyException(e));
        // Leave job in the NEW state. The MR AM will detect that the state is
        // not INITED and send a JOB_INIT_FAILED event.
        return JobStateInternal.NEW;
      }
{code}
At the following code of MRAppMaster#serviceStart, The MR AM detect the state is not INITED and send a JOB_INIT_FAILED event.
{code}
      // If job is still not initialized, an error happened during
      // initialization. Must complete starting all of the services so failure
      // events can be processed.
      initFailed = (((JobImpl)job).getInternalState() != JobStateInternal.INITED);
    if (initFailed) {
      JobEvent initFailedEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT_FAILED);
      jobEventDispatcher.handle(initFailedEvent);
    } else {
      // All components have started, start the job.
      startJobs();
    }
{code}

6.
After JobImpl receives the JOB_INIT_FAILED, it will call InitFailedTransition#transition and enter state JobStateInternal.FAIL_ABORT
{code}
          .addTransition(JobStateInternal.NEW, JobStateInternal.FAIL_ABORT,
              JobEventType.JOB_INIT_FAILED,
              new InitFailedTransition())
{code}

7.
JobImpl will send CommitterJobAbortEvent in  InitFailedTransition#transition 
{code}
    public void transition(JobImpl job, JobEvent event) {
        job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,
                job.jobContext,
                org.apache.hadoop.mapreduce.JobStatus.State.FAILED));
    }
{code}

8.
CommitterJobAbortEvent will be handled by CommitterEventHandler#handleJobAbort which will send JobAbortCompletedEvent(JobEventType.JOB_ABORT_COMPLETED)
{code}
    protected void handleJobAbort(CommitterJobAbortEvent event) {
      cancelJobCommit();
      try {
        committer.abortJob(event.getJobContext(), event.getFinalState());
      } catch (Exception e) {
        LOG.warn(""Could not abort job"", e);
      }
      context.getEventHandler().handle(new JobAbortCompletedEvent(
          event.getJobID(), event.getFinalState()));
    }
{code}

9.
After JobImpl receives the JOB_ABORT_COMPLETED, it will call JobAbortCompletedTransition#transition and enter state JobStateInternal.FAILED
{code}
          .addTransition(JobStateInternal.FAIL_ABORT, JobStateInternal.FAILED,
              JobEventType.JOB_ABORT_COMPLETED,
              new JobAbortCompletedTransition())
{code}

10.
JobAbortCompletedTransition#transition will call JobImpl#unsuccessfulFinish which will send JobUnsuccessfulCompletionEvent with finish time.
{code}
    public void transition(JobImpl job, JobEvent event) {
      JobStateInternal finalState = JobStateInternal.valueOf(
          ((JobAbortCompletedEvent) event).getFinalState().name());
      job.unsuccessfulFinish(finalState);
    }
  private void unsuccessfulFinish(JobStateInternal finalState) {
      if (finishTime == 0) setFinishTime();
      cleanupProgress = 1.0f;
      JobUnsuccessfulCompletionEvent unsuccessfulJobEvent =
          new JobUnsuccessfulCompletionEvent(oldJobId,
              finishTime,
              succeededMapTaskCount,
              succeededReduceTaskCount,
              finalState.toString(),
              diagnostics);
      eventHandler.handle(new JobHistoryEvent(jobId,
          unsuccessfulJobEvent));
      finished(finalState);
  }
{code}

11.
JobUnsuccessfulCompletionEvent will be handled by JobHistoryEventHandler#handleEvent with type EventType.JOB_FAILED
Based on the following code, you can see the JobIndexInfo#finishTime is set correctly but JobIndexInfo#submitTime and  JobIndexInfo#jobStartTime are still -1.
{code}
      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED
          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {
        try {
          JobUnsuccessfulCompletionEvent jucEvent = 
              (JobUnsuccessfulCompletionEvent) event
              .getHistoryEvent();
          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());
          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());
          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());
          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());
          closeEventWriter(event.getJobID());
          processDoneFiles(event.getJobID());
        } catch (IOException e) {
          throw new YarnRuntimeException(e);
        }
      }
{code}

The error job history file name in our log is ""job_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist""
Based on the filename, you can see submitTime is -1, finishTime is 1423572836007 and jobStartTime is 1423572836007.
The jobStartTime is not -1, and  jobStartTime is the same as  finishTime.
It is because jobStartTime is handled specially in FileNameIndexUtils#getDoneFileName:
{code}
    //JobStartTime
    if (indexInfo.getJobStartTime() >= 0) {
      sb.append(indexInfo.getJobStartTime());
    } else {
      sb.append(indexInfo.getFinishTime());
    }
{code}

",Surprise,-1
MAPREDUCE-6273,"HistoryFileManager should check whether summaryFile exists to avoid FileNotFoundException causing HistoryFileInfo into MOVE_FAILED state,
I saw the following error message:
{code}
2015-02-17 19:13:45,198 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
java.io.FileNotFoundException: File does not exist: /user/history/done_intermediate/agd_laci-sluice/job_1423740288390_1884.summary
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1878)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1819)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1771)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:527)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:85)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:356)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1181)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1169)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1159)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:237)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:230)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1457)
	at org.apache.hadoop.fs.Hdfs.open(Hdfs.java:318)
	at org.apache.hadoop.fs.Hdfs.open(Hdfs.java:59)
	at org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:621)
	at org.apache.hadoop.fs.FileContext$6.next(FileContext.java:789)
	at org.apache.hadoop.fs.FileContext$6.next(FileContext.java:785)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.open(FileContext.java:785)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:953)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$400(HistoryFileManager.java:82)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:370)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1400(HistoryFileManager.java:295)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:843)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/history/done_intermediate/agd_laci-sluice/job_1423740288390_1884.summary
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:65)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:55)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1878)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1819)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1771)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:527)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:85)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:356)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:246)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1179)
	... 21 more
{code}
We should avoid this error by checking whether summaryFile exists before call getJobSummary, otherwise we will see this error happen every time scanIntermediateDirectory is called.",Fear,-1
MAPREDUCE-6357,"After spending the afternoon debugging a user job where reduce tasks were failing on retry with the below exception, I think it would be worthwhile to add a note in the MultipleOutputs.write() documentation, saying that absolute paths may cause improper execution of tasks on retry or when MR speculative execution is enabled. 

{code}
2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists:wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2
       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)
       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)
       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)
       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)
       at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)
       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)
       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)
       at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)
       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)
       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)
       at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
       at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
       at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:415)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
{code}

As discussed in MAPREDUCE-3772, when the baseOutputPath passed to MultipleOutputs.write() is an absolute path (or more precisely a path that resolves outside of the job output-dir), the concept of output committing is not utilized. 

In this case, the user read thru the MultipleOutputs docs and was assuming that everything will be working fine, as there are blog posts saying that MultipleOutputs does handle output commit. ",Fear,-1
MAPREDUCE-6492,"For {{TaskAttemptImpl#DeallocateContainerTransition}} {{sendJHStartEventForAssignedFailTask}} is send for TaskAttemptStateInternal.UNASSIGNED also .


Causing NPE on {{taskAttempt.container.getNodeHttpAddress()}} 


{noformat}
2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)
2015-09-28 18:01:48,660 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2015-09-28 18:01:48,660 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_e04_1443430524957_0006_01_000059 taskAttempt attempt_1443430524957_0006_m_000000_9
{noformat}

Log aggregation fail for mapreduce application.
",,
MAPREDUCE-6554,"Create scenario so that MR app master gets preempted.
On next MRAppMaster launch tried to recover previous job history file {{MRAppMaster#parsePreviousJobHistory}}


{noformat}
2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at java.io.StringReader.<init>(StringReader.java:50)
        at org.apache.avro.Schema$Parser.parse(Schema.java:917)
        at org.apache.avro.Schema.parse(Schema.java:966)
        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)
        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)
2015-11-21 13:52:27,725 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0

{noformat}

EventReader(EventReader stream)
{noformat}
 this.version = in.readLine();
...
    Schema myschema = new SpecificData(Event.class.getClassLoader()).getSchema(Event.class);
    this.schema = Schema.parse(in.readLine());
{noformat}

",,
MAPREDUCE-6577,"If yarn.app.mapreduce.am.admin.user.env (or yarn.app.mapreduce.am.env) is not configured to set LD_LIBRARY_PATH, MR AM will fail to load the native library:

{noformat}
2015-12-15 21:29:22,473 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{noformat}

As a result, any code that needs the hadoop native library in the MR AM will fail. For example, an uber-AM with lz4 compression for the mapper task will fail:
{noformat}
2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available
	at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)
	at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)
	at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)
	at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)
	at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",Fear,-1
MAPREDUCE-6649,"The following command does not produce any failure info as to why the job failed. 

{noformat}
$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1
{noformat}

{noformat}
2016-03-07 10:34:58,112 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0004 failed with state FAILED due to: 
{noformat}

To contrast, here is a command and associated command line output to show a failed job that gives the correct failiure info. 

{noformat}
$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dyarn.app.mapreduce.am.command-opts=-goober -Dmapreduce.job.queuename=default -m 20 -r 0 -mt 30000
{noformat}

{noformat}
2016-03-07 10:30:13,103 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0003 failed with state FAILED due to: Application application_1457364518683_0003 failed 3 times due to AM Container for appattempt_1457364518683_0003_000003 exited with  exitCode: 1
Failing this attempt.Diagnostics: Exception from container-launch.
Container id: container_1457364518683_0003_03_000001
Exit code: 1
Stack trace: ExitCodeException exitCode=1: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)
	at org.apache.hadoop.util.Shell.run(Shell.java:838)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",Sadness,-1
MAPREDUCE-6693,"Job history entry missing when JOB name is of {{mapreduce.jobhistory.jobname.limit}} character

{noformat}
2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Interrupting Event Handling thread
2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Waiting for Event Handling thread to complete
2016-05-10 06:51:00,674 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.
java.lang.ArrayIndexOutOfBoundsException: 50
	at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)
	at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)
	at java.lang.Thread.run(Thread.java:745)
2016-05-10 06:51:00,675 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Shutting down timer for Job MetaInfo for job_1462840033869_0009 history file hdfs://hacluster:9820/staging-dir/dsperf/.staging/job_1462840033869_0009/job_1462840033869_0009_1.jhist
2016-05-10 06:51:00,675 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Shutting down timer Job MetaInfo for job_1462840033869_0009 history file hdfs://hacluster:9820/staging-dir/dsperf/.staging/job_1462840033869_0009/job_1462840033869_0009_1.jhist
2016-05-10 06:51:00,676 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Closing Writer
{noformat}

Looks like 50 character check is going wrong",,
MAPREDUCE-6836,"When I navigate the MR job web UI and click the configuration link, the AM shows an exception:
{noformat}
2017-01-25 11:40:55,521 ERROR [qtp2126664214-26] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduc
e/conf/job_1485372765455_0002
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
        at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:179)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1458)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:524)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:319)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:253)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error rendering block: nestLevel=6 expected 5
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:72)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:235)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)
        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)
        at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.conf(AppController.java:323)
        ... 52 more
{noformat}

The web page itself renders fine.",,
MAPREDUCE-6898,"TestKill.testKillTask() can fail if the async dispatcher thread is slower than the test's thread.

{noformat}
2017-05-26 11:43:26,532 INFO  [AsyncDispatcher event handler] impl.JobImpl (JobImpl.java:handle(1006)) - job_0_0000Job Transitioned from INITED to SETUP
Job State is : RUNNING
Job State is : RUNNING Waiting for state : SUCCEEDED   map progress : 0.0   reduce progress : 0.0
2017-05-26 11:43:26,538 INFO  [CommitterEvent Processor #0] commit.CommitterEventHandler (CommitterEventHandler.java:run(231)) - Processing the event EventType: JOB_SETUP
2017-05-26 11:43:26,540 INFO  [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:handle(661)) - task_0_0000_m_000000 Task Transitioned from NEW to KILLED
2017-05-26 11:43:26,540 ERROR [AsyncDispatcher event handler] impl.JobImpl (JobImpl.java:handle(998)) - Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_COMPLETED at SETUP
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:996)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:138)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1366)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1362)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:182)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)
2017-05-26 11:43:26,541 INFO  [AsyncDispatcher event handler] impl.JobImpl (JobImpl.java:handle(1006)) - job_0_0000Job Transitioned from SETUP to ERROR
2017-05-26 11:43:26,542 INFO  [AsyncDispatcher event handler] app.MRAppMaster (MRAppMaster.java:serviceStop(978)) - Skipping cleaning up the staging dir. assuming AM will be retried.
{noformat}

We have to wait until the job's internal state is {{JobInternalState.RUNNING}} and not {{JobInternalState.SETUP}}.",,
MAPREDUCE-7059,"Running teragen failed in the version of hadoop-3.1, and hdfs server is 2.8.
{code:java}
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 100000 /teragen
{code}

The reason of failing is 2.8 HDFS does not have setErasureCodingPolicy.

one  solution is parsing RemoteException in JobResourceUploader#disableErasure like this:
{code:java}
private void disableErasureCodingForPath(FileSystem fs, Path path)
      throws IOException {
    try {
      if (jtFs instanceof DistributedFileSystem) {
        LOG.info(""Disabling Erasure Coding for path: "" + path);
        DistributedFileSystem dfs = (DistributedFileSystem) jtFs;
        dfs.setErasureCodingPolicy(path,
            SystemErasureCodingPolicies.getReplicationPolicy().getName());
      }
    } catch (RemoteException e) {
      if (!e.getClassName().equals(RpcNoSuchMethodException.class.getName())) {
        throw e;
      } else {
        LOG.warn(
            ""hdfs server does not have method disableErasureCodingForPath,"" 
                + "" and skip disableErasureCodingForPath"", e);
      }
    }
  }
{code}

Does anyone have better solution?

The detailed exception trace is:
{code:java}
2018-02-26 11:22:53,178 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1518615699369_0006
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1437)
	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)
	at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)
	at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)
	at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)
	at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)
	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:304)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:218)
{code}
",,
MAPREDUCE-7077,"Steps:

Launch wordcount example with pipe
{code}
/usr/hdp/current/hadoop-client/bin/hadoop pipes ""-Dhadoop.pipes.java.recordreader=true"" ""-Dhadoop.pipes.java.recordwriter=true"" -input pipeInput -output pipeOutput -program bin/wordcount{code}

The application fails with below stacktrace
{code:title=AM}
attempt_1517534613368_0041_r_000000_2 is : 0.0

2018-02-02 02:40:51,071 ERROR [IPC Server handler 16 on 43391] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1517534613368_0041_r_000000_2 - exited : java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)

 at java.io.FileOutputStream.open0(Native Method)

 at java.io.FileOutputStream.open(FileOutputStream.java:270)

 at java.io.FileOutputStream.<init>(FileOutputStream.java:213)

 at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)

 at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)

 at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)

 at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)

 at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)

 at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)

 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)

 at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)

 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)

 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)

 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)

 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)

 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)

 at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)

 at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)

 at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)

 at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)

 at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)

 at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)

 at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)

 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)

 at java.security.AccessController.doPrivileged(Native Method)

 at javax.security.auth.Subject.doAs(Subject.java:422)

 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)

 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
{code}
",,
MAPREDUCE-838,"In MAPREDUCE-837, job succeeded with empty output even though all the tasks were throwing IOException at commiter.commitTask.

{noformat}
2009-08-07 17:51:47,458 INFO org.apache.hadoop.mapred.TaskRunner: Task attempt_200907301448_8771_r_000000_0 is allowed to commit now
2009-08-07 17:51:47,466 WARN org.apache.hadoop.mapred.TaskRunner: Failure committing: java.io.IOException: Can not get the relative path: \
base = hdfs://mynamenode:8020/user/knoguchi/test2.har/_temporary/_attempt_200907301448_8771_r_000000_0 \
child = hdfs://mynamenode/user/knoguchi/test2.har/_temporary/_attempt_200907301448_8771_r_000000_0/_index
  at org.apache.hadoop.mapred.FileOutputCommitter.getFinalPath(FileOutputCommitter.java:150)
  at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:106)
  at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:126)
  at org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:86)
  at org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:171)
  at org.apache.hadoop.mapred.Task.commit(Task.java:768)
  at org.apache.hadoop.mapred.Task.done(Task.java:692)
  at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
  at org.apache.hadoop.mapred.Child.main(Child.java:170)

2009-08-07 17:51:47,468 WARN org.apache.hadoop.mapred.TaskRunner: Failure asking whether task can commit: java.io.IOException: \
Can not get the relative path: base = hdfs://mynamenode:8020/user/knoguchi/test2.har/_temporary/_attempt_200907301448_8771_r_000000_0 \
child = hdfs://mynamenode/user/knoguchi/test2.har/_temporary/_attempt_200907301448_8771_r_000000_0/_index
  at org.apache.hadoop.mapred.FileOutputCommitter.getFinalPath(FileOutputCommitter.java:150)
  at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:106)
  at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:126)
  at org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:86)
  at org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:171)
  at org.apache.hadoop.mapred.Task.commit(Task.java:768)
  at org.apache.hadoop.mapred.Task.done(Task.java:692)
  at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
  at org.apache.hadoop.mapred.Child.main(Child.java:170)

2009-08-07 17:51:47,469 INFO org.apache.hadoop.mapred.TaskRunner: Task attempt_200907301448_8771_r_000000_0 is allowed to commit now
2009-08-07 17:51:47,472 INFO org.apache.hadoop.mapred.TaskRunner: Task 'attempt_200907301448_8771_r_000000_0' done.


{noformat}
",Surprise,-1
STORM-1114,"In production for some trident topology, we met the bug that some workers are trying to create a zk-node that is already existent or delete a zk node that has already been deleted. This causes the worker process to die.
 
We dissect the problem and figure out that there exists racing condition in trident TransactionalState's zk-node create and delete codes.

failure stack trace in worker.log:
{noformat}
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /ignoreStoredMetadata
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:100) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        ... 9 more
2015-10-14 18:10:43.786 b.s.util [ERROR] Halting process: (""Worker died"")
{noformat}


{noformat}
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /rainbowHdfsPath
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at storm.trident.topology.state.TransactionalState.delete(TransactionalState.java:126) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        ... 12 more
2015-10-14 18:10:28.799 b.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
{noformat}",Surprise,-1
STORM-1208,"A stack trace is seen on the UI via its thrift connection to nimbus.

On nimbus, a stack trace similar to the following is seen:

{noformat}
2015-11-09 19:26:48.921 o.a.t.s.TThreadPoolServer [ERROR] Error occurred during processing of message.
java.lang.NullPointerException
        at backtype.storm.stats$agg_bolt_streams_lat_and_count$iter__2219__2223$fn__2224.invoke(stats.clj:346) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:?]
        at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:?]
        at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.6.0.jar:?]
        at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:?]
        at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:?]
        at clojure.core$into.invoke(core.clj:6341) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$agg_bolt_streams_lat_and_count.invoke(stats.clj:344) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.stats$agg_pre_merge_comp_page_bolt.invoke(stats.clj:439) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.stats$fn__2578.invoke(stats.clj:1093) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.MultiFn.invoke(MultiFn.java:241) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
        at clojure.core$apply.invoke(core.clj:628) ~[clojure-1.6.0.jar:?]
        at clojure.core$partial$fn__4230.doInvoke(core.clj:2470) ~[clojure-1.6.0.jar:?]
        at clojure.lang.RestFn.invoke(RestFn.java:421) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6086.invoke(protocols.clj:143) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6057$G__6052__6066.invoke(protocols.clj:19) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:?]
        at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$aggregate_comp_stats_STAR_.invoke(stats.clj:1106) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
        at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$fn__2589.doInvoke(stats.clj:1127) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.RestFn.invoke(RestFn.java:436) ~[clojure-1.6.0.jar:?]
        at clojure.lang.MultiFn.invoke(MultiFn.java:236) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$agg_comp_execs_stats.invoke(stats.clj:1303) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.daemon.nimbus$fn__5893$exec_fn__1502__auto__$reify__5917.getComponentPageInfo(nimbus.clj:1715) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3677) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3661) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:143) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) [storm-core-0.10.1.jar:0.10.1]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_40]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]
{noformat}
",,
STORM-1470,"{noformat}
2016-01-12 20:07:45.642 o.a.s.s.o.e.j.s.ServletHandler [WARN] Error for /favicon.ico
java.lang.NoClassDefFoundError: org/apache/commons/codec/binary/Base64
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:343)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:519)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

We already shade commons-codec:commons-codec, but we don't apply that shading to org.apache.hadoop:hadoop-auth.
",,
STORM-1496,"Blobstore periodically throws exception:

{code}
2016-01-22 16:13:21.205 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormjar.jar with id 0d84a3e1-e4c9-46fc-b0c7-8f02f48fb016
2016-01-22 16:13:21.499 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormcode.ser with id c33cbb90-92a1-4191-8ac0-24b2547503da
2016-01-22 16:13:21.503 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormconf.ser with id ac1ba1ce-5697-4490-84eb-2c74b6415d62
2016-01-22 16:15:19.509 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.IllegalArgumentException: No matching method found: readBlob for class java.lang.String
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$read_storm_topology_as_nimbus.invoke(nimbus.clj:529)
	at org.apache.storm.daemon.nimbus$try_read_storm_topology.invoke(nimbus.clj:1249)
	at org.apache.storm.daemon.nimbus$fn__7312$exec_fn__1827__auto__$reify__7341.getTopology(nimbus.clj:1776)
	at org.apache.storm.generated.Nimbus$Processor$getTopology.getResult(Nimbus.java:3878)
	at org.apache.storm.generated.Nimbus$Processor$getTopology.getResult(Nimbus.java:3862)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Steps to reproduce:

1. Setup one node cluster.
2. Deploy word count topology.
3. Kill word count topology.
4. Monitor nimbus.log",,
STORM-1520,"Placeholder until I can gather more information for reproducing the issue.

The following appears in nimbus.log after deploying/undeploying topologies:

{code}
2016-02-02 21:34:04.308 o.a.s.s.o.a.c.f.l.ListenerContainer [ERROR] Listener (org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660@22587507) threw an exception
java.lang.IllegalArgumentException: No matching method found: stateChanged for class org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660.stateChanged(zookeeper_state_factory.clj:145)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)
	at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at org.apache.storm.shade.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:84)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

-Basic functionality does not seem to be affected.-

Nimbus becomes unresponsive and needs to be manually restarted.
",Surprise,-1
STORM-1596,"With multiple threads accessing same {{Subject}}, it can cause {{ServiceTicket}} in use be by one thread be destroyed by another thread.

Running BasicDRPCTopology with high parallelism in secure cluster would reproduce the issue.

Here is sample log from such a scenarios:
{code}
2016-01-20 15:52:26.904 o.a.t.t.TSaslTransport [ERROR] SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]
        at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1.y]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: The ticket isn't for us (35) - BAD TGS SERVER NAME)
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.KrbException: The ticket isn't for us (35) - BAD TGS SERVER NAME
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.init(TGSRep.java:65) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more


{code}",Fear,-1
STORM-1666,"{code}
2016-03-30 14:02:21.613 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.NullPointerException
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:26)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099$iter__3210__3214$fn__3215.invoke(nimbus.clj:1888)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.RT.seq(RT.java:507)
        at clojure.core$seq__4128.invoke(core.clj:137)
        at clojure.core$dorun.invoke(core.clj:3009)
        at clojure.core$doall.invoke(core.clj:3025)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099.getTopologyInfoWithOpts(nimbus.clj:1885)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3774)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3758)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}",,
STORM-1672,"Component page in UI
{code}
2016-03-31 14:21:44.576 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map
        at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)
        at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)
        at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)
        at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)
        at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)
        at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}",,
STORM-1941,"When zookeeper reconnect happens, nimbus registry can be deleted though nimbus is alive.

Below is zookeeper node for nimbus registry.

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x4000005ae
mtime = Fri Jul 01 11:43:51 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x50000000e
mtime = Fri Jul 01 11:46:08 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

Below is transaction log for that node.
{code}
7/1/16 11:43:51 AM UTC session 0x255a62e310c0005 cxid 0xd zxid 0x4000005ae create '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,v{s{31,s{'world,'anyone}}},T,10

7/1/16 11:46:08 AM UTC session 0x355a647bd8c0000 cxid 0x3 zxid 0x50000000e setData '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,1
{code}

Please take a look at ctime, mtime, and ephemeralOwner.
Ephemeral owner session was already closed from nimbus side but there's possible for node to be not deleted immediately, so new session doesn't create new node but set the value to ephemeral node for other session which is already closed.
*And eventually that node is deleted although session 0x355a647bd8c0000 is alive.*

{code}
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ClientCnxn [DEBUG] Disconnecting client for session: 0x255a62e310c0005
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x255a62e310c0005 closed
{code}

We can delete the node first and set ephemeral node when reconnect event handler is called.","Fear, Sadness, Surprise",-1
STORM-1977,"While investigating STORM-1976, I found that there're cases for nimbus to not having topology codes. 
Before BlobStore, only nimbuses which is having all topology codes can gain leadership, otherwise they give up leadership immediately. While introducing BlobStore, this logic is removed.

I don't know it's intended or not, but it incurs one of nimbus to gain leadership which doesn't have replicated topology code, and the nimbus will be crashed when getClusterInfo is requested.

Easiest way to reproduce is:

1. comment cleanup-corrupt-topologies! from nimbus.clj (It's a quick workaround for resolving STORM-1976), and patch Storm cluster
2. Launch Nimbus 1 (leader)
3. Run topology
4. Kill Nimbus 1
5. Launch Nimbus 2 from different node
6. Nimbus 2 gains leadership 
7. getClusterInfo is requested to Nimbus 2, and Nimbus 2 gets crashed

Log:

{code}
2016-07-17 08:47:48.378 o.a.s.b.FileBlobStoreImpl [INFO] Creating new blob store based in /grid/0/hadoop/storm/blobs
...
2016-07-17 08:47:48.619 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-07-17 08:47:48.651 o.a.s.zookeeper [INFO] <node1> gained leadership
...
2016-07-17 08:47:48.833 o.a.s.d.nimbus [INFO] Starting nimbus server for storm version '1.1.1-SNAPSHOT'
2016-07-17 08:47:49.295 o.a.s.t.ProcessFunction [ERROR] Internal error processing getClusterInfo
KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)
...
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__9520__9524$fn__9525.invoke(nimbus.clj:1427)
...
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3708)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
...
2016-07-17 08:47:49.397 o.a.s.b.BlobStoreUtils [ERROR] Could not download blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.400 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.402 o.a.s.d.nimbus [ERROR] Error when processing event
KeyNotFoundException(msg:production-topology-2-1468745167-stormconf.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:239)
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271)
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300)
...
       at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
        at org.apache.storm.daemon.nimbus$read_storm_conf_as_nimbus.invoke(nimbus.clj:548)
        at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:555)
        at org.apache.storm.daemon.nimbus$mk_assignments$iter__9205__9209$fn__9210.invoke(nimbus.clj:912)
...
        at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:911)
        at clojure.lang.RestFn.invoke(RestFn.java:410)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781$fn__9782.invoke(nimbus.clj:2216)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781.invoke(nimbus.clj:2215)
        at org.apache.storm.timer$schedule_recurring$this__1732.invoke(timer.clj:105)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:50)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
...
2016-07-17 08:47:49.408 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.nimbus$nimbus_data$fn__8727.invoke(nimbus.clj:205)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:71)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
2016-07-17 08:47:49.410 o.a.s.d.nimbus [INFO] Shutting down master
{code}
",Fear,-1
STORM-2142,"When EvaluationFilter / EvaluationFunction throws Exception, async loop for the executor is died but others will continue to work.

{code}
2016-10-08 14:12:29.597 o.a.s.u.Utils Thread-23-b-0-LOGICALFILTER_6-LOGICALPROJECT_7-executor[5 5] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
...
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
        at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982) ~[dep-janino-2.7.6-dcb5bd18-a5dd-4976-a967-0108dcf46df0.jar.1475903522000:2.7.6]
...
Caused by: java.lang.RuntimeException: Cannot convert null to int
        at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at SC.eval0(Unknown Source) ~[?:?]
{code}

While looking into detail, I found that ReportErrorAndDie implementation seems odd - completely opposite behavior compared to 1.x :report-error-and-die.
When InterruptedException or InterruptedIOException is thrown, it should just leave a log and shouldn't run suicide function. For others it should run suicide function.",Surprise,-1
STORM-2158,"{{OutOfMemoryError}} is thrown by Nimbus' {{SimpleTransportPlugin}} if malformed Thrift request is posted:
{code}
echo ""Hello"" | nc localhost 6627
{code}

In nimbus.log:
{noformat}
2016-10-20 12:54:09.978 b.s.d.nimbus [INFO] Starting Nimbus server...
2016-10-20 12:54:42.926 o.a.t.s.THsHaServer [ERROR] run() exiting due to uncaught error
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[?:1.8.0_92-internal]
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[?:1.8.0_92-internal]
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158) [storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
2016-10-20 12:54:42.942 b.s.d.nimbus [INFO] Shutting down master
2016-10-20 12:54:43.003 b.s.d.nimbus [INFO] Shut down master
{noformat}

The problem is caused by the lack of specification of the {{maxReadBufferBytes}} of {{THsHaServer}}'s arguments.",,
STORM-2275,"I am copying last few lines of the nimbus logs including stack trace.
{code}
2017-01-04 22:18:10.106 pool-15-thread-47 o.a.s.d.n.Nimbus [INFO] Activating DemoTest: DemoTest-21-1483568289
2017-01-04 22:18:11.646 timer o.a.s.s.EvenScheduler [INFO] Available slots: [f0ea57ab-86d6-401f-9429-52f479b1d69f:6704, f0ea57ab-86d6-401f-9429-52f479b1d69f:6705, f0ea57ab-86d6-401f-9429-52f479b1d69f:670\
6, f0ea57ab-86d6-401f-9429-52f479b1d69f:6707, f0ea57ab-86d6-401f-9429-52f479b1d69f:6708, f0ea57ab-86d6-401f-9429-52f479b1d69f:6709, f0ea57ab-86d6-401f-9429-52f479b1d69f:6700, f0ea57ab-86d6-401f-9429-52f4\
79b1d69f:6701, f0ea57ab-86d6-401f-9429-52f479b1d69f:6702, f0ea57ab-86d6-401f-9429-52f479b1d69f:6703]
2017-01-04 22:18:11.648 timer o.a.s.d.n.Nimbus [INFO] Setting new assignment for topology id DemoTest-21-1483568289: Assignment(master_code_dir:storm-local, node_host:{f0ea57ab-86d6-401f-9429-52f479b1d69\
f=node1}, executor_node_port:{[10, 10]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [14, 14]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [16, 16]=NodeInfo(node:\
f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [12, 12]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [8, 8]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [6,\
 6]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [20, 20]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [4, 4]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6700]), [2, 2]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [18, 18]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [11, 11]=NodeInfo(node:f0ea57ab-86d6-401\
f-9429-52f479b1d69f, port:[6701]), [15, 15]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [7, 7]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [9, 9]=NodeInfo(node\
:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [21, 21]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [5, 5]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [3\
, 3]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [19, 19]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [17, 17]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d6\
9f, port:[6701]), [1, 1]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [13, 13]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])}, executor_start_time_secs:{[12, 12]=1\
483568291, [6, 6]=1483568291, [18, 18]=1483568291, [2, 2]=1483568291, [8, 8]=1483568291, [14, 14]=1483568291, [16, 16]=1483568291, [20, 20]=1483568291, [4, 4]=1483568291, [10, 10]=1483568291, [9, 9]=1483\
568291, [3, 3]=1483568291, [15, 15]=1483568291, [21, 21]=1483568291, [5, 5]=1483568291, [11, 11]=1483568291, [13, 13]=1483568291, [17, 17]=1483568291, [19, 19]=1483568291, [1, 1]=1483568291, [7, 7]=14835\
68291}, worker_resources:{NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6701])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)})
2017-01-04 22:18:11.660 timer o.a.s.d.n.Nimbus [INFO] Cleaning up DemoTest-20-1483567429
2017-01-04 22:18:11.668 timer o.a.s.d.n.Nimbus [INFO] Removing dependency jars from blobs - []
2017-01-04 22:18:12.420 pool-15-thread-51 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormjar.jar
2017-01-04 22:18:12.990 pool-15-thread-38 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormcode.ser
2017-01-04 22:18:12.995 pool-15-thread-59 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormconf.ser
2017-01-04 22:18:20.303 timer o.a.s.d.n.Nimbus [INFO] TRANSITION: DemoTest-20-1483567429 REMOVE null false
2017-01-04 22:18:20.304 timer o.a.s.d.n.Nimbus [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)
        ... 1 more
2017-01-04 22:18:20.304 timer o.a.s.u.Utils [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)
2017-01-04 22:18:20.315 Thread-9 o.a.s.d.n.Nimbus [INFO] Shutting down master
{code}

The problem is that we are assuming that the base will be non-null which is incorrect leading to NPE.",Sadness,-1
STORM-2279,"With latest storm code, I am unable to open ui and see bolt information. I am using the vagrant setup. On the ui page that open, I see the following error.
{code}
Internal Server Error
org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1369)
	at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1353)
	at org.apache.storm.ui.core$component_page.invoke(core.clj:1026)
	at org.apache.storm.ui.core$fn__4308.invoke(core.clj:1214)
	at org.apache.storm.shade.compojure.core$make_route$fn__789.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__777.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__770.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__795.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__799.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__3573.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__3102.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__2152.invoke(helpers.clj:54)
	at org.apache.storm.ui.core$catch_errors$fn__4474.invoke(core.clj:1460)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__1844.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__1887.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__1816.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2139.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2125.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__1674.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__1678.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{code}
Url: http://node1:8080/component.html?id=SlidingTimeCorrectness-winSec1slideSec1VerificationBolt&topology_id=SlidingWindowTestw1s1-2-1483646178

There is a stacktrace corresponding to this in nimbus.log showing IndexOutOfBound error:
{code}
2017-01-05 19:57:26.934 pool-15-thread-41 o.a.s.d.n.Nimbus [WARN] getComponentPageInfo exception. (topo id='SlidingWindowTestw1s1-2-1483646178')
java.lang.ArrayIndexOutOfBoundsException: -2
        at java.util.ArrayList.elementData(ArrayList.java:418)
        at java.util.ArrayList.get(ArrayList.java:431)
        at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:3606)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4097)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4081)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

The problem is that we expect the index to be positive, but since it is a mod of hashcode it can be negative.
{code}
                int taskIndex = TupleUtils.listHashCode(Arrays.asList(componentId)) %
                        tasks.size();
                int taskId = tasks.get(taskIndex);
{code}
https://github.com/apache/storm/blob/2b82fc8b5328fd4fbd680998c6051d9496c102d7/storm-core/src/jvm/org/apache/storm/daemon/nimbus/Nimbus.java#L3605
",Surprise,-1
STORM-2321,"The nimbus was restarted during HA testing. After the restart the nimbus failed to come up. 
{code}
2017-01-18 04:57:58.231 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2017-01-18 04:57:58.247 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyKillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
2017-01-18 04:57:58.273 o.a.s.b.KeySequenceNumber [ERROR] Exception {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:149)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.274 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2017-01-18 04:57:58.296 o.a.s.m.n.Login [INFO] successfully logged in.
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x359afc1eaa2009b closed
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
2017-01-18 04:57:58.310 o.a.s.t.s.TThreadPoolServer [ERROR] Error occurred during processing of message.
java.util.NoSuchElementException
	at java.util.TreeMap.key(TreeMap.java:1327)
	at java.util.TreeMap.lastKey(TreeMap.java:297)
	at java.util.TreeSet.last(TreeSet.java:401)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:206)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.311 o.a.s.d.nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:92)
	at org.apache.storm.daemon.nimbus$fn__9373.invoke(nimbus.clj:1452)
	at clojure.lang.MultiFn.invoke(MultiFn.java:233)
	at org.apache.storm.daemon.nimbus$fn__9770$exec_fn__3656__auto____9771$fn__9786.invoke(nimbus.clj:2452)
	at org.apache.storm.timer$schedule_recurring$this__2188.invoke(timer.clj:105)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:50)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:114)
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:76)
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:252)
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:111)
	... 9 more
Caused by: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:349)
	at org.apache.storm.blobstore.BlobStoreUtils.createStateInZookeeper(BlobStoreUtils.java:217)
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:249)
	... 10 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)
	at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)
	at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)
	at org.apache.storm.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_createStateInZookeeper(Nimbus.java:1000)
	at org.apache.storm.generated.Nimbus$Client.createStateInZookeeper(Nimbus.java:987)
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:346)
	... 12 more
2017-01-18 04:57:58.314 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__8579.invoke(nimbus.clj:212)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:71)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
{code}",,
STORM-2324,"Since Storm 1.0,  due to STORM-876,
When the topo jar does not contain a resources directory, the topology fails if
{code}
supervisor.run.worker.as.user : true
{code}

Assessment:
After unpacking the topo jar file, supervisor.clj establishes a symlink to the resources directory without checking if the topology jar actually had a resources directory. This leads to a broken symlink. 

Subsequently when the worker-launcher tool runs and tries to chmod the resources dir, it fails. This stalls the topology execution.


{code}
2017-01-17 15:48:12.413 o.a.s.d.supervisor [INFO] Running as user:xyz-admin command:(""/usr/hdp/2.5.3.0-37/storm/bin/worker-launcher"" ""xyz-admin"" ""worker"" ""/apps/hadoop/storm/workers/46911423-af41-4db3-8480-7a95df96632a"" ""/apps/hadoop/storm/workers/46911423-af41-4db3-8480-7a95df96632a/storm-worker-script.sh"")
2017-01-17 15:48:12.417 o.a.s.d.supervisor [INFO] 46911423-af41-4db3-8480-7a95df96632a still hasn't started
2017-01-17 15:48:12.418 o.a.s.util [WARN] Worker Process 46911423-af41-4db3-8480-7a95df96632a:main : command provided worker
2017-01-17 15:48:12.426 o.a.s.util [WARN] Worker Process 46911423-af41-4db3-8480-7a95df96632a:main : user is xyz-admin
2017-01-17 15:48:12.426 o.a.s.util [WARN] Worker Process 46911423-af41-4db3-8480-7a95df96632a:Failure to exec app initialization process - No such file or directory
{code}",Surprise,-1
STORM-2400,"This issue is reported to Curator with CURATOR-358. 

org.apache.curator.framework.recipes.leader.LeaderLatch#getLeader() throws KeeperException with Code#NONODE intermittently as mentioned in the stack trace below. It may be possible participant's ephemeral ZK node is removed because its connection/session is closed.

You can see the below code at https://github.com/apache/curator/blob/master/curator-recipes/src/main/java/org/apache/curator/framework/recipes/leader/LeaderLatch.java#L451

{code}
public Participant getLeader() throws Exception
{ 
  Collection<String> participantNodes = LockInternals.getParticipantNodes(client, latchPath, LOCK_NAME, sorter); 
  return LeaderSelector.getLeader(client, participantNodes); 
}
{code}

I guess it hits a race condition where a participant node is retrieved but when it invokes LeaderSelector#getLeader() it would have been removed because of session timeout and it throws KeeperException with NoNode code. It does not retry as the RetryLoop retries only for connection/session timeouts. But in this case, NoNode should have been retried. I could not find any APIs on CuratorClient to configure the kind of KeeperException codes to be retried. It may be good to have a way to take what kind of errors should be retried in org.apache.curator.framework.CuratorFrameworkFactory.Builder APIs.
Intermittent Exception found with the stack trace:

{noformat}
2016-11-15 06:09:33.954 o.a.s.d.nimbus [ERROR] Error when processing event
org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock/_c_97c09eed-5bba-4ac8-a05f-abdc4e8e95cf-latch-0000000002
at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)
at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)
at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)
at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)
at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)
at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)
at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)
at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)
at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:454)
{noformat}","Sadness, Surprise",-1
STORM-2440,"During two somewhat extended outages of our Kafka cluster, we experienced a problem with our Storm topologies consuming data from that Kafka cluster.

Almost all our topologies just silently stopped processing data from some of the topics/partitions, an the only way to fix this situation was to restart those topologies.

I tracked down one occurrence of the failure to this worker, which was running one the KafkaSpouts:

{noformat}
2017-03-18 04:06:15.389 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
2017-03-18 04:06:15.389 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
        at org.apache.storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:213) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:189) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:15.390 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Refreshing partition manager connections
2017-03-18 04:06:15.395 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=tagging_log, partitionMap={0=kafka-03:9092, 1=kafka-12:9092,
 2=kafka-08:9092, 3=kafka-05:9092}}
2017-03-18 04:06:15.395 o.a.s.k.KafkaUtils [INFO] Task [1/1] assigned [Partition{host=kafka-03:9092, topic=tagging_log, partition=0}, Partition{host=kafka-12:9092, topic=tagging_log, partit
ion=1}, Partition{host=kafka-08:9092, topic=tagging_log, partition=2}, Partition{host=kafka-05:9092, topic=tagging_log, partition=3}]
2017-03-18 04:06:15.395 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Deleted partition managers: [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.396 o.a.s.k.ZkCoordinator [INFO] Task [1/1] New partition managers: [Partition{host=kafka-12:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.398 o.a.s.k.PartitionManager [INFO] Read partition information from: /log_processing/tagging/kafka-tagging-spout/partition_1  --> {""partition"":1,""off
set"":40567174332,""topology"":{""name"":""tagging-aerospike-1"",""id"":""tagging-aerospike-1-3-1489587827""},""topic"":""tagging_log"",""broker"":{""port"":9092,""host"":""kafka-08""}}
2017-03-18 04:06:25.408 k.c.SimpleConsumer [INFO] Reconnect due to error:
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:86) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:35.416 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.419 o.a.s.d.executor [ERROR] 
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.442 o.a.s.d.executor [INFO] Got interrupted excpetion shutting thread down...
{noformat}

There were no more outputs in the log after that until the toplogy was manually killed.

As you can see the {{java.net.SocketTimeoutException}} escapes the storm-kafka code (probably a problem in and of itself), but the worker is not killed. The thread that calls the {{.nextTuple}} method of the spout is exited on the other hand.
This is the culprit line: https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L270

I see that this has been fixed in the Java port of the executor code by explicitly excluding {{java.net.SocketTimeoutException}} from the condition.
I will open a pull request with a backport tomorrow.",Surprise,-1
STORM-2443,"Here's stacktrace from Nimbus log:

{code}
2017-03-30 16:53:26.954 o.a.s.d.n.Nimbus pool-14-thread-56 [WARN] set log config topology exception. (topology id='rolling-1-1490860365')
java.lang.NullPointerException: null
        at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]
{code}",,
STORM-2496,"When we submit topology via specific user with dependency artifacts, submitter uploads artifacts to the blobstore with user which runs the submission.

Since uploaded artifacts are uploaded once and shared globally, other user might need to use uploaded artifact. (This is completely fine for non-secured cluster.) In this case, Supervisor fails to get artifact and crashes in result.

{code}
2017-04-28 04:56:46.594 o.a.s.l.AsyncLocalizer Async Localizer [WARN] Caught Exception While Downloading (rethrowing)...
org.apache.storm.generated.AuthorizationException: null
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing event
java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_112]
	at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
Caused by: org.apache.storm.generated.AuthorizationException
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
	at org.apache.storm.utils.Utils.exitProcess(Utils.java:1774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
2017-04-28 04:56:46.599 o.a.s.d.s.Supervisor Thread-7 [INFO] Shutting down supervisor 775c158b-0a2d-40be-9e02-a9662d8bc5c4
{code}

So we need to upload artifacts with READ permission to all, or at least supervisor should be able to read them at all.",,
STORM-2518,"While adding ACL to USER from uploading artifacts, ""name"" field is actually optional for thrift specification, but Nimbus reads the value without checking null while fixing ACL.

{code}
2017-05-16 14:57:02.527 o.a.s.t.s.TThreadPoolServer pool-45-thread-136 [ERROR] Error occurred during processing of message.
java.lang.NullPointerException: null
        at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}

Uploading artifacts fails and topology submission also fails.",Surprise,-1
STORM-2568,"Hello

I've tried to use storm-kafka-monitor, and it works fine on command line If I changed 'toollib/storm-kafka-monitor-*.jar' to 'toollib/storm-kafka-monitor-1.1.0.jar'.

{code}
{""my-kafka-topic-name"":{""0"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805483, ""lag"": 485},""1"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805485, ""lag"": 487},""2"":{""consumerCommittedOffset"": 74804995, ""logHeadOffset"": 74805485, ""lag"": 490},""3"":{""consumerCommittedOffset"": 74805001, ""logHeadOffset"": 74805488, ""lag"": 487},""4"":{""consumerCommittedOffset"": 74805011, ""logHeadOffset"": 74805484, ""lag"": 473},""5"":{""consumerCommittedOffset"": 74805009, ""logHeadOffset"": 74805485, ""lag"": 476},""6"":{""consumerCommittedOffset"": 74805008, ""logHeadOffset"": 74805483, ""lag"": 475},""7"":{""consumerCommittedOffset"": 74805010, ""logHeadOffset"": 74805484, ""lag"": 474},""8"":{""consumerCommittedOffset"": 73641446, ""logHeadOffset"": 74805488, ""lag"": 1164042},""9"":{""consumerCommittedOffset"": 73641448, ""logHeadOffset"": 74805489, ""lag"": 1164041},""10"":{""consumerCommittedOffset"": 73641443, ""logHeadOffset"": 74805483, ""lag"": 1164040},""11"":{""consumerCommittedOffset"": 73641445, ""logHeadOffset"": 74805487, ""lag"": 1164042},""12"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805486, ""lag"": 483},""13"":{""consumerCommittedOffset"": 74804999, ""logHeadOffset"": 74805482, ""lag"": 483},""14"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805483, ""lag"": 481},""15"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805484, ""lag"": 482},""16"":{""consumerCommittedOffset"": 74804994, ""logHeadOffset"": 74805482, ""lag"": 488},""17"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805489, ""lag"": 487},""18"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805488, ""lag"": 485},""19"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805489, ""lag"": 486}}}
{code}

but it gives empty result when I call below api.

{code}
/api/v1/topology/:id/lag
...

{
""MySpoutName"": {
""spoutLagResult"": {},
""spoutId"": ""MySpoutName"",
""spoutType"": ""KAFKA""
}
}
{code}

-I think that needs to fix ""groupid"" to ""group.id"" in TopologySpoutLag.java I debug it, but groupid is right.-

the reason was topics has square brackets in command. 



{code}
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] json configuration: {config.security.protocol=null, config.bootstrap.servers=kafka.xxx.com:9092, config.topics=[my-kafka-topic], config.groupid=my-storm-kafka-spout-groupid, topology.tasks=5}

2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] /my/program/storm/bin/storm-kafka-monitor
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -t
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] [my-kafka-topic]
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -g
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] my-storm-kafka-spout-groupid
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -b
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] kafka.xxx.com:9092
{code}

the square brackets automatically added because of this
{code}
package org.apache.storm.kafka.spout;
public class NamedSubscription extends Subscription {
...
    @Override
    public String getTopicsString() {
        return String.valueOf(topics);
    }
{code}
topics is Collections. so String.valueOf returns value with square brackets.

I fixed the code that remove square brackets in TopologySpoutLag.java for my case. 
but I think that fixing 'getTopicsString of NamedSubscription.java in org.apache.storm.kafka.spout' is might be better. 
",Surprise,-1
STORM-2682,"When supervisor is started, it dies after about 30s like so:

{code:java}
...
2017-08-07 17:12:04.606 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6701 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6702 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.l.AsyncLocalizer main [INFO] Cleaning up unused topologies in /home/storm/data/supervisor/stormdist
2017-08-07 17:12:04.617 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 65a0f977-474c-4938-a4f5-bc99939e96ff at host 192.168.10.
21.
2017-08-07 17:12:04.619 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPrep
arableReporter
2017-08-07 17:12:04.620 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2017-08-07 17:12:04.624 o.a.s.m.StormMetricsRegistry main [INFO] Started statistics report plugin...
2017-08-07 17:12:34.620 o.a.s.e.EventManagerImp Thread-4 [ERROR] {} Error when processing event
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]
        at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.620 o.a.s.u.Utils Thread-4 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.631 o.a.s.d.s.Supervisor Thread-5 [INFO] Shutting down supervisor 65a0f977-474c-4938-a4f5-bc99939e96ff
{code}",,
STORM-2700,"When 
{code:java}
storm.blobstore.acl.validation.enabled: false
{code}
is set, blobstore still checks ACL.


{code:java}
2017-08-21 13:56:19.800 o.a.s.d.s.Slot SLOT_6702 [ERROR] Error when processing event
java.util.concurrent.ExecutionException: AuthorizationException(msg:ethan does not have READ access to key1)
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_131]
        at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_131]
        at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:410) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:305) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:789) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
Caused by: org.apache.storm.generated.AuthorizationException
        at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:527) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.localizer.Localizer.access$000(Localizer.java:68) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:497) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:473) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-08-21 13:56:19.800 o.a.s.u.Utils SLOT_6702 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:437) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:823) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
2017-08-21 13:56:19.802 o.a.s.d.s.Supervisor Thread-6 [INFO] Shutting down supervisor b350cfb4-b333-4ea5-965e-b0698aaea80f-10.88.214.182
2017-08-21 13:56:19.803 o.a.s.e.EventManagerImp Thread-5 [INFO] Event manager interrupted
{code}


Reproduce:

1. Create a blobstore with permission set to one user (e.g mapredqa).
{code:java}
sudo -u mapredqa storm blobstore create --file test-blobstore.txt --acl u:mapredqa:rwa key1
{code}

2. Submit a topology with topology.blobstore.map config as someone else (e.g. ethan).
{code:java}
sudo -u ethan storm jar /tmp/storm-starter-2.0.0-SNAPSHOT.jar org.apache.storm.starter.WordCountTopology wc -c topology.blobstore.map='{""key1"":{""localname"":""test-blobstore.txt"", ""uncompress"":false}}'
{code}
",Surprise,-1
STORM-2736,"Sometimes, after our topologies have been running for a while, Zookeeper does not respond within an appropriate time and we see
{code}
2017-08-16 10:18:38.859 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal lost leadership.
2017-08-16 10:21:31.144 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal gained leadership, checking if it has all the topology code locally.
2017-08-16 10:21:46.201 o.a.s.zookeeper [INFO] Accepting leadership, all active topology found localy.
{code}

That's fine, and we probably need to allocate more resources. But after a new leader is chosen, we then see:
{code}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}
over and over.

I can't figure out yet how to cause the conditions that lead to Zookeeper becoming unresponsive, but it is possible to reproduce the {{BlobStoreUtils}} error by restarting Zookeeper.

The problem, I think, is that the loop [here|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L175] never executes because the {{nimbusInfos}} list is empty. If I add a check similar to [this|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L244] for a node which exists but has no children, the error goes away.",Sadness,-1
STORM-2829,"
{code:java}
2017-11-21 21:06:19.369 o.e.j.s.HttpChannel qtp1471948789-17 [WARN] /api/v1/deepSearch/wc-1-1511188542
javax.servlet.ServletException: java.lang.RuntimeException: com.fasterxml.jackson.databind.JsonMappingException: Direct self-reference leading to cycle (through reference chain: org.apache.storm.daemon.logviewer.handler.Matched[""matches""]->java.util.ArrayList[0]->java.util.HashMap[""port""]->sun.nio.fs.UnixPath[""fileSystem""]->sun.nio.fs.LinuxFileSystem[""rootDirectories""]->sun.nio.fs.UnixPath[""root""])
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:489) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
{code}
",,
STORM-2873,"The backpressure implementation deletes the znode when not relevant but that hits zookeeper issue of too frequent deletion and creation or same path for ephemeral znode. Below is exception we get when zk hits this issue:

{code}
017-09-18 15:00:34.980 b.s.util WorkerBackpressureThread [WARN] Expecting exception of class: class org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException, but exception chain only contains: (#<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721>)
2017-09-18 15:00:34.980 b.s.d.worker WorkerBackpressureThread [ERROR] workerBackpressure update failed when connecting to ZK ... will retry
java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721
	at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at backtype.storm.zookeeper$delete_node.doInvoke(zookeeper.clj:110) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at clojure.lang.RestFn.invoke(RestFn.java:464) ~[clojure-1.6.0.jar:?]
	at backtype.storm.zookeeper$delete_recursive.invoke(zookeeper.clj:189) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4207.delete_node(zookeeper_state_factory.clj:117) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at sun.reflect.GeneratedMethodAccessor860.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
	at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__4254.delete_node(pacemaker_state_factory.clj:174) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at sun.reflect.GeneratedMethodAccessor859.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
	at backtype.storm.cluster$mk_storm_cluster_state$reify__3873.worker_backpressure_BANG_(cluster.clj:421) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at sun.reflect.GeneratedMethodAccessor857.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
	at backtype.storm.daemon.worker$mk_backpressure_handler$fn__7117.invoke(worker.clj:161) [storm-core-0.10.2.y.jar:0.10.2.y]
	at backtype.storm.disruptor$worker_backpressure_handler$reify__6432.onEvent(disruptor.clj:57) [storm-core-0.10.2.y.jar:0.10.2.y]
	at backtype.storm.utils.WorkerBackpressureThread.run(WorkerBackpressureThread.java:64) [storm-core-0.10.2.y.jar:0.10.2.y]
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:113) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	at backtype.storm.zookeeper$delete_node.doInvoke(zookeeper.clj:107) ~[storm-core-0.10.2.y.jar:0.10.2.y]
	... 23 more
{code}",Surprise,-1
STORM-2986,"So I set
{code:java}
logviewer.cleanup.interval.secs: 10
{code}
to start LogCleaner thread. But from logviewer.log:


{code:java}
2018-03-05 21:31:17.629 o.a.s.v.ConfigValidation main [WARN] storm.messaging.netty.max_retries is a deprecated config please see class org.apache.storm.Config.STORM_MESSAGING_NETTY_MAX_RETRIES for more information.
2018-03-05 21:31:17.650 o.a.s.d.l.LogviewerServer main [INFO] Starting Logviewer HTTP servers...
2018-03-05 21:31:17.684 o.e.j.u.log main [INFO] Logging initialized @2455ms to org.eclipse.jetty.util.log.Slf4jLog
2018-03-05 21:31:17.877 o.a.s.d.l.u.LogCleaner main [INFO] configured max total size of worker logs: 2 MB, max total size of worker logs per directory: 1 MB
2018-03-05 21:31:18.017 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-03-05 21:31:18.022 o.a.s.d.l.u.LogCleaner logviewer-cleanup [ERROR] Exception while cleaning up old log.
java.lang.NullPointerException: null
at java.util.Arrays.stream(Arrays.java:5004) ~[?:1.8.0_131]
at org.apache.storm.daemon.logviewer.utils.LogCleaner.selectDirsForCleanup(LogCleaner.java:217) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.daemon.logviewer.utils.LogCleaner.run(LogCleaner.java:135) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
2018-03-05 21:31:18.024 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2018-03-05 21:31:18.031 o.a.s.m.StormMetricsRegistry main [INFO] Started statistics report plugin...
2018-03-05 21:31:18.031 o.a.s.d.l.LogviewerServer main [INFO] Starting Logviewer...
2018-03-05 21:31:18.041 o.e.j.s.Server main [INFO] jetty-9.4.7.v20170914
2018-03-05 21:31:18.215 o.a.h.s.a.s.KerberosAuthenticationHandler main [INFO] Login using keytab /keytabs/HTTP.keytab, for principal HTTP/persistmist.corp.ne1.yahoo.com
2018-03-05 21:31:20.832 o.h.v.i.u.Version main [INFO] HV000001: Hibernate Validator 5.3.4.Final
2018-03-05 21:31:21.215 o.e.j.s.h.ContextHandler main [INFO] Started o.e.j.s.ServletContextHandler@65bb9029{/,file:///tmp/apache-storm-2.0.0-SNAPSHOT/public/,AVAILABLE}
2018-03-05 21:31:21.287 o.e.j.s.AbstractConnector main [INFO] Started ServerConnector@30506c0d{HTTP/1.1,[http/1.1]}{0.0.0.0:8000}
2018-03-05 21:31:21.288 o.e.j.s.Server main [INFO] Started @6060ms
2018-03-05 21:31:28.038 o.a.s.d.l.u.LogCleaner logviewer-cleanup [ERROR] Exception while cleaning up old log.
java.lang.NullPointerException: null
at java.util.Arrays.stream(Arrays.java:5004) ~[?:1.8.0_131]
at org.apache.storm.daemon.logviewer.utils.LogCleaner.selectDirsForCleanup(LogCleaner.java:217) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.daemon.logviewer.utils.LogCleaner.run(LogCleaner.java:135) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]{code}


It's because there is no workers-artifacts directory at the very beginning before submitting any topologies. Userscan fix it bymanually creating the directory. But it's better to have it solve fixed.",Surprise,-1
STORM-2988,"As per documentation, I configured metrics v2 in my storm.yaml using the following configuration:

{code:yaml}
storm.metrics.reporters:

  - class: ""org.apache.storm.metrics2.reporters.JmxStormReporter""
    daemons:
        - ""supervisor""
        - ""nimbus""
        - ""worker""
    report.period: 10
    report.period.units: ""SECONDS""
{code}

When I start nimbus and supervisors everything works properly, I can see metrics reported to JMX, and logs (for nimbus in this example) report:

{code}
2018-03-07 15:35:22.201 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-03-07 15:35:22.203 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2018-03-07 15:35:22.221 o.a.s.d.common main [INFO] Started statistics report plugin...
{code}

When I submit a topology, workers cannot initialize and report this error

{code:java}
2018-03-07 15:39:19.136 o.a.s.d.worker main [INFO] Launching worker for stp_topology-1-1520433551 on [... cut ...]
2018-03-07 15:39:19.169 o.a.s.m.StormMetricRegistry main [INFO] Starting metrics reporters...
2018-03-07 15:39:19.172 o.a.s.m.StormMetricRegistry main [INFO] Attempting to instantiate reporter class: org.apache.storm.metrics2.reporters.JmxStormReporter
2018-03-07 15:39:19.175 o.a.s.m.r.JmxStormReporter main [INFO] Preparing...
2018-03-07 15:39:19.182 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker
java.lang.IllegalArgumentException: Don't know how to convert {""class"" ""org.apache.storm.metrics2.reporters.JmxStormReporter"", ""daemons"" [""supervisor"" ""nimbus"" ""worker""], ""report.period"" 10, ""report.period.units"" ""SECONDS""} + to String
	at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.daemon.worker$fn__5545$exec_fn__1369__auto____5546.invoke(worker.clj:611) ~[storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
2018-03-07 15:39:19.195 o.a.s.util main [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
{code}

Looking at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain() I found that it passes ""reporterConf"" map to Utils.getString() instead of a string:
{code:java}
    public static String getMetricsJMXDomain(Map reporterConf) {
        return Utils.getString(reporterConf, JMX_DOMAIN);
}
{code}

The ""prepare"" method in org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter used by nimbus and supervisor correctly passes a string to Utils.getString():

{code:java}
public void prepare(MetricRegistry metricsRegistry, Map stormConf) {
        LOG.info(""Preparing..."");
        JmxReporter.Builder builder = JmxReporter.forRegistry(metricsRegistry);
        String domain = Utils.getString(stormConf.get(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_DOMAIN), null);
        if (domain != null) {
            builder.inDomain(domain);
}
[...]
{code}

Is this a bug or am I missing something in configuration?

Regards,
Federico Chiacchiaretta","Joy, Love, Surprise","1, -1"
STORM-3012,"Below is the nimbus.log when I restarted pacemaker. Nimbus crashed because of NPE.




{code:java}
2018-03-26 21:39:18.404 main o.a.s.z.LeaderElectorImp [INFO] Queued up for leader lock.
2018-03-26 21:39:18.458 main o.a.s.d.m.MetricsUtils [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableRepor
ter
2018-03-26 21:39:18.461 main o.a.s.d.m.r.JmxPreparableReporter [INFO] Preparing...
2018-03-26 21:39:18.527 main o.a.s.m.StormMetricsRegistry [INFO] Started statistics report plugin...
2018-03-26 21:39:18.710 main o.a.s.m.n.Login [INFO] successfully logged in.
2018-03-26 21:39:18.738 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh thread started.
2018-03-26 21:39:18.739 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-03-26 21:39:18.739 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-03-26 21:39:18.747 Refresh-TGT o.a.s.m.n.Login [INFO] TGT valid starting at:        Mon Mar 26 21:39:18 UTC 2018
2018-03-26 21:39:18.747 Refresh-TGT o.a.s.m.n.Login [INFO] TGT expires:                  Tue Mar 27 21:39:18 UTC 2018
2018-03-26 21:39:18.747 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh sleeping until: Tue Mar 27 17:39:22 UTC 2018
2018-03-26 21:39:18.756 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181 sessionTimeout
=60000 watcher=org.apache.curator.ConnectionState@148c7c4b
2018-03-26 21:39:18.807 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-03-26 21:39:18.814 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mec
hanism.
2018-03-26 21:39:18.815 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74b
lue-gw.blue.ygrid.yahoo.com/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-03-26 21:39:18.816 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue
-gw.blue.ygrid.yahoo.com/10.215.68.156:2181, initiating session
2018-03-26 21:39:18.817 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server open
qe74blue-gw.blue.ygrid.yahoo.com/10.215.68.156:2181, sessionid = 0x1624f6d49dd0cdd, negotiated timeout = 40000
2018-03-26 21:39:18.818 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-03-26 21:39:18.839 Curator-Framework-0 o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2018-03-26 21:39:18.841 main o.a.z.ZooKeeper [INFO] Session: 0x1624f6d49dd0cdd closed
2018-03-26 21:39:18.842 main-EventThread o.a.z.ClientCnxn [INFO] EventThread shut down
2018-03-26 21:39:18.844 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-03-26 21:39:18.844 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-03-26 21:39:18.875 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181/storm_ystormQE
_CI sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@211febf3
2018-03-26 21:39:18.908 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mec
hanism.
2018-03-26 21:39:18.909 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74b
lue-gw.blue.ygrid.yahoo.com/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-03-26 21:39:18.910 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue
-gw.blue.ygrid.yahoo.com/10.215.68.156:2181, initiating session
2018-03-26 21:39:18.911 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server open
qe74blue-gw.blue.ygrid.yahoo.com/10.215.68.156:2181, sessionid = 0x1624f6d49dd0cde, negotiated timeout = 40000
2018-03-26 21:39:18.920 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-03-26 21:39:18.923 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-03-26 21:39:18.986 main o.a.s.d.n.Nimbus [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-03-26 21:39:19.931 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-blobs [] local-topology-blobs [] diff-topology-blobs []
2018-03-26 21:39:19.932 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-dependencies [] local-blobs [] diff-topology-dependencies []
2018-03-26 21:39:19.932 main-EventThread o.a.s.z.Zookeeper [INFO] Accepting leadership, all active topologies and corresponding dependencies found local
ly.
2018-03-26 21:39:20.636 timer o.a.s.d.n.Nimbus [INFO] Scheduling took 1381 ms for 0 topologies
2018-03-26 21:39:20.901 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: open
qe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:20.901 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 101ms (NOT MAX)
2018-03-26 21:39:21.003 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: open
qe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.003 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 102ms (NOT MAX)
2018-03-26 21:39:21.106 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.106 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 106ms (NOT MAX)
2018-03-26 21:39:21.214 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.214 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 115ms (NOT MAX)
2018-03-26 21:39:21.331 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.331 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 129ms (NOT MAX)
2018-03-26 21:39:21.462 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.462 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 162ms (NOT MAX)
2018-03-26 21:39:21.626 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.626 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 176ms (NOT MAX)
2018-03-26 21:39:21.807 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:21.807 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 319ms (NOT MAX)
2018-03-26 21:39:21.888 timer o.a.s.p.PacemakerClient [ERROR] error attempting to write to a channel {}
org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.
        at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:22.128 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:22.128 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 603ms (NOT MAX)
2018-03-26 21:39:22.733 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:22.733 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 868ms (NOT MAX)
2018-03-26 21:39:22.888 timer o.a.s.p.PacemakerClient [ERROR] error attempting to write to a channel {}
org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.
        at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:23.603 client-boss-1 o.a.s.p.PacemakerClientHandler [WARN] Connection to pacemaker failed. Trying to reconnect Connection refused: openqe74blue-n1.blue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:23.603 client-boss-1 o.a.s.u.StormBoundedExponentialBackoffRetry [WARN] WILL SLEEP FOR 1494ms (NOT MAX)
2018-03-26 21:39:23.888 timer o.a.s.p.PacemakerClient [ERROR] error attempting to write to a channel {}
org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.
        at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:24.889 timer o.a.s.p.PacemakerClient [ERROR] error attempting to write to a channel {}
org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.
        at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:25.100 client-worker-4 o.a.s.m.n.KerberosSaslClientHandler [INFO] Connection established from /10.215.76.240:36922 to openqe74blue-n1.b        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:25.100 client-worker-4 o.a.s.m.n.KerberosSaslClientHandler [INFO] Connection established from /10.215.76.240:36922 to openqe74blue-n1.b
lue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:25.107 client-worker-4 o.a.s.m.n.KerberosSaslNettyClient [INFO] Creating Kerberos Client.
2018-03-26 21:39:25.116 client-worker-4 o.a.s.m.n.Login [INFO] successfully logged in.
2018-03-26 21:39:25.121 client-worker-4 o.a.s.m.n.KerberosSaslNettyClient [INFO] Got Client: com.sun.security.sasl.gsskerb.GssKrb5Client@116ce525
2018-03-26 21:39:25.753 client-worker-1 o.a.s.m.n.KerberosSaslClientHandler [INFO] Connection established from /10.215.76.240:37614 to openqe74blue-n2.blue.ygrid.yahoo.com/10.215.76.243:6699
2018-03-26 21:39:25.753 client-worker-1 o.a.s.m.n.KerberosSaslNettyClient [INFO] Creating Kerberos Client.
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:24.889 timer o.a.s.p.PacemakerClient [ERROR] error attempting to write to a channel {}
org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.
        at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:197) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:25.100 client-worker-4 o.a.s.m.n.KerberosSaslClientHandler [INFO] Connection established from /10.215.76.240:36922 to openqe74blue-n1.b
lue.ygrid.yahoo.com/10.215.76.240:6699
2018-03-26 21:39:25.107 client-worker-4 o.a.s.m.n.KerberosSaslNettyClient [INFO] Creating Kerberos Client.
2018-03-26 21:39:25.116 client-worker-4 o.a.s.m.n.Login [INFO] successfully logged in.
2018-03-26 21:39:25.121 client-worker-4 o.a.s.m.n.KerberosSaslNettyClient [INFO] Got Client: com.sun.security.sasl.gsskerb.GssKrb5Client@116ce525
2018-03-26 21:39:25.753 client-worker-1 o.a.s.m.n.KerberosSaslClientHandler [INFO] Connection established from /10.215.76.240:37614 to openqe74blue-n2.b
lue.ygrid.yahoo.com/10.215.76.243:6699
2018-03-26 21:39:25.753 client-worker-1 o.a.s.m.n.KerberosSaslNettyClient [INFO] Creating Kerberos Client.
2018-03-26 21:39:25.763 client-worker-1 o.a.s.m.n.Login [INFO] successfully logged in.
2018-03-26 21:39:25.765 client-worker-1 o.a.s.m.n.KerberosSaslNettyClient [INFO] Got Client: com.sun.security.sasl.gsskerb.GssKrb5Client@493cfe64
2018-03-26 21:39:26.596 timer o.a.s.d.n.Nimbus [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2508) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.NullPointerException
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:195) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-03-26 21:39:26.596 timer o.a.s.u.Utils [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$23(Nimbus.java:1154) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:106) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-03-26 21:39:26.600 Thread-16 o.a.s.u.Utils [INFO] Halting after 5 seconds
2018-03-26 21:39:26.606 Thread-15 o.a.s.d.n.Nimbus [INFO] Shutting down master
2018-03-26 21:39:31.600 Thread-16 o.a.s.u.Utils [WARN] Forcing Halt...
{code}




This is because when [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java#L195-L198]happens,


{code:java}
HBMessage ret = messages[next];
if(ret == null) {
// This can happen if we lost the connection and subsequently reconnected or timed out.
send(m);
}
messages[next] = null;
LOG.debug(""Got Response: {}"", ret);
return ret;
{code}
it returns null result. And the null result is inserted into [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientPool.java#L65-L66]
{code:java}
for(String s : servers) {
HBMessage response = getClientForServer(s).send(m);
responses.add(response);
}
{code}
which leads to [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java#L195]


{code:java}
for(HBMessage response : responses) {
if (response.get_type() != HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE) {
LOG.error(""get_worker_hb_children: Invalid Response Type"");
continue;
}
if(response.get_data().get_nodes().get_pulseIds() != null) {
retSet.addAll(response.get_data().get_nodes().get_pulseIds());
}
}
{code}


and this is where NPEhappens",,
STORM-3013,"Hi, I have deactivated the storm topology & then if I produce any records into Kafka, Storm throws an exception. Exception follows,
{code:java}
2018-03-28 09:50:23.804 o.a.s.d.executor Thread-83-kafkaLogs-executor[130 130] [INFO] Deactivating spout kafkaLogs:(130)
2018-03-28 09:51:01.289 o.a.s.util Thread-17-kafkaLogs-executor[139 139] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:634) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
Caused by: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1622) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$metrics_tick$fn__4899.invoke(executor.clj:345) ~[storm-core-1.2.1.jar:1.2.1]
at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?]
at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]
at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]
at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]
at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$tuple_action_fn__4981.invoke(executor.clj:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:471) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
... 7 more
{code}",,
STORM-3073,"Saw this while running the https://github.com/apache/storm/blob/master/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/ThroughputVsLatency.java topology.

{code}
2018-05-15 11:35:28.365 o.a.s.u.Utils Thread-16-spout-executor[8, 8] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: Queue full
	at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.lang.IllegalStateException: Queue full
	at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]
	at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 7 more
{code}

The executor's pendingEmits queue is full, and the executor then tries to add another tuple. It looks to me like we're preventing the queue from filling by emptying it between calls to nextTuple at https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java#L184.

The TVL topology reemits failed tuples directly from the fail method, which can be triggered by tick tuples. If the pendingEmits queue is already close to full when this happens, we might hit the error above. I think it can also happen if nextTuple emits too many tuples in a call, or if too many metrics ticks happen between pendingEmit flushes, since metrics ticks also trigger emits.",Fear,-1
STORM-3075,"{code:java}
2018-05-15 14:14:59.873 o.a.c.f.l.ListenerContainer main-EventThread [ERROR] Listener (org.apache.storm.zookeeper.Zookeeper$1@26d820eb) threw an exception
java.lang.NullPointerException: null
        at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:118) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:124) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) ~[curator-client-4.0.1.jar:?]
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]

{code}",,
STORM-3082,"[~aniket.alhat] reported on the mailing list that he got an NPE when trying to start the Trident spout.

{code}
2018-05-22 06:23:02.318 o.a.s.util [ERROR] Async loop died!
java.lang.NullPointerException: null
        at org.apache.storm.kafka.spout.NamedTopicFilter.getFilteredTopicPartitions(NamedTopicFilter.java:57) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.refreshAssignment(ManualPartitionSubscription.java:54) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.subscribe(ManualPartitionSubscription.java:49) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutManager.createAndSubscribeKafkaConsumer(KafkaTridentSpoutManager.java:59) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:84) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:100) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutOpaque.getEmitter(KafkaTridentSpoutOpaque.java:50) ~[stormjar.jar:?]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.<init>(OpaquePartitionedTridentSpoutExecutor.java:97) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:221) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:39) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.TridentSpoutExecutor.prepare(TridentSpoutExecutor.java:60) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.topology.TridentBoltExecutor.prepare(TridentBoltExecutor.java:245) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.daemon.executor$fn__5043$fn__5056.invoke(executor.clj:803) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.2.1.jar:1.2.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]
{code}

It looks to me like the partitionsFor method on the consumer will return null if the specified topic doesn't exist. We didn't account for this in the filter, because the return type of the method is a List, and we assumed it wouldn't be null.

I think it's reasonable that people should be able to subscribe to topics that don't exist yet, and the spout should pick up the new topics eventually.

We should check for null here https://github.com/apache/storm/blob/93ed601425a79759c0189a945c6b46266e5c9ced/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/NamedTopicFilter.java#L55, and maybe log a warning if the returned value is null.",,
STORM-3084,"{code:java}
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y' 2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event java.lang.RuntimeException: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y] Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y] ... 2 more 2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event java.lang.RuntimeException: Halting process: Error while processing event at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y] 2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master 2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}",,
STORM-3096,"STORM-3053 attempted to fix the race condition where a nimbus timer causes doCleanup() to delete the blobs during topology submission. After the fix went in, we still see the error occurring. I tracked the problem down toidsOfTopologiesWithPrivateWorkerKeys() at [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L893.]



The previous change to wait to delete topologies is useful, but should be moved after all the topologies are discovered.


{code:java}

 018-06-03 11:53:42.581 o.a.s.d.n.Nimbus pool-37-thread-1014 [INFO] Received topology submission for topology-testHardCoreFaultTolerance-4 (storm-0.10.2.y.248 JDK-1.8.0_131) with conf {topology.users=[hadoopqa@DEV.YGRID.YAHOO.COM, hadoopqa], topology.acker.executors=0, storm.zookeeper.superACL=sasl:gstorm, topology.workers=3, topology.submitter.principal=hadoopqa@DEV.YGRID.YAHOO.COM, topology.debug=true, topology.disable.loadaware.messaging=true, storm.zookeeper.topology.auth.payload=#########################################, topology.name=topology-testHardCoreFaultTolerance-4, storm.zookeeper.topology.auth.scheme=digest, topology.kryo.register={}, nimbus.task.timeout.secs=200, storm.id=topology-testHardCoreFaultTolerance-4-18-1528026822, topology.kryo.decorators=[], topology.eventlogger.executors=0, topology.submitter.user=hadoopqa, topology.max.task.parallelism=null}
 2018-06-03 11:53:42.591 o.a.s.d.n.Nimbus timer [INFO] Cleaning up topology-testHardCoreFaultTolerance-4-18-1528026822
 2018-06-03 11:53:42.597 o.a.s.d.n.Nimbus pool-37-thread-1014 [INFO] uploadedJar /home/y/var/storm/nimbus/inbox/stormjar-3c73de98-ced7-4fd0-86d9-8fba3e5100f1.jar
 2018-06-03 11:53:42.601 o.a.s.c.StormClusterStateImpl pool-37-thread-1014 [INFO] set-path: /blobstore/topology-testHardCoreFaultTolerance-4-18-1528026822-stormjar.jar/openqe82blue-n1.blue.ygrid.yahoo.com:50560-1
 2018-06-03 11:53:42.621 o.a.s.d.n.Nimbus timer [INFO] Exception {}
 org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser
 at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:394) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:680) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2389) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2443) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2730) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.StormTimer$1.run(StormTimer.java:111) [storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) [storm-client-2.0.0.y.jar:2.0.0.y]
 2018-06-03 11:53:42.871 o.a.s.c.StormClusterStateImpl pool-37-thread-1014 [INFO] set-path: /blobstore/topology-testHardCoreFaultTolerance-4-18-1528026822-stormconf.ser/openqe82blue-n1.blue.ygrid.yahoo.com:50560-1
 2018-06-03 11:53:42.881 o.a.s.c.StormClusterStateImpl pool-37-thread-1014 [INFO] set-path: /blobstore/topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser/openqe82blue-n1.blue.ygrid.yahoo.com:50560-1
 2018-06-03 11:53:42.886 o.a.s.d.n.Nimbus pool-37-thread-1023 [INFO] Created download session dd7fa916-e489-47a5-beea-ac3eba6ed905 for topology-testHardCoreFaultTolerance-0-14-1528026818-stormjar.jar
 2018-06-03 11:53:42.888 o.a.s.d.n.Nimbus pool-37-thread-1014 [WARN] Topology submission exception. (topology name='topology-testHardCoreFaultTolerance-4')
 org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormjar.jar
 at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:423) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1499) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.waitForDesiredCodeReplication(Nimbus.java:1509) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2982) [storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) [libthrift-0.11.0.jar:0.11.0]
 at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.11.0.jar:0.11.0]
 at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [libthrift-0.11.0.jar:0.11.0]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
 2018-06-03 11:53:42.888 o.a.t.ProcessFunction pool-37-thread-1014 [ERROR] Internal error processing submitTopologyWithOpts
 org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormjar.jar
 at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:423) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1499) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.waitForDesiredCodeReplication(Nimbus.java:1509) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2982) ~[storm-server-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) ~[storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) ~[storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) [libthrift-0.11.0.jar:0.11.0]
 at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.11.0.jar:0.11.0]
 at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
 at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [libthrift-0.11.0.jar:0.11.0]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]{code}",Sadness,-1
STORM-3103,"When debugging an Nimbus NPE that caused restarts, I noticed that a forced halt occurred:


{code:java}
2018-05-24 09:27:05.569 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Opening socket connection to server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-05-24 09:27:05.570 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Socket connection established to openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, initiating session
2018-05-24 09:27:05.571 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Session establishment complete on server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, sessionid = 0x1624a86300f7f6b, negotiated timeout = 40000
2018-05-24 09:27:05.571 o.a.c.f.s.ConnectionStateManager main-EventThread [INFO] State change: CONNECTED
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master
2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}
At times this would cause leadership confusion:


{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
We should endeavor to shutdown cleanly.







",Surprise,-1
STORM-3117,"The following test pseudo-code causes issues:
{code:java}
cluster.submitTopology(cluster.getTopologiesJarFile(), topoName, config, topology);
cluster.waitTopologyUp(topoName);
cluster.deleteAllBlobs();
{code}
This causes nimbus to get stuck and restart:


{code:java}
2018-06-20 15:48:14.273 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Received topology submission for wc-topology-test (storm-0.10.2.y.251 JDK-1.8.0_131) 
2018-06-20 15:48:14.629 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Activating wc-topology-test: wc-topology-test-1-1529509694
2018-06-20 15:48:14.724 o.a.s.d.n.Nimbus pool-27-thread-703 [INFO] TRANSITION: wc-topology-test-1-1529509694 KILL null true
2018-06-20 15:48:14.812 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormconf.ser
2018-06-20 15:48:14.830 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormcode.ser
2018-06-20 15:48:14.863 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormjar.jar
2018-06-20 15:48:18.449 o.a.s.s.r.s.p.DefaultSchedulingPriorityStrategy timer [INFO] SIM Scheduling wc-topology-test-1-1529509694 with score of 0.3125
2018-06-20 15:48:18.492 o.a.s.s.Cluster timer [INFO] STATUS - wc-topology-test-1-1529509694 Running - Fully Scheduled by DefaultResourceAwareStrategy
2018-06-20 15:48:18.527 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology id wc-topology-test-1-1529509694:

2018-06-20 15:48:18.979 o.a.s.d.n.Nimbus pool-27-thread-722 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormjar.jar
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

2018-06-20 15:48:22.884 o.a.s.d.n.Nimbus timer [INFO] Renewing Creds For wc-topology-test-1-1529509694 with org.apache.storm.security.auth.kerberos.AutoTGT@4482469c owned by hadoopqa@DEV.YGRID.YAHOO.COM


2018-06-20 15:48:37.947 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: KeyNotFoundException(msg:wc-topology-test-1-1529509694-stormcode.ser)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2822) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormcode.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:420) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1517) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2675) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.sendClusterMetricsToExecutors(Nimbus.java:2686) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2819) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-06-20 15:48:37.948 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:468) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:488) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) [storm-client-2.0.0.y.jar:2.0.0.y]
2018-06-20 15:48:37.950 o.a.s.d.n.Nimbus Thread-11 [INFO] Shutting down master
2018-06-20 15:48:37.950 o.a.s.u.Utils Thread-12 [INFO] Halting after 10 seconds

2018-06-20 15:48:46.672 o.a.s.d.n.Nimbus pool-27-thread-798 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormconf.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-06-20 15:48:47.950 o.a.s.u.Utils Thread-12 [WARN] Forcing Halt...

{code}
Nimbus then continually restarts:
{code:java}
2018-06-20 15:48:54.635 o.a.s.u.Utils main [ERROR] Received error in main thread.. terminating server...
java.lang.Error: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:603) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:582) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils$5.uncaughtException(Utils.java:931) [storm-client-2.0.0.y.jar:2.0.0.y]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_131]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_131]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.zookeeper.AclEnforcement.getTopoAcl(AclEnforcement.java:194) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithTopoChildren(AclEnforcement.java:250) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithReadOnlyTopoChildren(AclEnforcement.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:136) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1155) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1162) ~[storm-server-2.0.0.y.jar:2.0.0.y]
{code}",,
STORM-3118,"Nimbus has issues with Pacemaker:
{code:java}
2018-06-21 08:55:17.762 o.a.s.p.PacemakerClientHandler client-worker-2 [ERROR] Exception occurred in Pacemaker.
org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:582) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.ensureWritable0(AbstractByteBuf.java:276) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.writeShort(AbstractByteBuf.java:966) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.SaslMessageToken.write(SaslMessageToken.java:104) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable(ThriftEncoder.java:44) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encode(ThriftEncoder.java:77) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        ... 26 more
{code}
Prevents topology submission:


{code:java}
2018-06-21 09:10:46.343 o.a.s.d.n.Nimbus pool-37-thread-250 [WARN] Topology submission exception. (topology name='testStormKafkaNewApi')
java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3009) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
{code}",,
STORM-3168,"I was investigating these blobstore download messages which keep repeating for hours in the supervisor (and nimbus logs). I turned on debug logging, and was expecting a cleanup debug message every 30 seconds ([https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L606).] It did not log. I restarted the supervisor, and it started logging again. It appears to have crashed with some error.

We should make sure the cleanup runs continuously and logs any failures to investigate.


{code:java}
2018-07-30 23:25:35.691 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [ERROR] Could not update blob, will retry again later

java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...

    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_131]

    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_131]

    at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303) ~[storm-server-2.0.0.y.jar:2.0.0.y]

    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]

    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_131]

    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]

    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]

    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

Caused by: java.lang.RuntimeException: Could not download...

    at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268) ~[storm-server-2.0.0.y.jar:2.0.0.y]

    at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

    ... 3 more

Caused by: org.apache.storm.generated.KeyNotFoundException

    at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25821) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85) ~[storm-client-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122) ~[storm-server-2.0.0.y.jar:2.0.0.y]

    at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252) ~[storm-server-2.0.0.y.jar:2.0.0.y]

    at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

    ... 3 more
{code}",Surprise,-1
STORM-3208,"{code:java}
2018-08-29 15:37:47.891 o.a.s.u.Utils main [INFO] UNNAMED:main : user is gstorm
2018-08-29 15:37:47.893 o.a.s.d.s.Supervisor main [ERROR] Error trying to kill 7f4dd1bb-ea77-4f13-a785-0299e81bf5a5
java.lang.NullPointerException: null
        at org.apache.storm.daemon.supervisor.BasicContainer.cleanUpForRestart(BasicContainer.java:216) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Container.cleanUp(Container.java:360) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.killWorkers(Supervisor.java:482) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.ReadClusterState.<init>(ReadClusterState.java:111) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:282) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launchDaemon(Supervisor.java:312) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.main(Supervisor.java:185) [storm-server-2.0.0.y.jar:2.0.0.y]
2018-08-29 15:37:47.904 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 03ee87f5-28ca-491b-95cb-15b841f249e1-10.215.76.240 at host openqe74blue-n1.blue.ygrid.yahoo.com.

{code}",,
STORM-3213,"
{code:java}
org.apache.storm.thrift.TApplicationException: Internal error processing getComponentPageInfo
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1359)
	at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1343)
	at org.apache.storm.daemon.ui.UIHelpers.getComponentPage(UIHelpers.java:1559)
	at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyComponent(StormApiResource.java:438)
{code}



{code:java}
2018-09-05 16:15:24.927 o.a.s.t.ProcessFunction pool-21-thread-55 [ERROR] Internal error processing getComponentPageInfo
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4238) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4577) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4556) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:169) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.NullPointerException
        at org.apache.storm.scheduler.resource.ResourceUtils.getBoltResources(ResourceUtils.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4192) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 10 more
{code}
",,
YARN-1032,"We found a case where our rack resolve script was not returning rack due to problem with resolving host address. This exception was see in RackResolver.java as NPE, ultimately caught in RMContainerAllocator. 

{noformat}
2013-08-01 07:11:37,708 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
java.lang.NullPointerException
	at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)
	at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)
	at java.lang.Thread.run(Thread.java:722)

{noformat}",,
YARN-1149,"When nodemanager receives a kill signal when an application has finished execution but log aggregation has not kicked in, InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING is thrown

{noformat}
2013-08-25 20:45:00,875 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(254)) - Application just finished : application_1377459190746_0118
2013-08-25 20:45:00,876 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainer(105)) - Starting aggregate log-file for app application_1377459190746_0118 at /app-logs/foo/logs/application_1377459190746_0118/<host>_45454.tmp
2013-08-25 20:45:00,876 INFO  logaggregation.LogAggregationService (LogAggregationService.java:stopAggregators(151)) - Waiting for aggregation to complete for application_1377459190746_0118
2013-08-25 20:45:00,891 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainer(122)) - Uploading logs for container container_1377459190746_0118_01_000004. Current good log dirs are /tmp/yarn/local
2013-08-25 20:45:00,915 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:doAppLogAggregation(182)) - Finished aggregate log-file for app application_1377459190746_0118
2013-08-25 20:45:00,925 WARN  application.Application (ApplicationImpl.java:handle(427)) - Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305) 
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)   
        at java.lang.Thread.run(Thread.java:662)
2013-08-25 20:45:00,926 INFO  application.Application (ApplicationImpl.java:handle(430)) - Application application_1377459190746_0118 transitioned from RUNNING to null
2013-08-25 20:45:00,927 WARN  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(463)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.
2013-08-25 20:45:00,938 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 8040
{noformat}

",,
YARN-1274,"LCE container launch assumes the usercache/USER directory exists and it is owned by the user running the container process.

But the directory is created only if there are resources to localize by the LCE localization command, if there are not resourcdes to localize, LCE localization never executes and launching fails reporting 255 exit code and the NM logs have something like:

{code}
2013-10-04 14:07:56,425 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: main : command provided 1
2013-10-04 14:07:56,425 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: main : user is llama
2013-10-04 14:07:56,425 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Can't create directory llama in /yarn/nm/usercache/llama/appcache/application_1380853306301_0004/container_1380853306301_0004_01_000004 - Permission denied
{code}
",Surprise,-1
YARN-1374,"Resource Manager is failing to start with the below ConcurrentModificationException.

{code:xml}
2013-10-30 20:22:42,371 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2013-10-30 20:22:42,376 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state INITED; cause: java.util.ConcurrentModificationException
java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)
2013-10-30 20:22:42,378 INFO org.apache.hadoop.yarn.server.resourcemanager.RMHAProtocolService: Transitioning to standby
2013-10-30 20:22:42,378 INFO org.apache.hadoop.yarn.server.resourcemanager.RMHAProtocolService: Transitioned to standby
2013-10-30 20:22:42,378 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)
2013-10-30 20:22:42,379 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at HOST-10-18-40-24/10.18.40.24
************************************************************/
{code}",,
YARN-1409,"This problem is caused by handling APPLICATION_FINISHED events after calling sched.shotdown() in NonAggregatingLongHandler#serviceStop(). org.apache.hadoop.mapred.TestJobCleanup can fail because of RejectedExecutionException by NonAggregatingLogHandler.

{code}
2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)
        at java.lang.Thread.run(Thread.java:724)
{code}",,
YARN-1661,"Run:
/usr/bin/yarn  org.apache.hadoop.yarn.applications.distributedshell.Client -jar <distributed shell jar> -shell_command ls

Open AM logs. Last line would indicate AM failure even though container logs print good ls result.

{code}
2014-01-24 21:45:29,592 INFO  [main] distributedshell.ApplicationMaster (ApplicationMaster.java:finish(599)) - Application completed. Signalling finish to RM
2014-01-24 21:45:29,612 INFO  [main] impl.AMRMClientImpl (AMRMClientImpl.java:unregisterApplicationMaster(315)) - Waiting for application to be successfully unregistered.
2014-01-24 21:45:29,816 INFO  [main] distributedshell.ApplicationMaster (ApplicationMaster.java:main(267)) - Application Master failed. exiting
{code}",Surprise,-1
YARN-1675,"I dont see any stacktraces in logs. But the debug logs show negative vcores-

{noformat}
2014-01-29 18:42:26,357 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(808)) - assignContainers: node=hor11n39.gq1.ygridcore.net #applications=5
2014-01-29 18:42:26,357 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(827)) - pre-assignContainers for application application_1390986573180_0269
2014-01-29 18:42:26,358 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0269 headRoom=<memory:22528, vCores:0> currentConsumption=2048
2014-01-29 18:42:26,358 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0269 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,358 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(911)) - post-assignContainers for application application_1390986573180_0269
2014-01-29 18:42:26,358 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0269 headRoom=<memory:22528, vCores:0> currentConsumption=2048
2014-01-29 18:42:26,358 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0269 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,358 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(827)) - pre-assignContainers for application application_1390986573180_0272
2014-01-29 18:42:26,358 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0272 headRoom=<memory:18432, vCores:-2> currentConsumption=2048
2014-01-29 18:42:26,359 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0272 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,359 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(911)) - post-assignContainers for application application_1390986573180_0272
2014-01-29 18:42:26,359 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0272 headRoom=<memory:18432, vCores:-2> currentConsumption=2048
2014-01-29 18:42:26,359 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0272 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,359 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(827)) - pre-assignContainers for application application_1390986573180_0273
2014-01-29 18:42:26,359 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0273 headRoom=<memory:18432, vCores:-2> currentConsumption=2048
2014-01-29 18:42:26,359 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0273 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,360 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(911)) - post-assignContainers for application application_1390986573180_0273
2014-01-29 18:42:26,360 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0273 headRoom=<memory:18432, vCores:-2> currentConsumption=2048
2014-01-29 18:42:26,360 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0273 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,360 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(827)) - pre-assignContainers for application application_1390986573180_0274
2014-01-29 18:42:26,360 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0274 headRoom=<memory:16384, vCores:-3> currentConsumption=2048
2014-01-29 18:42:26,360 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0274 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,360 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(911)) - post-assignContainers for application application_1390986573180_0274
2014-01-29 18:42:26,361 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0274 headRoom=<memory:16384, vCores:-3> currentConsumption=2048
2014-01-29 18:42:26,361 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0274 request={Priority: 0, Capability: <memory:2048, vCores:1>, # Containers: 0, Location: *, Relax Locality: true}
2014-01-29 18:42:26,361 DEBUG capacity.LeafQueue (LeafQueue.java:assignContainers(827)) - pre-assignContainers for application application_1390986573180_0286
2014-01-29 18:42:26,361 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(326)) - showRequests: application=application_1390986573180_0286 headRoom=<memory:14336, vCores:-4> currentConsumption=2048
2014-01-29 18:42:26,361 DEBUG scheduler.SchedulerApplicationAttempt (SchedulerApplicationAttempt.java:showRequests(330)) - showRequests: application=application_1390986573180_0286 request={Priority: 0, Capability: <memory:2048, vCores:1>, # 

{noformat}",Surprise,-1
YARN-1678,"Come on FS. We really don't need to know every time a node with a reservation on it heartbeats.

{code}
2014-01-29 03:48:16,043 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Trying to fulfill reservation for application appattempt_1390547864213_0347_000001 on node: host: a2330.halxg.cloudera.com:8041 #containers=8 available=<memory:0, vCores:8> used=<memory:8192, vCores:8>
2014-01-29 03:48:16,043 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable: Making reservation: node=a2330.halxg.cloudera.com app_id=application_1390547864213_0347
2014-01-29 03:48:16,043 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: Application application_1390547864213_0347 reserved container container_1390547864213_0347_01_000003 on node host: a2330.halxg.cloudera.com:8041 #containers=8 available=<memory:0, vCores:8> used=<memory:8192, vCores:8>, currently has 6 at priority 0; currentReservation 6144
2014-01-29 03:48:16,044 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode: Updated reserved container container_1390547864213_0347_01_000003 on node host: a2330.halxg.cloudera.com:8041 #containers=8 available=<memory:0, vCores:8> used=<memory:8192, vCores:8> for application org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp@1cb01d20
{code}",Sadness,-1
YARN-1689,"When running some Hive on Tez jobs, the RM after a while gets into an unusable state where no jobs run. In the RM log I see the following exception:
{code}
2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)
        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
......
2014-02-04 20:28:08,544 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(626)) - Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
        at java.lang.Thread.run(Thread.java:662)
2014-02-04 20:28:08,549 INFO  resourcemanager.RMAuditLogger (RMAuditLogger.java:logSuccess(140)) - USER=hrt_qa  IP=172.18.145.156       OPERATION=Kill Application Request      TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1391543307203_0001
2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)
        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
{code}",Surprise,-1
YARN-1692,"We saw a ConcurrentModificationException thrown in the fair scheduler:

{noformat}
2014-02-07 01:40:01,978 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Exception in fair scheduler UpdateThread
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
        at java.util.HashMap$ValueIterator.next(HashMap.java:954)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)
        at java.lang.Thread.run(Thread.java:724)
{noformat}

The map that  gets returned by FSSchedulerApp.getResourceRequests() are iterated on without proper synchronization.",Sadness,-1
YARN-174,"{noformat}
2012-10-19 12:18:23,941 FATAL [Node Status Updater] nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(277)) - Error starting NodeManager
org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme
        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)
        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)
        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)
        at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)
        at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)
        at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)
        at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)
        at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)
{noformat}

The NM then calls System.exit(-1), which makes the unit test exit and produces an error that is hard to track down.",Sadness,-1
YARN-1752,"{code}
2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED
  at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
  at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
  at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)
  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)
  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)
  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)
  at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
  at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
  at java.lang.Thread.run(Thread.java:695)
{code}

",,
YARN-1839,"Use single-node cluster. Turn on capacity scheduler preemption. Run MR sleep job as app 1. Take entire cluster. Run MR sleep job as app 2. Preempt app1 out. Wait till app 2 finishes. App 1 AM attempt 2 will start. It won't be able to launch a task container with this error stack trace in AM logs:

{code}
2014-03-13 20:13:50,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1394741557066_0001_m_000000_1009: Container launch failed for container_1394741557066_0001_02_000021 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)
	at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)
	at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)
	at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)
	at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{code}

",Sadness,-1
YARN-196,"If NM is started before starting the RM ,NM is shutting down with the following error
{code}
ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager
org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)
Caused by: java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)
	... 3 more
Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)
	at $Proxy23.registerNodeManager(Unknown Source)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)
	... 5 more
Caused by: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:857)
	at org.apache.hadoop.ipc.Client.call(Client.java:1141)
	at org.apache.hadoop.ipc.Client.call(Client.java:1100)
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:128)
	... 7 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:659)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:469)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:563)
	at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:211)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1247)
	at org.apache.hadoop.ipc.Client.call(Client.java:1117)
	... 9 more
2012-01-16 15:04:13,336 WARN org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher thread interrupted
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:76)
	at java.lang.Thread.run(Thread.java:619)
2012-01-16 15:04:13,337 INFO org.apache.hadoop.yarn.service.AbstractService: Service:Dispatcher is stopped.
2012-01-16 15:04:13,392 INFO org.mortbay.log: Stopped SelectChannelConnector@0.0.0.0:9999
2012-01-16 15:04:13,493 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is stopped.
2012-01-16 15:04:13,493 INFO org.apache.hadoop.ipc.Server: Stopping server on 24290
2012-01-16 15:04:13,494 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 24290
2012-01-16 15:04:13,495 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2012-01-16 15:04:13,496 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler is stopped.
2012-01-16 15:04:13,496 WARN org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher thread interrupted
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:76)
	at java.lang.Thread.run(Thread.java:619)
{code}",,
YARN-2124,"When I play with scheduler with preemption, I found ProportionalCapacityPreemptionPolicy cannot work. NPE will be raised when RM start
{code}
2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.
java.lang.NullPointerException
        at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)
        at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)
        at java.lang.Thread.run(Thread.java:744)
{code}

This is caused by ProportionalCapacityPreemptionPolicy needs ResourceCalculator from CapacityScheduler. But ProportionalCapacityPreemptionPolicy get initialized before CapacityScheduler initialized. So ResourceCalculator will set to null in ProportionalCapacityPreemptionPolicy. ",Surprise,-1
YARN-2230,"When a user requests more vcores than the allocation limit (e.g. mapreduce.map.cpu.vcores  is larger than yarn.scheduler.maximum-allocation-vcores), then InvalidResourceRequestException is thrown - https://svn.apache.org/repos/asf/hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
{code}
    if (resReq.getCapability().getVirtualCores() < 0 ||
        resReq.getCapability().getVirtualCores() >
        maximumResource.getVirtualCores()) {
      throw new InvalidResourceRequestException(""Invalid resource request""
          + "", requested virtual cores < 0""
          + "", or requested virtual cores > max configured""
          + "", requestedVirtualCores=""
          + resReq.getCapability().getVirtualCores()
          + "", maxVirtualCores="" + maximumResource.getVirtualCores());
    }
{code}

According to documentation - yarn-default.xml http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml, the request should be capped to the allocation limit.
{code}
  <property>
    <description>The maximum allocation for every container request at the RM,
    in terms of virtual CPU cores. Requests higher than this won't take effect,
    and will get capped to this value.</description>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>32</value>
  </property>
{code}

This means that:
* Either documentation or code should be corrected (unless this exception is handled elsewhere accordingly, but it looks that it is not).

This behavior is confusing, because when such a job (with mapreduce.map.cpu.vcores is larger than yarn.scheduler.maximum-allocation-vcores) is submitted, it does not make any progress. The warnings/exceptions are thrown at the scheduler (RM) side e.g.
{code}
2014-06-29 00:34:51,469 WARN org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Invalid resource ask by application appattempt_1403993411503_0002_000001
org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested virtual cores < 0, or requested virtual cores > max configured, requestedVirtualCores=32, maxVirtualCores=3
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:237)
	at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.validateResourceRequests(RMServerUtils.java:80)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:420)
        .....
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)
{code}

* IMHO, such an exception should be forwarded to client. Otherwise, it is non obvious to discover why a job does not make any progress.

The same looks to be related to memory.","Sadness, Surprise",-1
YARN-2273,"One DN experienced memory errors and entered a cycle of rebooting and rejoining the cluster. After the second time the node went away, the RM produced this:
{code}
2014-07-09 21:47:36,571 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1404858438119_4352_000001 released container container_1404858438119_4352_01_000004 on node: host: node-A16-R09-19.hadoop.dfw.wordpress.com:8041 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL
2014-07-09 21:47:36,571 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Removed node node-A16-R09-19.hadoop.dfw.wordpress.com:8041 cluster capacity: <memory:335872, vCores:328>
2014-07-09 21:47:36,571 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[ContinuousScheduling,5,main] threw an Exception.
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)
	at java.util.TimSort.sort(TimSort.java:203)
	at java.util.TimSort.sort(TimSort.java:173)
	at java.util.Arrays.sort(Arrays.java:659)
	at java.util.Collections.sort(Collections.java:217)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)
	at java.lang.Thread.run(Thread.java:744)
{code}

A few cycles later YARN was crippled. The RM was running and jobs could be submitted but containers were not assigned and no progress was made. Restarting the RM resolved it.",,
YARN-2308,"I encountered a NPE when RM restart
{code}
2014-07-16 07:22:46,957 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
        at java.lang.Thread.run(Thread.java:744)
{code}
And RM will be failed to restart.

This is caused by queue configuration changed, I removed some queues and added new queues. So when RM restarts, it tries to recover history applications, and when any of queues of these applications removed, NPE will be raised.",,
YARN-2409,"{code}
	at java.lang.Thread.run(Thread.java:662)
2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
	at java.lang.Thread.run(Thread.java:662)
2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at LAUNCHED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
	at java.lang.Thread.run(Thread.java:662)
2014-08-12 07:03:00,839 ERROR org.apache.hadoop.ya
{code}",,
YARN-241,"After restarting the Node Manager it fails to launch containers with the below exception.

 {code:xml}
2012-11-24 17:21:56,141 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8048: readAndProcess threw exception java.lang.IllegalArgumentException: Invalid key to HMAC computation from client 158.1.131.10. Count of bytes read: 0
java.lang.IllegalArgumentException: Invalid key to HMAC computation
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:153)
        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.retrievePassword(ContainerTokenSecretManager.java:109)
        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.retrievePassword(ContainerTokenSecretManager.java:44)
        at org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler.getPassword(SaslRpcServer.java:194)
        at org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler.handle(SaslRpcServer.java:220)
        at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:568)
        at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:226)
        at org.apache.hadoop.ipc.Server$Connection.saslReadAndProcess(Server.java:1199)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1393)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:710)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:509)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:484)
Caused by: java.security.InvalidKeyException: No installed provider supports this key: javax.crypto.spec.SecretKeySpec
        at javax.crypto.Mac.a(DashoA13*..)
        at javax.crypto.Mac.init(DashoA13*..)
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:151)
        ... 11 more

 {code}",,
YARN-2414,"{code}
2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
	at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
	at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)
	at org.apache.hadoop.yarn.webapp.View.render(View.java:235)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)
	at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
	at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)
	at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)
	at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)
	... 44 more
{code}",,
YARN-2612,"We are testing RM work preserving restart and found the following logs when we ran a simple MapReduce task ""PI"". Some completed containers which already pulled by AM never reported back to NM, so NM continuously report the completed containers while AM had finished. 
{code}
2014-09-26 17:00:42,228 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:42,228 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:43,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:43,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:44,233 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:44,233 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
{code}

In YARN-1372, NM will report completed containers to RM until it gets ACK from RM.  If AM does not call allocate, which means that AM does not ack RM, RM will not ack NM. We([~chenchun]) have observed these two cases when running Mapreduce task 'pi':
1) RM sends completed containers to AM. After receiving it, AM thinks it has done the work and does not need resource, so it does not call allocate.
2) When AM finishes, it could not ack to RM because AM itself has not finished yet.

We think when RMAppAttempt call BaseFinalTransition, it means AppAttempt finishes, then RM could send this AppAttempt's completed containers to NM.
",,
YARN-2617,"We([~chenchun]) are testing RM work preserving restart and found the following logs when we ran a simple MapReduce task ""PI"". NM continuously reported completed containers whose Application had already finished while AM had finished. 
{code}
2014-09-26 17:00:42,228 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:42,228 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:43,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:43,230 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:44,233 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2014-09-26 17:00:44,233 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
{code}

In the patch for YARN-1372, ApplicationImpl on NM should guarantee to  clean up already completed applications. But it will only remove appId from  'app.context.getApplications()' when ApplicaitonImpl received evnet 'ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED' , however NM might receive this event for a long time or could not receive. 
* For NonAggregatingLogHandler, it wait for YarnConfiguration.NM_LOG_RETAIN_SECONDS which is 3 * 60 * 60 sec by default, then it will be scheduled to delete Application logs and send the event.
* For LogAggregationService, it might fail(e.g. if user does not have HDFS write permission), and it will not send the event.",Surprise,-1
YARN-2649,"Sometimes the test fails with the following error:

testAMRMUnusableNodes(org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates)  Time elapsed: 41.73 sec  <<< FAILURE!
junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)
	at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)



When this happens, SchedulerEventType.NODE_UPDATE was processed before RMAppAttemptEvent.ATTEMPT_ADDED was processed. That is possible, given the test only waits for RMAppState.ACCEPTED before having NM sending heartbeat. This can be reproduced using custom AsyncDispatcher with CountDownLatch. Here is the log when this happens.

{noformat}
App State is : ACCEPTED
2014-10-05 21:25:07,305 INFO  [AsyncDispatcher event handler] attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from NEW to SUBMITTED
2014-10-05 21:25:07,305 DEBUG [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE
2014-10-05 21:25:07,305 DEBUG [AsyncDispatcher event handler] rmnode.RMNodeImpl (RMNodeImpl.java:handle(384)) - Processing 127.0.0.1:1234 of type STATUS_UPDATE
AppAttempt : appattempt_1412569506932_0001_000001 State is : SUBMITTED Waiting for state : ALLOCATED
2014-10-05 21:25:07,306 DEBUG [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent.EventType: APP_ATTEMPT_ADDED

2014-10-05 21:25:07,328 DEBUG [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE

2014-10-05 21:25:07,330 DEBUG [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent.EventType: ATTEMPT_ADDED
2014-10-05 21:25:07,331 DEBUG [AsyncDispatcher event handler] attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(658)) - Processing event for appattempt_1412569506932_0001_000
001 of type ATTEMPT_ADDED

2014-10-05 21:25:07,333 INFO  [AsyncDispatcher event handler] attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from SUBMITTED to SCHEDULED

{noformat}


",,
YARN-2671,"After YARN-2493, app submission goes wrong with the following exception:
{code}
2014-10-09 15:50:35,774 WARN  [297524352@qtp-1314143300-2 - /ws/v1/cluster/apps] webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateResourceRequest(RMAppManager.java:390)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:346)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:273)
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:570)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$2.run(RMWebServices.java:896)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$2.run(RMWebServices.java:1)
{code}

This is because resource is putting into ResourceRequest of ApplicationSubmissionContext, but not directly into ApplicationSubmissionContext, therefore the sanity check won't get resource object from context.",,
YARN-2742,"FairSchedulerConfiguration is very strict about the number of space characters between the value and the unit: 0 or 1 space.

For example, for values like the following:

{noformat}
<maxResources>4096  mb, 2 vcores<maxResources>
{noformat}
(note 2 spaces)

This above line fails to parse:

{noformat}
2014-10-24 22:56:40,802 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService: Failed to reload fair scheduler config file - will use existing allocations.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfigurationException: Missing resource: mb
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration.findResource(FairSchedulerConfiguration.java:247)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration.parseResourceConfigValue(FairSchedulerConfiguration.java:231)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.loadQueue(AllocationFileLoaderService.java:347)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.loadQueue(AllocationFileLoaderService.java:381)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadAllocations(AllocationFileLoaderService.java:293)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService$1.run(AllocationFileLoaderService.java:117)
{noformat}",Sadness,-1
YARN-2790,"We shorten hdfs delegation token lifetime, set RM to act as as a proxy user in order to be able to renew the token on behalf of a user submitting the application. Still we see NM log aggregation fail due to token expiry error.

{code}
2014-10-31 00:11:56,579 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(481)) - Application just finished : application_1414705229376_0004
2014-10-31 00:11:56,588 WARN  ipc.Client (Client.java:run(675)) - Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 75 for user1) is expired
2014-10-31 00:11:56,589 ERROR logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainers(233)) - Cannot create writer for app application_1414705229376_0004. Skip log upload this time.
{code}",Sadness,-1
YARN-2805,"{code}
2014-11-04 08:41:08,705 INFO  resourcemanager.ResourceManager (SignalLogger.java:register(91)) - registered UNIX signal handlers for [TERM, HUP, INT]
2014-11-04 08:41:10,636 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service ResourceManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)
Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)

{code}",,
YARN-2813,"{code}
2014-11-04 20:50:05,146 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
javax.ws.rs.WebApplicationException: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
        at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
        at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)
        at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)
        at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)
        ... 54 more
{code}",,
YARN-2816,"NM fail to start with NPE during container recovery.
We saw the following crash happen:
2014-10-30 22:22:37,211 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl failed in state INITED; cause: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recoverContainer(ContainerManagerImpl.java:289)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:252)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:235)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:250)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:445)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:492)

The reason is some DB files used in NMLeveldbStateStoreService are accidentally deleted to save disk space at /tmp/hadoop-yarn/yarn-nm-recovery/yarn-nm-state. This leaves some incomplete container record which don't have CONTAINER_REQUEST_KEY_SUFFIX(startRequest) entry in the DB. When container is recovered at ContainerManagerImpl#recoverContainer, 
The NullPointerException at the following code cause NM shutdown.
{code}
    StartContainerRequest req = rcs.getStartRequest();
    ContainerLaunchContext launchContext = req.getContainerLaunchContext();
{code}",Surprise,-1
YARN-2823,"Branch:
2.6.0

Environment: 
A 3-node cluster with RM HA enabled. The HA setup went pretty smooth (used Ambari) and then installed HBase using Slider. After some time the RMs went down and would not come back up anymore. Following is the NPE we see in both the RM logs.

{noformat}
2014-09-16 01:36:28,037 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(612)) - Error in handling event type APP_ATTEMPT_ADDED to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)
        at java.lang.Thread.run(Thread.java:744)
2014-09-16 01:36:28,042 INFO  resourcemanager.ResourceManager (ResourceManager.java:run(616)) - Exiting, bbye..
{noformat}

All the logs for this 3-node cluster has been uploaded.",Surprise,-1
YARN-2834,"Resource manager failed after restart. 

{noformat}
2014-11-09 04:12:53,013 INFO  capacity.CapacityScheduler (CapacityScheduler.java:initializeQueues(467)) - Initialized root queue root: numChildQueue= 2, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2014-11-09 04:12:53,013 INFO  capacity.CapacityScheduler (CapacityScheduler.java:initializeQueueMappings(436)) - Initialized queue mappings, override: false
2014-11-09 04:12:53,013 INFO  capacity.CapacityScheduler (CapacityScheduler.java:initScheduler(305)) - Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:256, vCores:1>>, maximumAllocation=<<memory:2048, vCores:32>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2014-11-09 04:12:53,015 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service ResourceManager failed in state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)
{noformat}",,
YARN-2846,"The NM restart work preserving feature could make running AM container get LOST and killed during stop NM daemon. The exception is like below:
{code}
2014-11-11 00:48:35,214 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 22140 for container-id container_1415666714233_0001_01_000084: 53.8 MB of 512 MB physical memory used; 931.3 MB of 1.0 GB virtual memory used
2014-11-11 00:48:35,223 ERROR nodemanager.NodeManager (SignalLogger.java:handle(60)) - RECEIVED SIGNAL 15: SIGTERM
2014-11-11 00:48:35,299 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50060
2014-11-11 00:48:35,337 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(512)) - Applications still running : [application_1415666714233_0001]
2014-11-11 00:48:35,338 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 45454
2014-11-11 00:48:35,344 INFO  ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 45454
2014-11-11 00:48:35,346 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceStop(141)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit
2014-11-11 00:48:35,347 INFO  ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder
2014-11-11 00:48:35,347 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(502)) - Aborting log aggregation for application_1415666714233_0001
2014-11-11 00:48:35,348 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(382)) - Aggregation did not complete for application application_1415666714233_0001
2014-11-11 00:48:35,358 WARN  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(476)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.
2014-11-11 00:48:35,406 ERROR launcher.RecoveredContainerLaunch (RecoveredContainerLaunch.java:call(87)) - Unable to recover container container_1415666714233_0001_01_000001
java.io.IOException: Interrupted while waiting for process 20001 to exit
        at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)
        ... 6 more
{code}
In reacquireContainer() of ContainerExecutor.java, the while loop of checking container process (AM container) will be interrupted by NM stop. The IOException get thrown and failed to generate an ExitCodeFile for the running container. Later, the IOException will be caught in upper call (RecoveredContainerLaunch.call()) and the ExitCode (by default to be LOST without any setting) get persistent in NMStateStore. 
After NM restart again, this container is recovered as COMPLETE state but exit code is LOST (154) - cause this (AM) container get killed later.
We should get rid of recording the exit code of running containers if detecting process is interrupted. ","Fear, Surprise",-1
YARN-2931,"When the data directory is cleaned up and NM is started with existing recovery state, because of YARN-90, it will not recreate the local dirs.
This causes a PublicLocalizer to fail until getInitializedLocalDirs is called due to some LocalizeRunner for private localization.

Example error 

{noformat}
2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Failed to download rsrc { { hdfs:/<blah machine>:8020/tmp/hive-hive/hive_2014-12-02_22-56-58_741_2045919883676051996-3/-mr-10004/8060c9dd-54b6-42fc-9d77-34b655fa5e82/reduce.xml, 1417589819618, FILE, null },pending,[(container_1417589109512_0001_02_000003)],119413444132127,DOWNLOADING}
java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)
	at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)
	at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)
	at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)
	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)
	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)
	at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1417589109512_0001_02_000003 transitioned from LOCALIZING to LOCALIZATION_FAILED
{noformat}",,
YARN-298,"{code:xml}
2012-12-28 06:03:49,125 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.security.ApplicationACLsManager.checkAccess(ApplicationACLsManager.java:104)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices.hasAccess(RMWebServices.java:100)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices.getApps(RMWebServices.java:362)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.util.IpValidationFilter.doFilter(IpValidationFilter.java:60)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:985)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}",,
YARN-3351,"After YARN-2713, the AppMaster link is broken in HA.  To repro 
a) setup RM HA and ensure the first RM is not active,
b) run a long sleep job and view the tracking url on the RM applications page

The log and full stack trace is shown below
{noformat}
2015-02-05 20:47:43,478 WARN org.mortbay.log: /proxy/application_1423182188062_0002/: java.net.BindException: Cannot assign requested address
{noformat}
{noformat}
java.net.BindException: Cannot assign requested address
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)
	at java.net.Socket.bind(Socket.java:631)
	at java.net.Socket.<init>(Socket.java:423)
	at java.net.Socket.<init>(Socket.java:280)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
{noformat}",,
YARN-3369,"In AppSchedulingInfo.java the method checkForDeactivation() has these 2 consecutive lines:
{code}
ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);
if (request.getNumContainers() > 0) {
{code}
the first line calls getResourceRequest and it can return null.
{code}
synchronized public ResourceRequest getResourceRequest(
Priority priority, String resourceName) {
    Map<String, ResourceRequest> nodeRequests = requests.get(priority);
    return  (nodeRequests == null) ? {color:red} null : nodeRequests.get(resourceName);
}
{code}
The second line dereferences the pointer directly without a check.
If the pointer is null, the RM dies. 

{quote}2015-03-17 14:14:04,757 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)
        at java.lang.Thread.run(Thread.java:722)
{color:red} *2015-03-17 14:14:04,758 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye..*{color} {quote}",Surprise,-1
YARN-3425,"Configure yarn.node-labels.enabled to true 
and yarn.node-labels.fs-store.root-dir /node-labels
Start resource manager without starting DN/NM

{quote}
2015-03-31 16:44:13,782 WARN org.apache.hadoop.service.AbstractService: When stopping the service org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager : java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.stopDispatcher(CommonNodeLabelsManager.java:261)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStop(CommonNodeLabelsManager.java:267)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
	at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:171)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:556)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:984)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:251)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1207)

{quote}


{code}
 protected void stopDispatcher() {
    AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;
       asyncDispatcher.stop(); 
  }
{code}

Null check missing during stop",,
YARN-345,"{code:xml}
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
{code}
{code:xml}
2013-01-17 04:03:46,726 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at APPLICATION_RESOURCES_CLEANINGUP
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
{code}
{code:xml}
2013-01-17 00:01:11,006 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHING_CONTAINERS_WAIT
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
{code}
{code:xml}

2013-01-17 10:56:36,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1358385982671_1304_01_000001 transitioned from NEW to DONE
2013-01-17 10:56:36,975 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_CONTAINER_FINISHED at FINISHED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
2013-01-17 10:56:36,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Application application_1358385982671_1304 transitioned from FINISHED to null
{code}
{code:xml}

2013-01-17 10:56:36,026 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: INIT_CONTAINER at FINISHED
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
	at java.lang.Thread.run(Thread.java:662)
2013-01-17 10:56:36,026 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Application application_1358385982671_1304 transitioned from FINISHED to null
{code}
",,
YARN-3493,"RM fails to come up for the following case:
1. Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml
2. Start a randomtextwriter job with mapreduce.map.memory.mb=4000 in background and wait for the job to reach running state
3. Restore yarn-site.xml to have yarn.scheduler.maximum-allocation-mb to 2048 before the above job completes
4. Restart RM
5. RM fails to come up with the below error
{code:title= RM error for Mem settings changed}
 - RM app submission failed in validating AM resource request for application application_1429094976272_0008
org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
2015-04-15 13:19:18,623 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(579)) - Failed to load/recover state
org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)
at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
2015-04-15 13:19:18,624 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service RMActiveServices failed in state STARTED; cause: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048
org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)
        at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
2015-04-15 13:19:18,625 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(211)) - Stopping ResourceManager metrics system...
2015-04-15 13:19:18,626 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - timeline thread interrupted.
2015-04-15 13:19:18,626 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(217)) - ResourceManager metrics system stopped.
2015-04-15 13:19:18,627 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(606)) - ResourceManager metrics system shutdown complete.
2015-04-15 13:19:18,627 INFO  event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(140)) - AsyncDispatcher is draining to stop, igonring any new events.
2015-04-15 13:19:18,633 INFO  zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x44cbc922670001c closed
2015-04-15 13:19:18,633 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2015-04-15 13:19:18,634 INFO  event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(140)) - AsyncDispatcher is draining to stop, igonring any new events.
2015-04-15 13:19:18,634 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service Dispatcher failed in state STOPPED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.yarn.event.AsyncDispatcher.serviceStop(AsyncDispatcher.java:142)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)
        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)
        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)
        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:601)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)
{code}

",,
YARN-351,"ResourceManager seem to die due to NPE shown below on FairScheduler.
This is easily reproduced on a cluster with multiple racks and nodes within each rack. Simple job with multiple tasks on each node triggers NPE in RM.

Without understanding actual workings, I tried to do a null check which looked like it solved problem. But I am not sure if that is the right behavior yet.

I feel this is serious enough to be marked as blocker, what do you guys think?

{noformat}
2013-01-22 20:07:45,073 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: allocate: applicationId=application_1358885180585_0001 container=container_1358885180585_0001_01_000830 host=x.x.x.x:36186
2013-01-22 20:07:45,074 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:259)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:220)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.allocate(FSSchedulerApp.java:544)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:250)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:318)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:180)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:796)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:859)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:375)
        at java.lang.Thread.run(Thread.java:662)
2013-01-22 20:07:45,075 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye..
{noformat}",Sadness,-1
YARN-363,"Starting up the proxy server fails with this error:

{noformat}
2013-01-29 17:37:41,357 FATAL webproxy.WebAppProxy (WebAppProxy.java:start(99)) - Could not start proxy web server
java.io.FileNotFoundException: webapps/proxy not found in CLASSPATH
	at org.apache.hadoop.http.HttpServer.getWebAppsPath(HttpServer.java:533)
	at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:225)
	at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:164)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxy.start(WebAppProxy.java:90)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer.main(WebAppProxyServer.java:94)
{noformat}
",,
YARN-3641,"If NM' services not get stopped properly, we cannot start NM with enabling NM restart with work preserving. The exception is as following:
{noformat}
org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: lock /var/log/hadoop-yarn/nodemanager/recovery-state/yarn-nm-state/LOCK: Resource temporarily unavailable
	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartRecoveryStore(NodeManager.java:175)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:217)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:507)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:555)
Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: lock /var/log/hadoop-yarn/nodemanager/recovery-state/yarn-nm-state/LOCK: Resource temporarily unavailable
	at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)
	at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)
	at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.initStorage(NMLeveldbStateStoreService.java:930)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService.serviceInit(NMStateStoreService.java:204)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	... 5 more
2015-05-12 00:34:45,262 INFO  nodemanager.NodeManager (LogAdapter.java:info(45)) - SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NodeManager at c6403.ambari.apache.org/192.168.64.103
************************************************************/
{noformat}

The related code is as below in NodeManager.java:
{code}
  @Override
  protected void serviceStop() throws Exception {
    if (isStopping.getAndSet(true)) {
      return;
    }
    super.serviceStop();
    stopRecoveryStore();
    DefaultMetricsSystem.shutdown();
  }
{code}
We can see we stop all NM registered services (NodeStatusUpdater, LogAggregationService, ResourceLocalizationService, etc.) first. Any of services get stopped with exception could cause stopRecoveryStore() get skipped which means levelDB store is not get closed. So next time NM start, it will get failed with exception above. 
We should put stopRecoveryStore(); in a finally block.",Fear,-1
YARN-3742,"The RM goes down showing the following stacktrace if the ZK client connection fails to be created. We should not exit but transition to StandBy and stop doing things and let the other RM take over.

{code}
2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:
java.io.IOException: Wait for ZKClient creation timed out
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
	at java.lang.Thread.run(Thread.java:745)
{code}",Surprise,-1
YARN-3753,"RM failed to come up with the following error while submitting an mapreduce job.
{code:title=RM log}
015-05-30 03:40:12,190 ERROR recovery.RMStateStore (RMStateStore.java:transition(179)) - Error storing app: application_1432956515242_0006
java.io.IOException: Wait for ZKClient creation timed out
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1098)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1122)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:923)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:937)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.createWithRetries(ZKRMStateStore.java:970)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:609)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:175)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:160)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:837)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:900)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:895)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)
	at java.lang.Thread.run(Thread.java:745)
2015-05-30 03:40:12,194 FATAL resourcemanager.ResourceManager (ResourceManager.java:handle(750)) - Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:
java.io.IOException: Wait for ZKClient creation timed out
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1098)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1122)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:923)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:937)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.createWithRetries(ZKRMStateStore.java:970)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:609)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:175)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:160)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:837)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:900)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:895)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)
	at java.lang.Thread.run(Thread.java:745)
{code}",,
YARN-3878,"The sequence of events is as under :
# RM is stopped while putting a RMStateStore Event to RMStateStore's AsyncDispatcher. This leads to an Interrupted Exception being thrown.
# As RM is being stopped, RMStateStore's AsyncDispatcher is also stopped. On {{serviceStop}}, we will check if all events have been drained and wait for event queue to drain(as RM State Store dispatcher is configured for queue to drain on stop). 
# This condition never becomes true and AsyncDispatcher keeps on waiting incessantly for dispatcher event queue to drain till JVM exits.

*Initial exception while posting RM State store event to queue*
{noformat}
2015-06-27 20:08:35,922 DEBUG [main] service.AbstractService (AbstractService.java:enterState(452)) - Service: Dispatcher entered state STOPPED
2015-06-27 20:08:35,923 WARN  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)
	at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.access$3300(RMAppAttemptImpl.java:109)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1619)
	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)
{noformat}

*JStack of AsyncDispatcher hanging on stop*
{noformat}
""AsyncDispatcher event handler"" prio=10 tid=0x00007fb980222800 nid=0x4b1e waiting on condition [0x00007fb9654e9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000700b79250> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:113)
        at java.lang.Thread.run(Thread.java:744)

""main"" prio=10 tid=0x00007fb98000a800 nid=0x49c3 in Object.wait() [0x00007fb989851000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000700b79430> (a java.lang.Object)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.serviceStop(AsyncDispatcher.java:156)
	- locked <0x0000000700b79430> (a java.lang.Object)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	- locked <0x0000000700b79420> (a java.lang.Object)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStop(RMStateStore.java:515)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	- locked <0x0000000700b79630> (a java.lang.Object)
	at org.apache.hadoop.service.AbstractService.close(AbstractService.java:250)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:599)
{noformat}

We keep on getting below logs
{noformat}
2015-06-27 20:08:35,926 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(140)) - AsyncDispatcher is draining to stop, igonring any new events.
2015-06-27 20:08:36,926 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:37,927 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:38,927 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:39,928 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:40,929 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:41,929 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:42,930 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:43,930 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:44,931 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:45,931 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2015-06-27 20:08:46,932 INFO  [main] event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(144)) - Waiting for AsyncDispatcher to drain. Thread state is :WAITING
{noformat}","Sadness, Surprise",-1
YARN-3896,"{noformat}
2015-07-03 16:49:39,075 INFO org.apache.hadoop.yarn.util.RackResolver: Resolved 10.208.132.153 to /default-rack
2015-07-03 16:49:39,075 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: Reconnect from the node at: 10.208.132.153
2015-07-03 16:49:39,075 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node 10.208.132.153(cmPort: 8041 httpPort: 8080) registered with capability: <memory:6144, vCores:60, diskCapacity:213>, assigned nodeId 10.208.132.153:8041
2015-07-03 16:49:39,104 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: Too far behind rm response id:2506413 nm response id:0
2015-07-03 16:49:39,137 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Deactivating Node 10.208.132.153:8041 as it is now REBOOTED
2015-07-03 16:49:39,137 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: 10.208.132.153:8041 Node Transitioned from RUNNING to REBOOTED
{noformat}

The node(10.208.132.153) reconnected with RM. When it registered with RM, RM set its lastNodeHeartbeatResponse's id to 0 asynchronously. But the node's heartbeat come before RM succeeded setting the id to 0.",Surprise,-1
YARN-3917,"Since the user has not configured a specific plugin, any problems with the default resource calculator instantiation should be ignored.

{code}
2015-07-10 08:16:18,445 INFO org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.UnsupportedOperationException: Could not determine OS
java.lang.UnsupportedOperationException: Could not determine OS
        at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)
        at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)
        at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)
{code}",Surprise,-1
YARN-3963,"Currently as per the code in {{CommonNodeLabelManager#addToClusterNodeLabels}} when we add same nodelabel again event will not be fired so no updation is done. 
 
{noformat}
./yarn rmadmin addToClusterNodeLabels x
./yarn rmadmin addToClusterNodeLabels x(exclusive=true)
./yarn rmadmin addToClusterNodeLabels x(exclusive=false)
 {noformat}


All these commands will give success when applied again through CLI 
 
{code}
2015-07-22 21:16:57,779 INFO org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager: Add labels: [<z:exclusivity=true>]
2015-07-22 21:16:57,779 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dsperf   IP=10.19.92.117 OPERATION=addToClusterNodeLabels        TARGET=AdminService     RESULT=SUCCESS
2015-07-22 21:17:06,431 INFO org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager: Add labels: [<z:exclusivity=false>]
2015-07-22 21:17:06,431 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dsperf   IP=<IP> OPERATION=addToClusterNodeLabels        TARGET=AdminService     RESULT=SUCCESS
 {code}

Also since exclusive=true to false is not supported success is misleading","Joy, Sadness","1, -1"
YARN-403,"{code:xml}
2013-02-09 22:59:47,490 WARN org.apache.hadoop.mapred.ShuffleHandler: Shuffle failure 
java.io.IOException: Verification of the hashReply failed
	at org.apache.hadoop.mapreduce.security.SecureShuffleUtils.verifyReply(SecureShuffleUtils.java:98)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.verifyRequest(ShuffleHandler.java:436)
	at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:383)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{code}",,
YARN-4109,"Configure node label and load scheduler Page
On each reload of the page the below exception gets thrown in logs


{code}
2015-09-03 11:27:08,544 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/scheduler
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
	at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
	at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:139)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:663)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:291)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:615)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1211)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error rendering block: nestLevel=10 expected 5
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:71)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
	at org.apache.hadoop.yarn.webapp.View.render(View.java:235)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)
	at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
	at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)
	at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)
	at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.scheduler(RmController.java:82)
	... 47 more

{code}",,
YARN-4152,"NM crash during of log aggregation.
Ran Pi job with 500 container and killed application in between

*Logs*
{code}
2015-09-12 18:44:25,597 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_e51_1442063466801_0001_01_000099 is : 143
2015-09-12 18:44:25,670 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Event EventType: KILL_CONTAINER sent to absent container container_e51_1442063466801_0001_01_000101
2015-09-12 18:44:25,670 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_e51_1442063466801_0001_01_000101 from application application_1442063466801_0001
2015-09-12 18:44:25,670 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
        at java.lang.Thread.run(Thread.java:745)
2015-09-12 18:44:25,692 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1442063466801_0001
2015-09-12 18:44:25,692 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2015-09-12 18:44:25,692 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=dsperf       OPERATION=Container Finished - Succeeded        TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1442063466801_0001    CONTAINERID=container_e51_1442063466801_0001_01_000100

{code}

*Analysis*

Looks like for absent container also {{stopContainer}} is called 
{code}
      case CONTAINER_FINISHED:
        LogHandlerContainerFinishedEvent containerFinishEvent =
            (LogHandlerContainerFinishedEvent) event;
        stopContainer(containerFinishEvent.getContainerId(),
            containerFinishEvent.getExitCode());
        break;
{code}

*Event EventType: KILL_CONTAINER sent to absent container container_e51_1442063466801_0001_01_000101*

Should skip when {{null==context.getContainers().get(containerId)}} ",,
YARN-4167,"Configure {{yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs}} mismatching with {{yarn.nm.liveness-monitor.expiry-interval-ms}}

On startup NPE is thrown on {{RMActiveServices#serviceStop}}

{noformat}
2015-09-16 12:23:29,504 INFO org.apache.hadoop.service.AbstractService: Service RMActiveServices failed in state INITED; cause: java.lang.IllegalArgumentException: yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs should be more than 3 X yarn.nm.liveness-monitor.expiry-interval-ms
java.lang.IllegalArgumentException: yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs should be more than 3 X yarn.nm.liveness-monitor.expiry-interval-ms
 at org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager.<init>(RMContainerTokenSecretManager.java:82)
 at org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService.createContainerTokenSecretManager(RMSecretManagerService.java:109)
 at org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService.<init>(RMSecretManagerService.java:57)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createRMSecretManagerService(ResourceManager.java:1111)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:423)
 at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:963)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:256)
 at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1193)
2015-09-16 12:23:29,507 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error closing store.
java.lang.NullPointerException
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:608)
 at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
 at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
 at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
 at org.apache.hadoop.service.AbstractService.init(AbstractService.java:171)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:963)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:256)
 at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
 at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1193
{noformat}

*Impact Area*: RM failover with wrong configuration",,
YARN-42,"NM throws NPE on startup if it doesn't have persmission's on nm local dir's


{code:xml}
2012-05-14 16:32:13,468 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager
org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)
	at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)
	at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)
Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed
	at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)
	at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)
	at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)
	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)
	at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)
	at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2325)
	at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)
	... 6 more
2012-05-14 16:32:13,472 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)
	at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}
",,
YARN-4235,"We see NPE if empty groups are returned for a user. This causes a NPE and cause RM to crash as below

{noformat}
2015-09-22 16:51:52,780  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ADDED to the scheduler
java.lang.IndexOutOfBoundsException: Index: 0
	at java.util.Collections$EmptyList.get(Collections.java:3212)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)
	at java.lang.Thread.run(Thread.java:745)
2015-09-22 16:51:52,797  INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye..
{noformat}",,
YARN-4288,"When NM get restarted, NodeStatusUpdaterImpl will try to register to RM with RPC which could throw following exceptions when RM get restarted at the same time, like following exception shows:
{noformat}
2015-08-17 14:35:59,434 ERROR nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:rebootNodeStatusUpdaterAndRegisterWithRM(222)) - Unexpected error rebooting NodeStatusUpdater
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ""172.27.62.28""; destination host is: ""172.27.62.57"":8025;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        at org.apache.hadoop.ipc.Client.call(Client.java:1400)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)
2015-08-17 14:35:59,436 FATAL nodemanager.NodeManager (NodeManager.java:run(307)) - Error while rebooting NodeStatusUpdater.
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ""172.27.62.28""; destination host is: ""172.27.62.57"":8025;
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:223)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ""ebdp-ch2-172.27.62.28""; destination host is: ""172.27.62.57"":8025;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
        at org.apache.hadoop.ipc.Client.call(Client.java:1473)
        at org.apache.hadoop.ipc.Client.call(Client.java:1400)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)
        ... 1 more
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)
2015-08-17 14:35:59,445 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042
2015-08-17 14:35:59,547 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(512)) - Applications still running : [application_1439417357296_45357, application_1439417357296_45403, application_1439417357296_45355, application_1439417357296_45111, application_1439417357296_45452, application_1439417357296_45350, application_1439417357296_45499, application_1439417357296_45205, application_1439417357296_21009]
2015-08-17 14:35:59,548 INFO  ipc.Server (Server.java:stop(2469)) - Stopping server on 45454
2015-08-17 14:35:59,551 INFO  ipc.Server (Server.java:run(717)) - Stopping IPC Server listener on 45454
2015-08-17 14:35:59,551 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceStop(141)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit
2015-08-17 14:35:59,552 INFO  ipc.Server (Server.java:run(843)) - Stopping IPC Server Responder
{noformat}
It will make NM restart get failed. We should have a simple fix to allow this register to RM can retry with connection failures.",,
YARN-4321,"This applies to only branch-2.7 or earlier code.
When a {{NoAuthException}} is thrown in non HA mode(like in the scenario of YARN-4127), RM incessantly keeps on retrying the ZK operation.

{noformat}
2015-10-23 09:22:10,209 DEBUG [SyncThread:0] server.DataTree (DataTree.java:processTxn(949)) - Ignoring processTxn failure hdr: -1 : error: -102
2015-10-23 09:22:10,210 DEBUG [main-SendThread(127.0.0.1:11221)] zookeeper.ClientCnxn (ClientCnxn.java:readResponse(818)) - Reading reply sessionid:0x15092d1ebe10001, packet:: clientPath:null serverPath:null finished:false header:: 7591,1  replyHeader:: 7591,7610,-102  request:: '/rmstore/ZKRMStateRoot/RMAppRoot,,v{s{31,s{'world,'anyone}}},0  response::
2015-10-23 09:22:10,210 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest(645)) - Got user-level KeeperException when processing sessionid:0x15092d1ebe10001 type:create cxid:0x1da8 zxid:0x1dbb txntype:-1 reqpath:n/a Error Path:null Error:KeeperErrorCode = NoAuth
{noformat}

This is because we do not handle NoAuthException properly in branch-2.7 code when HA is not enabled.
In {{ZKRMStateStore#runWithRetries}}, we have code as under. As can be seen if HA is not enabled, we neither rethrow NoAuthException nor do we have any logic to increment retries and back out if retries are maxed out.
{code}
 T runWithRetries() throws Exception {
      int retry = 0;
      while (true) {
        try {
          return runWithCheck();
        } catch (KeeperException.NoAuthException nae) {
          if (HAUtil.isHAEnabled(getConfig())) {
            // NoAuthException possibly means that this store is fenced due to
            // another RM becoming active. Even if not,
            // it is safer to assume we have been fenced
            throw new StoreFencedException();
          }
        } catch (KeeperException ke) {
          .............
       }
     }
  }
{code}",Sadness,-1
YARN-4326,"The timeout originates in ApplicationMaster, where it fails to connect to timeline server, and retry exceeds limits:

{code}
2015-11-02 21:57:38,066 INFO  [main] impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(299)) - Timeline service address: http://mdinglin02:0/ws/v1/timeline/
2015-11-02 21:57:38,099 INFO  [main] impl.TimelineClientImpl (TimelineClientImpl.java:logException(213)) - Exception caught by TimelineClientConnectionRetry, will try 30 more time(s).
...
...
java.lang.RuntimeException: Failed to connect to timeline server. Connection retries limit exceeded. The posted timeline event may be missing
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:206)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter.handle(TimelineClientImpl.java:245)
        at com.sun.jersey.api.client.Client.handle(Client.java:648)
        at com.sun.jersey.api.client.WebResource.handle(WebResource.java:670)
        at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
        at com.sun.jersey.api.client.WebResource$Builder.post(WebResource.java:563)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPostingObject(TimelineClientImpl.java:477)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$1.run(TimelineClientImpl.java:326)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$1.run(TimelineClientImpl.java:323)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1669)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.doPosting(TimelineClientImpl.java:323)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:308)
        at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.publishApplicationAttemptEvent(ApplicationMaster.java:1184)
        at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.run(ApplicationMaster.java:571)
        at org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.main(ApplicationMaster.java:302)
{code}",,
YARN-4347,"Resource manager fails with NPE while trying to load or recover a finished application. 
{code}
2015-11-11 17:53:22,351 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(597)) - Failed to load/recover state
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
{code}",,
YARN-4392,"{code}2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437453994768 is ahead of started time 1440308399674 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437454008244 is ahead of started time 1440308399676 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444305171 is ahead of started time 1440308399653 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444293115 is ahead of started time 1440308399647 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444379645 is ahead of started time 1440308399656 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444361234 is ahead of started time 1440308399655 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444342029 is ahead of started time 1440308399654 
2015-09-01 12:39:09,852 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444323447 is ahead of started time 1440308399654 
2015-09-01 12:39:09,853 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444430006 is ahead of started time 1440308399660 
2015-09-01 12:39:09,853 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444415698 is ahead of started time 1440308399659 
2015-09-01 12:39:09,853 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444419060 is ahead of started time 1440308399658 
2015-09-01 12:39:09,853 WARN util.Times (Times.java:elapsed(53)) - Finished time 1437444393931 is ahead of started time 1440308399657
{code} . 

From ATS logs, we would see a large amount of 'stale alerts' messages periodically",,
YARN-4402,"
https://builds.apache.org/job/Hadoop-Yarn-trunk/1465/testReport/

{noformat}
2015-12-01 04:56:07,150 INFO  [main] http.HttpServer2 (HttpServer2.java:start(846)) - HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:8042
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:906)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:843)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:306)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:73)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStart(NodeManager.java:368)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testContainerPreservationOnResyncImpl(TestNodeManagerResync.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync.testKillContainersOnResync(TestNodeManagerResync.java:141)
{noformat}",,
YARN-4424,"{code}
yarn@XXX:/mnt/hadoopqe$ /usr/hdp/current/hadoop-yarn-client/bin/yarn application -list -appStates NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING
15/12/04 21:59:54 INFO impl.TimelineClientImpl: Timeline service address: http://XXX:8188/ws/v1/timeline/
15/12/04 21:59:54 INFO client.RMProxy: Connecting to ResourceManager at XXX/0.0.0.0:8050
15/12/04 21:59:55 INFO client.AHSProxy: Connecting to Application History server at XXX/0.0.0.0:10200
{code}

{code:title=RM log}
2015-12-04 21:59:19,744 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 237000
2015-12-04 22:00:50,945 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 238000
2015-12-04 22:02:22,416 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 239000
2015-12-04 22:03:53,593 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 240000
2015-12-04 22:05:24,856 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 241000
2015-12-04 22:06:56,235 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 242000
2015-12-04 22:08:27,510 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 243000
2015-12-04 22:09:58,786 INFO  event.AsyncDispatcher (AsyncDispatcher.java:handle(243)) - Size of event-queue is 244000
{code}
",,
YARN-4431,"{noformat}
2015-12-07 12:16:57,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 12:16:58,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-07 12:16:58,876 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Unregistration of the Node 10.200.10.53:25454 failed.
java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
        at org.apache.hadoop.ipc.Client.call(Client.java:1452)
        at org.apache.hadoop.ipc.Client.call(Client.java:1385)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy74.unRegisterNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)
        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:255)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
        at com.sun.proxy.$Proxy75.unRegisterNodeManager(Unknown Source)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)
        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)
        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:377)
{noformat}
If RM down for some reason, NM's NodeStatusUpdaterImpl will retry the connection with proper retry policy. After retry the maximum times (15 minutes by default), it will send NodeManagerEventType.SHUTDOWN to shutdown NM. But NM shutdown will call NodeStatusUpdaterImpl.serviceStop() which will call unRegisterNM() to unregister NM from RM and get retry again (another 15 minutes). This is completely unnecessary and we should skip unRegisterNM when NM get shutdown because of connection issues.",Sadness,-1
YARN-4530,"In our cluster, I found that LocalizedResource download failed trigger a NPE Cause the NodeManager shutdown.

{noformat}
2015-12-29 17:18:33,706 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://ns3:8020/user/username/projects/user_insight/lookalike/oozie/workflow/conf/hive-site.xml transitioned from DOWNLOADING to FAILED
2015-12-29 17:18:33,708 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Downloading public rsrc:{ hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/user_insight_pig_udf-0.0.1-SNAPSHOT-jar-with-dependencies.jar, 1451380519635, FILE, null }
2015-12-29 17:18:33,710 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Failed to download rsrc { { hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar, 1451380519452, FILE, null },pending,[(container_1451039893865_261670_01_000578)],42332661980495938,DOWNLOADING}
java.io.IOException: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar changed on src filesystem (expected 1451380519452, was 1451380611793
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:50)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-12-29 17:18:33,710 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar transitioned from DOWNLOADING to FAILED
2015-12-29 17:18:33,710 FATAL org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Error: Shutting down
java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)
2015-12-29 17:18:33,710 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Public cache exiting
{noformat}",,
YARN-4598,"In our cluster, I found that the container has some problems in state transitionthis is my log
{noformat}
2016-01-12 17:42:50,088 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1452588902899_0001_01_000087 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
2016-01-12 17:42:50,088 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Can't handle this event at current state: Current: 
[CONTAINER_CLEANEDUP_AFTER_KILL], eventType: [RESOURCE_FAILED]
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL                                       
    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)                                                                  
    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)                                                                     
    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)                                             
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)                                           
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)                                             
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)              
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)              
    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)                                                                              
    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)                                                                                 
    at java.lang.Thread.run(Thread.java:744)                                                                                                                        
2016-01-12 17:42:50,089 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1452588902899_0001_01_000094 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to null
2016-01-12 17:42:50,089 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=hadoop   OPERATION=Container Finished - Killed   TARGET=ContainerImpl    
RESULT=SUCCESS  APPID=application_1452588902899_0001    CONTAINERID=container_1452588902899_0001_01_000094                                                          
2016-01-12 17:42:50,089 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1452588902899_0001_01_000094 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE 
{noformat}",,
YARN-4709,"Following exception is thrown when we run below command.
{panel}
root@varun-Inspiron-5558:/opt1/hadoop3/bin# ./yarn logs -applicationId application_1455999168135_0002 -am ALL -logFiles ALL


Container: container_e31_1455999168135_0002_01_000001
=======================================================
{color:red}LogType:syslogstderrstdout
Log Upload Time:Sun Feb 21 01:44:55 +0530 2016
Log Contents:
java.lang.Exception: Cannot find this log on the local disk.
End of LogType:syslogstderrstdout{color}
LogType:syslog
Log Upload Time:Sun Feb 21 01:44:55 +0530 2016
Log Contents:
2016-02-21 01:44:49,565 INFO \[main\] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1455999168135_0002_000001
2016-02-21 01:44:49,914 INFO \[main\] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: 
/************************************************************
{panel}

This is because we annotate containerLogFiles list with XmlElementWrapper which generates XML output as under. And when we read this XML at client side, reading the value associated with containerLogFiles also leads to one value being syslogstderrstdout because both parent and child tags are same. This leads to the exception. 
{noformat}
<containerLogFiles>
  <containerLogFiles>syslog</containerLogFiles>
  <containerLogFiles>stderr</containerLogFiles>
  <containerLogFiles>stdout</containerLogFiles>
</containerLogFiles>
{noformat}

Moreover, as we use XMLElementWrapper, the JSON generated is as under. This JSON cannot be properly parsed by JSON parser(as a list). This is because child containerLogsFiles entries are treated as a key-value pair(map) and hence only last entry i.e. stdout is picked up. This was found while working on YARN-4517. This makes output unusable. 
This will be an issue for 2 REST endpoints i.e. {{/ws/v1/node/containers}} and {{/ws/v1/node/containers/\{\{containerId\}\}}}
{noformat}
  ""containerLogFiles"":[
    {
      ""containerLogFiles"":""syslog"",
      ""containerLogFiles"":""stderr"",
      ""containerLogFiles"":""stdout""
    }
  ]
{noformat}

Ideally the JSON output should be as under.
{noformat}
""containerLogFiles"":[""syslog"",""stderr"",""stdout""]
{noformat}

We can indicate in the JAXB context to ignore the outer wrapper while marshalling to JSON. But this can only be done at class level. If we do so for ContainerInfo, it would break backward compatibility.
Hence, to fix it we can remove XmlElementWrapper annotation for containerLogFiles list.
Another solution would be to wrap the list inside another class.

But going with former as of now as we do not specify XmlElementWrapper for lists at most of the places in our code.","Sadness, Surprise",-1
YARN-4743,"{code}
2016-02-26 14:08:50,821 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.IllegalArgumentException: Comparison method violates its general contract!
         at java.util.TimSort.mergeHi(TimSort.java:868)
         at java.util.TimSort.mergeAt(TimSort.java:485)
         at java.util.TimSort.mergeCollapse(TimSort.java:410)
         at java.util.TimSort.sort(TimSort.java:214)
         at java.util.TimSort.sort(TimSort.java:173)
         at java.util.Arrays.sort(Arrays.java:659)
         at java.util.Collections.sort(Collections.java:217)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)
         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)
         at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)
         at java.lang.Thread.run(Thread.java:745)
2016-02-26 14:08:50,822 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye..
{code}

Actually, this bug found in 2.6.0-cdh5.4.7. {{FairShareComparator}} is not transitive.

We get NaN when memorySize=0 and weight=0.
{code:title=FairSharePolicy.java}
useToWeightRatio1 = s1.getResourceUsage().getMemorySize() /
  s1.getWeights().getWeight(ResourceType.MEMORY)
{code}
",,
YARN-476,"ProcfsBasedProcessTree has a habit of emitting not-so-helpful messages such as the following:

{noformat}
2013-03-13 12:41:51,957 INFO [communication thread] org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: The process 28747 may have finished in the interim.
2013-03-13 12:41:51,958 INFO [communication thread] org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: The process 28978 may have finished in the interim.
2013-03-13 12:41:51,958 INFO [communication thread] org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: The process 28979 may have finished in the interim.
{noformat}

As described in MAPREDUCE-4570, this is something that naturally occurs in the process of monitoring processes via procfs.  It's uninteresting at best and can confuse users who think it's a reason their job isn't running as expected when it appears in their logs.

We should either make this DEBUG or remove it entirely.",Sadness,-1
YARN-4762,"Seeing this exception and the NMs crash.
{code}
2016-03-03 16:47:57,807 DEBUG org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService is started
2016-03-03 16:47:58,027 DEBUG org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: checkLinuxExecutorSetup: [/hadoop/hadoop-yarn-nodemanager/bin/container-executor, --checksetup]
2016-03-03 16:47:58,043 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: Mount point Based on mtab file: /proc/mounts. Controller mount point not writable for: cpu
2016-03-03 16:47:58,043 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime: Unable to get cgroups handle.
2016-03-03 16:47:58,044 DEBUG org.apache.hadoop.service.AbstractService: noteFailure org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor
2016-03-03 16:47:58,044 INFO org.apache.hadoop.service.AbstractService: Service NodeManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)
Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)
        ... 3 more
2016-03-03 16:47:58,047 DEBUG org.apache.hadoop.service.AbstractService: Service: NodeManager entered state STOPPED
2016-03-03 16:47:58,047 DEBUG org.apache.hadoop.service.CompositeService: NodeManager: stopping services, size=0
2016-03-03 16:47:58,047 DEBUG org.apache.hadoop.service.AbstractService: Service: org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService entered state STOPPED
2016-03-03 16:47:58,047 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)
Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)
        ... 3 more
{code}",,
YARN-4763,"{noformat}
=application_1457010932347_0121
2016-03-04 10:16:27,016 INFO org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: application_1457010932347_0121 found existing hdfs token Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hacluster, Ident: (HDFS_DELEGATION_TOKEN token 128 for yarn with renewer yarn)
2016-03-04 10:16:27,029 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/apps
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)
        at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:235)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:235)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)
        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)
        at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)

{noformat}
Application state is NEW the apptempts can be empty as per inital analysis
{noformat}
rm.getRMContext().getRMApps()
          .get(appAttemptId.getApplicationId()).getAppAttempts()
          .get(appAttemptId)
{noformat}
",,
YARN-4831,"{code}
2016-03-04 19:43:48,130 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1456335621285_0040_01_000066 transitioned from NEW to DONE
2016-03-04 19:43:48,130 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=henkins-service	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1456335621285_0040
{code}",,
YARN-4880,"While going throw TestZKRMStateStorePerf class , found that we are not initializing variable {{TestingServer curatorTestingServer}} if real zookeeper cluster are passed to utility.  But down the line , this variables are used which causes NPE

I tested by passing program arguments which result in NPE
{noformat} 
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/C:/Users/r00902292/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/C:/Users/r00902292/.m2/repository/ch/qos/logback/logback-classic/1.1.2/logback-classic-1.1.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2016-03-26 14:47:50,937 INFO  [main] recovery.TestZKRMStateStore (TestZKRMStateStorePerf.java:run(119)) - Starting ZKRMStateStorePerf ver.0.1
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStorePerf.initStore(TestZKRMStateStorePerf.java:102)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStorePerf.run(TestZKRMStateStorePerf.java:156)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStorePerf.main(TestZKRMStateStorePerf.java:273)
{noformat}

There are 2 places variable {{curatorTestingServer}} used that need to be guarded with null check.",Surprise,-1
YARN-4882,"I think for recovering completed applications no need to log as INFO, rather it can be made it as DEBUG.  The problem seen from large cluster is if any issue happens during RM start up and continuously switching , then  RM logs are filled with most with recovering applications only. 
There are 6 lines are logged for 1 applications as I shown in below logs, then consider RM default value for max-completed applications is 10K. So for each switch 10K*6=60K lines will be added which is not useful I feel.
{noformat}
2016-03-01 10:20:59,077 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Default priority level is set to application:application_1456298208485_21507
2016-03-01 10:20:59,094 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Recovering app: application_1456298208485_21507 with 1 attempts and final state = FINISHED
2016-03-01 10:20:59,100 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Recovering attempt: appattempt_1456298208485_21507_000001 with final state: FINISHED
2016-03-01 10:20:59,107 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1456298208485_21507_000001 State change from NEW to FINISHED
2016-03-01 10:20:59,111 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1456298208485_21507 State change from NEW to FINISHED
2016-03-01 10:20:59,112 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=rohith   OPERATION=Application Finished - Succeeded      TARGET=RMAppManager     RESULT=SUCCESS  APPID=application_1456298208485_21507
{noformat}

The main problem is missing important information's from the logs before RM unstable. Even though log roll back is 50 or 100, in a short period all these logs will be rolled out and all the logs contains only RM switching information that too recovering applications!!. 

I suggest at least completed applications recovery should be logged as DEBUG.","Fear, Sadness, Surprise",-1
YARN-4984,"Due to YARN-4325, many stale applications still exists in NM state store and get recovered after NM restart. The app initiation will get failed due to token invalid, but exception is swallowed and aggregator thread is still created for invalid app.

Exception is:
{noformat}
158 2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448        060878692_11842
    159 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be fo        und in cache
    160         at org.apache.hadoop.ipc.Client.call(Client.java:1427)
    161         at org.apache.hadoop.ipc.Client.call(Client.java:1358)
    162         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
    163         at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)
    164         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
    165         at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)
    166         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    167         at java.lang.reflect.Method.invoke(Method.java:606)
    168         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
    169         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    170         at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
    171         at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)
    172         at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)
    173         at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)
    174         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    175         at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)
    176         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)
    177         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)
    178         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)
    179         at java.security.AccessController.doPrivileged(Native Method)
    180         at javax.security.auth.Subject.doAs(Subject.java:415)
    181         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
    182         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)
    183         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)
    184         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)
    185         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)
    186         at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)

{noformat}",Surprise,-1
YARN-5098,"Environment : HA cluster

Yarn application logs for long running application could not be gathered because Nodemanager failed to talk to HDFS with below error.
{code}
2016-05-16 18:18:28,533 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(555)) - Application just finished : application_1463170334122_0002
2016-05-16 18:18:28,545 WARN  ipc.Client (Client.java:run(705)) - Exception encountered while connecting to the server :
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache
        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)
        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)
        at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)
        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)
        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)
        at org.apache.hadoop.ipc.Client$Connection.access$3100(Client.java:398)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)
        at org.apache.hadoop.ipc.Client.call(Client.java:1439)
        at org.apache.hadoop.ipc.Client.call(Client.java:1386)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)
        at com.sun.proxy.$Proxy83.getServerDefaults(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy84.getServerDefaults(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)
        at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)
        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)
        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)
{code}",,
YARN-5136,"move app cause rm exit
{noformat}
2016-05-24 23:20:47,202 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler
java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)
    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)
    at java.lang.Thread.run(Thread.java:745)
2016-05-24 23:20:47,202 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1464073905025_15410_01_001759 Container Transitioned from ACQUIRED to RELEASED
2016-05-24 23:20:47,202 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye..
{noformat}",,
YARN-5379,"The {{TestHBaseTimelineStorage. testWriteApplicationToHBase()}} test seems to fail intermittently:
{noformat}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorage.testWriteApplicationToHBase(TestHBaseTimelineStorage.java:817)
{noformat}

The stdout output:
{noformat}
2016-07-13 00:15:48,883 INFO  [main] zookeeper.RecoverableZooKeeper (RecoverableZooKeeper.java:<init>(120)) - Process identifier=hconnection-0x2b7962a2 connecting to ZooKeeper ensemble=localhost:53474
2016-07-13 00:15:48,883 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=localhost:53474 sessionTimeout=90000 watcher=hconnection-0x2b7962a20x0, quorum=localhost:53474, baseZNode=/hbase
2016-07-13 00:15:48,886 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server localhost/127.0.0.1:53474. Will not attempt to authenticate using SASL (unknown error)
2016-07-13 00:15:48,887 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to localhost/127.0.0.1:53474, initiating session
2016-07-13 00:15:48,887 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:38097
2016-07-13 00:15:48,887 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:38097
2016-07-13 00:15:48,896 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x155e19baa520025 with negotiated timeout 40000 for client /127.0.0.1:38097
2016-07-13 00:15:48,896 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server localhost/127.0.0.1:53474, sessionid = 0x155e19baa520025, negotiated timeout = 40000
2016-07-13 00:15:48,911 INFO  [main] zookeeper.RecoverableZooKeeper (RecoverableZooKeeper.java:<init>(120)) - Process identifier=hconnection-0x32130e61 connecting to ZooKeeper ensemble=localhost:53474
2016-07-13 00:15:48,912 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=localhost:53474 sessionTimeout=90000 watcher=hconnection-0x32130e610x0, quorum=localhost:53474, baseZNode=/hbase
2016-07-13 00:15:48,917 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server localhost/127.0.0.1:53474. Will not attempt to authenticate using SASL (unknown error)
2016-07-13 00:15:48,918 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:38098
2016-07-13 00:15:48,921 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to localhost/127.0.0.1:53474, initiating session
2016-07-13 00:15:48,921 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:38098
2016-07-13 00:15:48,929 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x155e19baa520026 with negotiated timeout 40000 for client /127.0.0.1:38098
2016-07-13 00:15:48,929 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server localhost/127.0.0.1:53474, sessionid = 0x155e19baa520026, negotiated timeout = 40000
2016-07-13 00:15:48,938 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(541)) - closing the entity table
2016-07-13 00:15:48,938 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(546)) - closing the app_flow table
2016-07-13 00:15:48,938 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(551)) - closing the application table
2016-07-13 00:15:48,941 INFO  [RpcServer.reader=1,bindAddress=2588a1932efe,port=37493] hbase.Server (RpcServer.java:processConnectionHeader(1678)) - Connection from 172.17.0.3 port: 35467 with version info: version: ""1.1.3"" url: ""git://diocles.local/Volumes/hbase-1.1.3RC1/hbase"" revision: ""72bc50f5fafeb105b2139e42bbee3d61ca724989"" user: ""ndimiduk"" date: ""Sat Jan 16 18:31:39 PST 2016"" src_checksum: ""72c910d76fba182fa4e3c7e048275ca3""
2016-07-13 00:15:48,950 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(555)) - closing the flow run table
2016-07-13 00:15:48,957 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(560)) - closing the flowActivityTable table
2016-07-13 00:15:48,957 INFO  [main] storage.HBaseTimelineWriterImpl (HBaseTimelineWriterImpl.java:serviceStop(565)) - closing the hbase Connection
2016-07-13 00:15:48,958 INFO  [main] client.ConnectionManager$HConnectionImplementation (ConnectionManager.java:closeZooKeeperWatcher(1677)) - Closing zookeeper sessionid=0x155e19baa520026
2016-07-13 00:15:48,958 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x155e19baa520026
2016-07-13 00:15:48,968 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x155e19baa520026 closed
2016-07-13 00:15:48,968 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
2016-07-13 00:15:48,968 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:38098 which had sessionid 0x155e19baa520026
2016-07-13 00:15:48,979 INFO  [main] zookeeper.RecoverableZooKeeper (RecoverableZooKeeper.java:<init>(120)) - Process identifier=hconnection-0x4596f8f3 connecting to ZooKeeper ensemble=localhost:53474
2016-07-13 00:15:48,979 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:<init>(438)) - Initiating client connection, connectString=localhost:53474 sessionTimeout=90000 watcher=hconnection-0x4596f8f30x0, quorum=localhost:53474, baseZNode=/hbase
2016-07-13 00:15:48,980 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server localhost/127.0.0.1:53474. Will not attempt to authenticate using SASL (unknown error)
2016-07-13 00:15:48,981 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to localhost/127.0.0.1:53474, initiating session
2016-07-13 00:15:48,981 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.NIOServerCnxnFactory (NIOServerCnxnFactory.java:run(197)) - Accepted socket connection from /127.0.0.1:38100
2016-07-13 00:15:48,981 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.ZooKeeperServer (ZooKeeperServer.java:processConnectRequest(868)) - Client attempting to establish new session at /127.0.0.1:38100
2016-07-13 00:15:48,987 INFO  [SyncThread:0] server.ZooKeeperServer (ZooKeeperServer.java:finishSessionInit(617)) - Established session 0x155e19baa520027 with negotiated timeout 40000 for client /127.0.0.1:38100
2016-07-13 00:15:48,988 INFO  [main-SendThread(localhost:53474)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server localhost/127.0.0.1:53474, sessionid = 0x155e19baa520027, negotiated timeout = 40000
2016-07-13 00:15:48,992 INFO  [RpcServer.reader=2,bindAddress=2588a1932efe,port=37493] hbase.Server (RpcServer.java:processConnectionHeader(1678)) - Connection from 172.17.0.3 port: 35470 with version info: version: ""1.1.3"" url: ""git://diocles.local/Volumes/hbase-1.1.3RC1/hbase"" revision: ""72bc50f5fafeb105b2139e42bbee3d61ca724989"" user: ""ndimiduk"" date: ""Sat Jan 16 18:31:39 PST 2016"" src_checksum: ""72c910d76fba182fa4e3c7e048275ca3""
2016-07-13 00:15:49,001 INFO  [RpcServer.reader=3,bindAddress=2588a1932efe,port=37493] hbase.Server (RpcServer.java:processConnectionHeader(1678)) - Connection from 172.17.0.3 port: 35472 with version info: version: ""1.1.3"" url: ""git://diocles.local/Volumes/hbase-1.1.3RC1/hbase"" revision: ""72bc50f5fafeb105b2139e42bbee3d61ca724989"" user: ""ndimiduk"" date: ""Sat Jan 16 18:31:39 PST 2016"" src_checksum: ""72c910d76fba182fa4e3c7e048275ca3""
2016-07-13 00:15:49,010 INFO  [main] storage.HBaseTimelineReaderImpl (HBaseTimelineReaderImpl.java:serviceStop(64)) - closing the hbase Connection
2016-07-13 00:15:49,010 INFO  [main] client.ConnectionManager$HConnectionImplementation (ConnectionManager.java:closeZooKeeperWatcher(1677)) - Closing zookeeper sessionid=0x155e19baa520025
2016-07-13 00:15:49,011 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor (PrepRequestProcessor.java:pRequest2Txn(494)) - Processed session termination for sessionid: 0x155e19baa520025
2016-07-13 00:15:49,021 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x155e19baa520025 closed
2016-07-13 00:15:49,021 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:53474] server.NIOServerCnxn (NIOServerCnxn.java:closeSock(1007)) - Closed socket connection for client /127.0.0.1:38097 which had sessionid 0x155e19baa520025
2016-07-13 00:15:49,021 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down
{noformat}
",Surprise,-1
YARN-5545,"Issues as part of Max apps in Capacity scheduler:
1. Cap total applications across the queue hierarchy based on existing max app calculation
2. Introduce a new configuration to take default max apps per queue irrespective of the queue capacity configuration
3. When the capacity configuration of the default partition is ZERO but queue has capacity for other partition then app is not getting submitted, though app is submitted in other partition

Steps to reproduce Issue 3 : 

Configure capacity scheduler 
yarn.scheduler.capacity.root.default.capacity=0
yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50
yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50


Submit application as below

./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1

{noformat}
2016-08-21 18:21:31,375 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1471670113386_0001
java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001
	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)
...
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)
	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)
	... 25 more
{noformat}
",Surprise,-1
YARN-5594,"We've got that error after upgrade cluster from v.2.5.1 to 2.7.0.
{noformat}
2016-08-25 17:20:33,293 ERROR
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to
load/recover state
com.google.protobuf.InvalidProtocolBufferException: Protocol message contained
an invalid tag (zero).
at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)
at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)
at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)
at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)
at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)
at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)
at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)
at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)
at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)
at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044
{noformat}
The reason of this problem is that we use different formats of files /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMDTSecretManagerRoot/RMDelegationToken* in these hadoop versions.

This fix handle old data format during RM recover if InvalidProtocolBufferException occures.",,
YARN-5837,"If you decommission a node, the {{yarn node}} command shows it like this:
{noformat}
>> bin/yarn node -list -all
2016-11-04 08:54:37,169 INFO client.RMProxy: Connecting to ResourceManager at 0.0.0.0/0.0.0.0:8032
Total Nodes:1
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
192.168.1.69:57560	 DECOMMISSIONED	192.168.1.69:8042	                           0
{noformat}
And a full report like this:
{noformat}
>> bin/yarn node -status 192.168.1.69:57560
2016-11-04 08:55:08,928 INFO client.RMProxy: Connecting to ResourceManager at 0.0.0.0/0.0.0.0:8032
Node Report :
	Node-Id : 192.168.1.69:57560
	Rack : /default-rack
	Node-State : DECOMMISSIONED
	Node-Http-Address : 192.168.1.69:8042
	Last-Health-Update : Fri 04/Nov/16 08:53:58:802PDT
	Health-Report :
	Containers : 0
	Memory-Used : 0MB
	Memory-Capacity : 8192MB
	CPU-Used : 0 vcores
	CPU-Capacity : 8 vcores
	Node-Labels :
	Resource Utilization by Node :
	Resource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0
{noformat}

If you then restart the ResourceManager, you get this report:
{noformat}
>> bin/yarn node -list -all
2016-11-04 08:57:18,512 INFO client.RMProxy: Connecting to ResourceManager at 0.0.0.0/0.0.0.0:8032
Total Nodes:4
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
 192.168.1.69:-1	 DECOMMISSIONED	  192.168.1.69:-1	                           0
{noformat}
And when you try to get the full report on the now ""-1"" node, you get an NPE:
{noformat}
>> bin/yarn node -status 192.168.1.69:-1
2016-11-04 08:57:57,385 INFO client.RMProxy: Connecting to ResourceManager at 0.0.0.0/0.0.0.0:8032
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)
	at org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)
{noformat}",,
YARN-5873,"{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)
        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)
        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)
        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)
        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)
        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocator.assignContainers(ContainerAllocator.java:81)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)
        at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)
        at java.lang.Thread.run(Thread.java:745)
2016-11-12 14:22:07,153 INFO SchedulerEventDispatcher:Event Processor org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:79) org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye..
{noformat}",,
YARN-5918,"Allocate request failure during Opportunistic container allocation when nodemanager is lost 

{noformat}
2016-11-20 10:38:49,011 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root     OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1479637990302_0002    CONTAINERID=container_e12_1479637990302_0002_01_000006  RESOURCE=<memory:1024, vCores:1>
2016-11-20 10:38:49,011 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Removed node docker2:38297 clusterResource: <memory:4096, vCores:8>
2016-11-20 10:38:49,434 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 8030, call Call#35 Retry#0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate from 172.17.0.2:51584
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)
        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)
        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)
        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2539)
2016-11-20 10:38:50,824 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e12_1479637990302_0002_01_000002 Container Transitioned from RUNNING to COMPLETED

{noformat}",,
YARN-6054,"We encountered an issue recently where the TimelineServer failed to start because some state files went missing.

{code}
2016-11-21 20:46:43,134 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer failed in state INITED
; cause: org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelines
erver/leveldb-timeline-store.ldb/127897.sst
org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/lev
eldb-timeline-store.ldb/127897.sst

2016-11-21 20:46:43,135 FATAL org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: Error starting ApplicationHistoryServer
org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)
Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst
        at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)
        at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)
        at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)
        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        ... 5 more
2016-11-21 20:46:43,136 INFO org.apache.hadoop.util.ExitUtil: Exiting with status -1
{code}
Ideally we shouldn't have any missing state files. However I'd posit that the TimelineServer should have graceful degradation instead of failing to start at all.","Sadness, Surprise",-1
YARN-6068,"The exception log is as following:
{noformat}
2017-01-05 19:16:36,352 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(527)) - Aborting log aggregation for application_1483640789847_0001
2017-01-05 19:16:36,352 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(399)) - Aggregation did not complete for application application_1483640789847_0001
2017-01-05 19:16:36,353 WARN  application.ApplicationImpl (ApplicationImpl.java:handle(461)) - Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)
        at java.lang.Thread.run(Thread.java:745)
2017-01-05 19:16:36,355 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1483640789847_0001 transitioned from RUNNING to null
{noformat}",,
YARN-6072,"Resource manager is unable to start in secure mode

{code}
2017-01-08 14:27:29,917 INFO org.apache.hadoop.conf.Configuration: found resource hadoop-policy.xml at file:/opt/hadoop/release/hadoop-3.0.0-alpha2-SNAPSHOT/etc/hadoop/hadoop-policy.xml
2017-01-08 14:27:29,918 INFO org.apache.hadoop.yarn.server.resourcemanager.AdminService: Refresh All
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2017-01-08 14:27:29,919 ERROR org.apache.hadoop.yarn.server.resourcemanager.AdminService: RefreshAll failed so firing fatal event
org.apache.hadoop.ha.ServiceFailedException
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2017-01-08 14:27:29,920 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2017-01-08 14:27:29,948 WARN org.apache.hadoop.ha.ActiveStandbyElector: Exception handling the winning of election
org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
Caused by: org.apache.hadoop.ha.ServiceFailedException: Error on refreshAll during transition to Active
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:311)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)
        ... 4 more
Caused by: org.apache.hadoop.ha.ServiceFailedException
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)
        ... 5 more

{code}

ResourceManager services are added in following order
# EmbeddedElector
# AdminService

During resource manager service start() .EmbeddedElector starts first and invokes  {{AdminService#refreshAll()}} but {{AdminService#serviceStart()}} happens after {{ActiveStandbyElectorBasedElectorService}} service start is complete. So {{AdminService#server}} will be *null* which causes  {{AdminService#refreshAll()}}  to fail
{code}
      if (getConfig().getBoolean(
          CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,
          false)) {
        refreshServiceAcls();
      }
{code}",Sadness,-1
YARN-6102,"{code}2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread
java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)
        at java.lang.Thread.run(Thread.java:745)
2017-01-17 16:42:17,914 INFO  [AsyncDispatcher ShutDown handler] event.AsyncDispatcher (AsyncDispatcher.java:run(303)) - Exiting, bbye..{code}

The same stack i was also noticed in {{TestResourceTrackerOnHA}} exits abnormally, after some analysis, i was able to reproduce.

Once the nodeHeartBeat is sent to RM, inside {{org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService.nodeHeartbeat(NodeHeartbeatRequest)}}, before sending it to dispatcher through
{{this.rmContext.getDispatcher().getEventHandler().handle(nodeStatusEvent);}} if RM failover is called, the dispatcher is reset
The new dispatcher is however first started and then the events are registered at {{org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.reinitialize(boolean)}}

So event order will look like
1. Send Node heartbeat to {{ResourceTrackerService}}
2. In {{ResourceTrackerService.nodeHeartbeat}}, before passing to dispatcher call RM failover
3. In RM Failover, current active will reset dispatcher @reinitialize i.e ( {{resetDispatcher();}} + {{createAndInitActiveServices();}} )

Now between {{resetDispatcher();}} and {{createAndInitActiveServices();}} , the {{ResourceTrackerService.nodeHeartbeat}} invokes dipatcher

This will cause the above error as at point of time when {{STATUS_UPDATE}} event is given to dispatcher in {{ResourceTrackerService}} , the new dispatcher(from the failover) may be started but not yet registered for events
Using same steps(with pausing JVM at debug), i was able to reproduce this in production cluster also. for {{STATUS_UPDATE}} active service event, when the service is yet to forward the event to RM dispatcher but a failover is called and dispatcher reset is between {{resetDispatcher();}} & {{createAndInitActiveServices();}}",Surprise,-1
YARN-6117,"The webapp directory for the SharedCacheManager is missing and the SCM fails to start up with the following:
{noformat}
2017-01-22 00:14:25,162 INFO org.apache.hadoop.service.AbstractService: Service SharedCacheManager failed in state STARTED; cause: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:330)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:377)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:373)
        at org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer.serviceStart(SCMWebServer.java:65)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager.main(SharedCacheManager.java:157)
Caused by: java.io.FileNotFoundException: webapps/sharedcache not found in CLASSPATH
        at org.apache.hadoop.http.HttpServer2.getWebAppsPath(HttpServer2.java:972)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:478)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)
        at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:392)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:291)
        ... 7 more
{noformat}",,
YARN-6448,"YARN-4719 remove the lock in continuous scheduling while sorting nodes. It breaks the order in comparison if nodes changes while sorting.
{code}
2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!
java.lang.IllegalArgumentException: Comparison method violates its general contract!
        at java.util.TimSort.mergeHi(TimSort.java:899)
        at java.util.TimSort.mergeAt(TimSort.java:516)
        at java.util.TimSort.mergeForceCollapse(TimSort.java:457)
        at java.util.TimSort.sort(TimSort.java:254)
        at java.util.Arrays.sort(Arrays.java:1512)
        at java.util.ArrayList.sort(ArrayList.java:1454)
        at java.util.Collections.sort(Collections.java:175)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)
{code}",,
YARN-6534,"In a non-secured cluster, RM get failed consistently due to TimelineServiceV1Publisher tries to init TimelineClient with SSLFactory without any checking on if https get used.

{noformat}
2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager
org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)
        at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)
Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)
        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)
        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)
        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)
        at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)
        at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        ... 11 more
{noformat}
CC [~rohithsharma] and [~gtCarrera9]",,
YARN-6643,"We've seen various tests in {{TestRMFailover}} fail very rarely with a message like ""org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: ResourceManager failed to start. Final state is STOPPED"".  

After some digging, it turns out that it's due to a port conflict with the embedded ZooKeeper in the tests.  The embedded ZooKeeper uses {{ServerSocketUtil#getPort}} to choose a free port, but the RMs are configured to 10000 + <default-port> and 20000 + <default-port> (e.g. the default port for the RM is 8032, so you'd use 18032 and 28032).

When I was able to reproduce this, I saw that ZooKeeper was using port 18033, which is 10000 + 8033, the default RM Admin port.  It results in an error like this, causing the RM to be unable to start, and hence the original error message in the test failure:
{noformat}
2017-05-24 01:16:52,735 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service ResourceManager failed in state STARTED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:18033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:18033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
        at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:139)
        at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
        at org.apache.hadoop.yarn.ipc.YarnRPC.getServer(YarnRPC.java:54)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:171)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:158)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1147)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.MiniYARNCluster$2.run(MiniYARNCluster.java:310)
Caused by: java.net.BindException: Problem binding to [0.0.0.0:18033] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:720)
        at org.apache.hadoop.ipc.Server.bind(Server.java:482)
        at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:688)
        at org.apache.hadoop.ipc.Server.<init>(Server.java:2376)
        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1042)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:535)
        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:510)
        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:887)
        at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.createServer(RpcServerFactoryPBImpl.java:169)
        at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:132)
        ... 9 more
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:444)
        at sun.nio.ch.Net.bind(Net.java:436)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.apache.hadoop.ipc.Server.bind(Server.java:465)
        ... 17 more
2017-05-24 01:16:52,736 DEBUG service.AbstractService (AbstractService.java:enterState(452)) - Service: ResourceManager entered state STOPPED
{noformat}",Surprise,-1
YARN-6649,"When Using tez ui (makes REST api calls to timeline service REST api), some calls were coming back as 500 internal server error. The root cause was YARN-6654. This jira is to handle object decoding to prevent sending back internal server errors to the client and instead respond with a partial message instead.

{code}
2017-05-30 12:47:10,670 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000
	at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
	at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:636)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:294)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:588)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1352)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000
	at org.nustaq.serialization.util.FSTUtil.rethrow(FSTUtil.java:122)
	at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:879)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:414)
	at org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)
	at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)
	at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)
	at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:155)
	... 52 more
Caused by: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000
	at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)
	at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:877)
	... 58 more
Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000
	at org.nustaq.serialization.FSTClazzNameRegistry.decodeClass(FSTClazzNameRegistry.java:173)
	at org.nustaq.serialization.coders.FSTStreamDecoder.readClass(FSTStreamDecoder.java:431)
	at org.nustaq.serialization.FSTObjectInput.readClass(FSTObjectInput.java:853)
	at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:338)
	at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)
	at org.nustaq.serialization.serializers.FSTArrayListSerializer.instantiate(FSTArrayListSerializer.java:63)
	at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)
	at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)
	at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)
	at org.nustaq.serialization.serializers.FSTMapSerializer.instantiate(FSTMapSerializer.java:78)
	at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)
	at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)
	at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)
	at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:304)
	at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:238)
	... 59 more

{code}
",,
YARN-6683,"{code}
2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)
{code}

Below code already gets the RMApp instance and then send an event to RMApp to update the collector address. Instead of updating via event, it could just update via a method of RMApp. This also avoids state-machine changes.
Also, is there any implications that COLLECTOR_UPDATE happened at KILLED state ?

{code}
          } else {
            String previousCollectorAddr = rmApp.getCollectorAddr();
            if (previousCollectorAddr == null
                || !previousCollectorAddr.equals(collectorAddr)) {
              // sending collector update event.
              RMAppCollectorUpdateEvent event =
                  new RMAppCollectorUpdateEvent(appId, collectorAddr);
              rmContext.getDispatcher().getEventHandler().handle(event);
            }
          }
{code}",Surprise,-1
YARN-6714,"Currently in async-scheduling mode of CapacityScheduler, after AM failover and unreserve all reserved containers, it still have chance to get and commit the outdated reserve proposal of the failed app attempt. This problem happened on an app in our cluster, when this app stopped, it unreserved all reserved containers and compared these appAttemptId with current appAttemptId, if not match it will throw IllegalStateException and make RM crashed.

Error log:
{noformat}
2017-06-08 11:02:24,339 FATAL [ResourceManager Event Processor] org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler
java.lang.IllegalStateException: Trying to unreserve  for application appattempt_1495188831758_0121_000002 when currently reserved  for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)
        at java.lang.Thread.run(Thread.java:834)
{noformat}

When async-scheduling enabled, CapacityScheduler#doneApplicationAttempt and CapacityScheduler#tryCommit both need to get write_lock before executing, so we can check the app attempt state in commit process to avoid committing outdated proposals.
",Surprise,-1
YARN-6798,"YARN-6703 rolled back the state store version number for the RM from 2.0 to 1.4.

YARN-6127 bumped the version for the NM to 3.0

    private static final Version CURRENT_VERSION_INFO = Version.newInstance(3, 0);

YARN-5049 bumped the version for the NM to 2.0

    private static final Version CURRENT_VERSION_INFO = Version.newInstance(2, 0);

During an upgrade, all NMs died after upgrading a C6 cluster from alpha2 to alpha4.

{noformat}
2017-07-07 15:48:17,259 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager
org.apache.hadoop.service.ServiceStateException: java.io.IOException: Incompatible version for NM state: expecting NM state version 3.0, but loading version 2.0
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartRecoveryStore(NodeManager.java:246)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:307)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:748)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:809)
Caused by: java.io.IOException: Incompatible version for NM state: expecting NM state version 3.0, but loading version 2.0
        at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.checkVersion(NMLeveldbStateStoreService.java:1454)
        at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.initStorage(NMLeveldbStateStoreService.java:1308)
        at org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService.serviceInit(NMStateStoreService.java:307)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        ... 5 more
2017-07-07 15:48:17,277 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NodeManager at xxx.gce.cloudera.com/aa.bb.cc.dd
************************************************************/
{noformat}
",,
YARN-6948,"When I send kill command to a running job, I check the logs and find the Exception:

{code:java}
2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)
        at java.lang.Thread.run(Thread.java:745)
{code}
",,
YARN-699,"Just run into this. Looks like YARN-617 regressed TestUnmanagedAMLauncher.

From the test log:
{noformat}
2013-05-19 12:39:10,631 INFO  distributedshell.ApplicationMaster (ApplicationMaster.java:run(682)) - Setting up container launch container for containerid=container_1368992334149_0001_01_000001
2013-05-19 12:39:10,647 INFO  distributedshell.ApplicationMaster (ApplicationMaster.java:run(690)) - Setting user in ContainerLaunchContext to: ivanmi
2013-05-19 12:39:10,678 ERROR containermanager.ContainerManagerImpl (ContainerManagerImpl.java:authorizeRequest(412)) - Unauthorized request to start container. 
Expected containerId: ivanmi Found: container_1368992334149_0001_01_000001
2013-05-19 12:39:10,678 ERROR security.UserGroupInformation (UserGroupInformation.java:doAs(1492)) - PriviledgedActionException as:ivanmi (auth:SIMPLE) cause:org.apache.hadoop.yarn.exceptions.YarnRemoteException: Unauthorized request to start container. 
Expected containerId: ivanmi Found: container_1368992334149_0001_01_000001
2013-05-19 12:39:10,678 INFO  ipc.Server (Server.java:run(1864)) - IPC Server handler 5 on 49529, call org.apache.hadoop.yarn.api.ContainerManagerPB.startContainer from 10.120.19.109:49566: error: org.apache.hadoop.yarn.exceptions.YarnRemoteException: Unauthorized request to start container. 
Expected containerId: ivanmi Found: container_1368992334149_0001_01_000001
org.apache.hadoop.yarn.exceptions.YarnRemoteException: Unauthorized request to start container. 
Expected containerId: ivanmi Found: container_1368992334149_0001_01_000001
	at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:43)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:413)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:440)
	at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:72)
	at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1842)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1838)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1836)
2013-05-19 12:39:10,678 INFO  distributedshell.ApplicationMaster (ApplicationMaster.java:run(761)) - Start container failed for :, containerId=container_1368992334149_0001_01_000001
{noformat}

ContainerManagerImpl expected containerId to be equal to the remote UGI and since this was not the case, failed the authorization:
{noformat}
Unauthorized request to start container. 
Expected containerId: ivanmi Found: container_1368992334149_0001_01_000001
{noformat}",Surprise,-1
YARN-7118,"ApplicationHistoryService REST Api returns NullPointerException
{code}
[prabhu@prabhu2 root]$ curl --negotiate -u: 'http://<ATS IP>:8188/ws/v1/applicationhistory/apps?queue=test'
{""exception"":""NullPointerException"",""javaClassName"":""java.lang.NullPointerException""}
{code}

TimelineServer logs shows below.

{code}
2017-08-17 17:54:54,128 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
{code}


",,
YARN-715,"Tests are timing out. Looks like this is related to YARN-617.
{code}
2013-05-21 17:40:23,693 ERROR [IPC Server handler 0 on 54024] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:authorizeRequest(412)) - Unauthorized request to start container.
Expected containerId: user Found: container_1369183214008_0001_01_000001
2013-05-21 17:40:23,694 ERROR [IPC Server handler 0 on 54024] security.UserGroupInformation (UserGroupInformation.java:doAs(1492)) - PriviledgedActionException as:user (auth:SIMPLE) cause:org.apache.hado
Expected containerId: user Found: container_1369183214008_0001_01_000001
2013-05-21 17:40:23,695 INFO  [IPC Server handler 0 on 54024] ipc.Server (Server.java:run(1864)) - IPC Server handler 0 on 54024, call org.apache.hadoop.yarn.api.ContainerManagerPB.startContainer from 10.
Expected containerId: user Found: container_1369183214008_0001_01_000001
org.apache.hadoop.yarn.exceptions.YarnRemoteException: Unauthorized request to start container.
Expected containerId: user Found: container_1369183214008_0001_01_000001
  at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:43)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:413)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:440)
  at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:72)
  at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)
  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
{code}",,
YARN-7249,"This issue could happen when 3 conditions satisfied:

1) A node is removing from scheduler.
2) A container running on the node is being preempted. 
3) A rare race condition causes scheduler pass a null node to leaf queue.

Fix of the problem is to add a null node check inside CapacityScheduler.

Stack trace:
{code}
2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler 
java.lang.NullPointerException 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308) 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469) 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497) 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505) 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341) 
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127) 
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705) 
{code}

This is an issue only existed in 2.8.x",,
YARN-7308,"{{TestApplicationACLs}} fails when using FairScheduler:
{noformat}
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 98.389 sec <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
testApplicationACLs(org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs)  Time elapsed: 94.563 sec  <<< FAILURE!
java.lang.AssertionError: App State is not correct (timeout). expected:<FAILED> but was:<ACCEPTED>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.apache.hadoop.yarn.server.resourcemanager.MockRM.waitForState(MockRM.java:284)
	at org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs.verifyInvalidQueueWithAcl(TestApplicationACLs.java:422)
	at org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs.testApplicationACLs(TestApplicationACLs.java:186)
{noformat}

There's a bunch of messages like this in the output:
{noformat}
2017-10-09 17:00:54,572 INFO  [main] resourcemanager.MockRM (MockRM.java:waitForState(277)) - App : application_1507593559080_0006 State is : ACCEPTED Waiting for state : FAILED
{noformat}",,
YARN-7382,"While running an MR job (e.g. sleep) and an RM failover occurs, once the maps gets to 100%, the now active RM will crash due to:
{noformat}
2017-10-18 15:02:05,347 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1508361403235_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
2017-10-18 15:02:05,347 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=systest  OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1508361403235_0001    CONTAINERID=container_1508361403235_0001_01_000002      RESOURCE=<memory:1024, vCores:1>
2017-10-18 15:02:05,349 FATAL org.apache.hadoop.yarn.event.EventDispatcher: Error in handling event type NODE_UPDATE to the Event Dispatcher
java.util.NoSuchElementException
        at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)
        at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)
        at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)
        at java.lang.Thread.run(Thread.java:748)
2017-10-18 15:02:05,360 INFO org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye..
{noformat}
This leaves the cluster with no RMs!",Fear,-1
YARN-7511,"Error log:
{noformat}
2017-09-30 20:14:32,839 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.NullPointerException
    at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)
    at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)
    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)
    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)
    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)
    at java.lang.Thread.run(Thread.java:834)
2017-09-30 20:14:32,842 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
{noformat}

Reproduce this problem:
1. Container was running and ContainerManagerImpl#localize was called for this container
2. Localization failed in ResourceLocalizationService$LocalizerRunner#run and sent out ContainerResourceFailedEvent with null LocalResourceRequest.
3. NPE when ResourceLocalizationFailedWhileRunningTransition#transition --> container.resourceSet.resourceLocalizationFailed(null)

I think we can fix this problem through ensuring that request is not null before remove it.",,
YARN-7692,"Test scenario
------------------
1. A cluster is created, no ACLs are included
2. Submit jobs with an existing user say 'user_a'
3. Enable ACLs and create a priority ACL entry via the property yarn.scheduler.capacity.priority-acls. Do not include the user, 'user_a' in this ACL.
4. Submit a job with the 'user_a'

The observed behavior in this case is that the job is rejected as 'user_a' does not have the permission to run the job which is expected behavior. But Resource Manager also goes down when it tries to recover previous applications and fails to recover them.
Below is the exception seen,
{noformat}
2017-12-27 10:52:30,064 INFO  conf.Configuration (Configuration.java:getConfResourceAsInputStream(2659)) - found resource yarn-site.xml at file:/etc/hadoop/3.0.0.0-636/0/yarn-site.xml
2017-12-27 10:52:30,065 INFO  scheduler.AbstractYarnScheduler (AbstractYarnScheduler.java:setClusterMaxPriority(911)) - Updated the cluste max priority to maxClusterLevelAppPriority = 10
2017-12-27 10:52:30,066 INFO  resourcemanager.ResourceManager (ResourceManager.java:transitionToActive(1177)) - Transitioning to active state
2017-12-27 10:52:30,097 INFO  resourcemanager.ResourceManager (ResourceManager.java:serviceStart(765)) - Recovery started
2017-12-27 10:52:30,102 INFO  recovery.RMStateStore (RMStateStore.java:checkVersion(747)) - Loaded RM state version info 1.5
2017-12-27 10:52:30,375 INFO  security.RMDelegationTokenSecretManager (RMDelegationTokenSecretManager.java:recover(196)) - recovering RMDelegationTokenSecretManager.
2017-12-27 10:52:30,380 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(561)) - Recovering 51 applications
2017-12-27 10:52:30,432 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(571)) - Successfully recovered 0 out of 51 applications
2017-12-27 10:52:30,432 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(776)) - Failed to load/recover state
org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1143)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1183)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1179)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1179)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:611)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
Caused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0
        ... 20 more
2017-12-27 10:52:30,434 INFO  service.AbstractService (AbstractService.java:noteFailure(273)) - Service RMActiveServices failed in state STARTED; cause: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0
org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1143)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1183)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1179)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1179)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:611)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
Caused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0
        ... 20 more
2017-12-27 10:52:30,435 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ResourceManager metrics system...
2017-12-27 10:52:30,435 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - ResourceManager metrics system stopped.
2017-12-27 10:52:30,436 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - ResourceManager metrics system shutdown complete.
2017-12-27 10:52:30,436 INFO  event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(155)) - AsyncDispatcher is draining to stop, ignoring any new events.
2017-12-27 10:52:30,437 INFO  event.AsyncDispatcher (AsyncDispatcher.java:register(223)) - Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2017-12-27 10:52:30,438 INFO  security.NMTokenSecretManagerInRM (NMTokenSecretManagerInRM.java:<init>(75)) - NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2017-12-27 10:52:30,438 INFO  security.RMContainerTokenSecretManager (RMContainerTokenSecretManager.java:<init>(79)) - ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2017-12-27 10:52:30,438 INFO  security.AMRMTokenSecretManager (AMRMTokenSecretManager.java:<init>(94)) - AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2017-12-27 10:52:30,439 INFO  recovery.RMStateStoreFactory (RMStateStoreFactory.java:getStore(33)) - Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore
2017-12-27 10:52:30,439 INFO  event.AsyncDispatcher (AsyncDispatcher.java:register(223)) - Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2017-12-27 10:52:30,439 WARN  curator.CuratorZookeeperClient (CuratorZookeeperClient.java:<init>(96)) - session timeout [10000] is less than connection timeout [15000]
2017-12-27 10:52:30,440 INFO  imps.CuratorFrameworkImpl (CuratorFrameworkImpl.java:start(235)) - Starting
{noformat}
","Sadness, Surprise",-1
YARN-7737,"Hit this exception when a container failed:{noformat}2018-01-11 19:04:08,036 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Failed to get tail of the container's prelaunch error log file
java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist
        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745){noformat}
containerLogDir is picked on container launch via {{LocalDirAllocator#getLocalPathForWrite}}, which is where it looks for {{prelaunch.err}} when the container fails. But prelaunch.err (and prelaunch.out) are created in the first log dir (in {{ContainerLaunch#call}}: {noformat}        exec.writeLaunchEnv(containerScriptOutStream, environment,
            localResources, launchContext.getCommands(),
            new Path(containerLogDirs.get(0)), user);{noformat}",Surprise,-1
YARN-7786,"Before launching the ApplicationMaster, send kill command to the job, then some Null pointer appears:

{code}

2017-11-25 21:27:25,333 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1511616410268_0001_000001. Got exception: java.lang.NullPointerException
 at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)
 at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)
 at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)
 at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)

{code}",,
YARN-7818,"steps:
 1) Run Dshell Application
{code:java}
yarn  org.apache.hadoop.yarn.applications.distributedshell.Client -jar /usr/hdp/3.0.0.0-751/hadoop-yarn/hadoop-yarn-applications-distributedshell-*.jar -keep_containers_across_application_attempts -timeout 900000 -shell_command ""sleep 110"" -num_containers 4{code}
2) Find out host where AM is running. 
 3) Find Containers launched by application
 4) Restart NM where AM is running
 5) Validate that new attempt is not started and containers launched before restart are in RUNNING state.

In this test, step#5 fails because containers failed to launch with error 143
{code:java}
2018-01-24 09:48:30,547 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from RUNNING to KILLING
2018-01-24 09:48:30,547 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1516787230461_0001_01_000003
2018-01-24 09:48:30,552 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 143. Privileged Execution Operation Stderr:

Stdout: main : command provided 1
main : run as user is hrt_qa
main : requested yarn user is hrt_qa
Getting exit code file...
Creating script paths...
Writing pid file...
Writing to tmp file /grid/0/hadoop/yarn/local/nmPrivate/application_1516787230461_0001/container_e04_1516787230461_0001_01_000003/container_e04_1516787230461_0001_01_000003.pid.tmp
Writing to cgroup task files...
Creating local dirs...
Launching container...
Getting exit code file...
Creating script paths...

Full command array for failed execution:
[/usr/hdp/3.0.0.0-751/hadoop-yarn/bin/container-executor, hrt_qa, hrt_qa, 1, application_1516787230461_0001, container_e04_1516787230461_0001_01_000003, /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1516787230461_0001/container_e04_1516787230461_0001_01_000003, /grid/0/hadoop/yarn/local/nmPrivate/application_1516787230461_0001/container_e04_1516787230461_0001_01_000003/launch_container.sh, /grid/0/hadoop/yarn/local/nmPrivate/application_1516787230461_0001/container_e04_1516787230461_0001_01_000003/container_e04_1516787230461_0001_01_000003.tokens, /grid/0/hadoop/yarn/local/nmPrivate/application_1516787230461_0001/container_e04_1516787230461_0001_01_000003/container_e04_1516787230461_0001_01_000003.pid, /grid/0/hadoop/yarn/local, /grid/0/hadoop/yarn/log, cgroups=none]
2018-01-24 09:48:30,553 WARN  runtime.DefaultLinuxContainerRuntime (DefaultLinuxContainerRuntime.java:launchContainer(127)) - Launch container failed. Exception:
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer(DefaultLinuxContainerRuntime.java:124)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.launchContainer(DelegatingLinuxContainerRuntime.java:152)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:549)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:285)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:95)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: ExitCodeException exitCode=143:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 10 more
2018-01-24 09:48:30,553 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(557)) - Exit code from container container_e04_1516787230461_0001_01_000003 is : 143
2018-01-24 09:48:30,582 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(1365)) - Stopping container with container Id: container_e04_1516787230461_0001_01_000005
2018-01-24 09:48:31,093 INFO  impl.TimelineV2ClientImpl (TimelineV2ClientImpl.java:setTimelineCollectorInfo(172)) - Updated timeline service address to xxxxxx:40757
2018-01-24 09:48:32,675 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL{code}",,
YARN-7890,"While running a recent build of trunk, I saw the following:
{noformat}
2018-02-02 21:02:40,026 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_e02_1517604848419_0002_01_000004 transitioned from RELAUNCHING to RUNNING
2018-02-02 21:02:40,026 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch: Failed to relaunch container.
java.lang.NullPointerException
        at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)
        at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)
        at java.util.Collections.unmodifiableList(Collections.java:1287)
        at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}",,
YARN-7942,"Even with sasl:rm:cdrwa set on the ZK node (from the registry system accounts property), the RM fails to remove the node with the below error. Also, the destroy call succeeds.

{code}
2018-02-16 15:49:29,691 WARN  client.ServiceClient (ServiceClient.java:actionDestroy(470)) - Error deleting registry entry /users/hbase/services/yarn-service/hbase-app-test
org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test
        at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)
        at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)
        at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)
        at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)
        at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)
        at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)
        at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)
        at org.apache.hadoop.yarn.service.webapp.ApiServer.stopService(ApiServer.java:243)
        at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:89)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.security.AuthenticationWithProxyUserFilter.doFilter(AuthenticationWithProxyUserFilter.java:101)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1617)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:534)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:250)
        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:244)
        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)
        at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:241)
        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:225)
        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35)
        at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:718)
        ... 75 more
2018-02-16 15:49:29,694 INFO  client.ServiceClient (ServiceClient.java:actionDestroy(473)) - Successfully destroyed service hbase-app-test
2018-02-16 15:49:29,694 INFO  webapp.ApiServer (ApiServer.java:run(254)) - Successfully deleted service hbase-app-test
{code}","Joy, Surprise","1, -1"
YARN-7962,"[https://github.com/apache/hadoop/blob/69fa81679f59378fd19a2c65db8019393d7c05a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java]
{code:java}
  private ThreadPoolExecutor renewerService;

  private void processDelegationTokenRenewerEvent(
      DelegationTokenRenewerEvent evt) {
    serviceStateLock.readLock().lock();
    try {
      if (isServiceStarted) {
        renewerService.execute(new DelegationTokenRenewerRunnable(evt));
      } else {
        pendingEventQueue.add(evt);
      }
    } finally {
      serviceStateLock.readLock().unlock();
    }
  }

  @Override
  protected void serviceStop() {
    if (renewalTimer != null) {
      renewalTimer.cancel();
    }
    appTokens.clear();
    allTokens.clear();
    this.renewerService.shutdown();
{code}
{code:java}
2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)
{code}
What I think is going on here is that the{{serviceStop}}method is not setting the{{isServiceStarted}}flag to 'false'.

Please update so that the{{serviceStop}}method grabs the{{serviceStateLock}}and sets{{isServiceStarted}}to_false_, before shutting down the{{renewerService}}thread pool,to avoid this condition.",Surprise,-1
YARN-8035,"In the case of a container relaunch event, the container ID is reused but a new process is spawned.For resource monitoring, {{ContainersMonitorImpl}} will obtain the new PID post relaunch and initialize the process tree monitoring. As part of this initialization, a tag called {{ContainerPid}}, whose value is thePID for the container, ispopulated forthe metrics associated with the container. If the prior container failed after its process started, the original PID will already be populated for the container, resulting in the {{MetricsException}} below.
{code:java}
2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002
org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!
at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)
at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)
at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448){code}
{{MetricsRegistry}} provides a {{tag}} method that allows for updating the value of an existing tag. Updating the value ensures that the PID associated with container is the currently running process, which appears to be an appropriate fix. However, it's unclear how this tag might be being used by other systems. I'm not finding any usage in Hadoop itself.","Joy, Sadness, Surprise","1, -1"
YARN-8116,"Steps followed.
1) Update nodemanager debug delay config
{code}
<property>
      <name>yarn.nodemanager.delete.debug-delay-sec</name>
      <value>350</value>
    </property>{code}
2) Launch distributed shell application multiple times
{code}
/usr/hdp/current/hadoop-yarn-client/bin/yarn  jar hadoop-yarn-applications-distributedshell-*.jar  -shell_command ""sleep 120"" -num_containers 1 -shell_env YARN_CONTAINER_RUNTIME_TYPE=docker -shell_env YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=centos/httpd-24-centos7:latest -shell_env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true -jar hadoop-yarn-applications-distributedshell-*.jar{code}
3) restart NM

Nodemanager fails to start with below error.
{code}

{code:title=NM log}
2018-03-23 21:32:14,437 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:serviceInit(181)) - ContainersMonitor enabled: true
2018-03-23 21:32:14,439 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceInit(130)) - rollingMonitorInterval is set as 3600. The logs will be aggregated every 3600 seconds
2018-03-23 21:32:14,455 INFO  service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl failed in state INITED
java.lang.NumberFormatException: For input string: """"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:601)
	at java.lang.Long.parseLong(Long.java:631)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)
2018-03-23 21:32:14,458 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceStop(148)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit
2018-03-23 21:32:14,460 INFO  service.AbstractService (AbstractService.java:noteFailure(267)) - Service NodeManager failed in state INITED
java.lang.NumberFormatException: For input string: """"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:601)
	at java.lang.Long.parseLong(Long.java:631)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)
	at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)
2018-03-23 21:32:14,463 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NodeManager metrics system...
2018-03-23 21:32:14,464 INFO  impl.MetricsSinkAdapter (MetricsSinkAdapter.java:publishMetricsFromQueue(141)) - timeline thread interrupted.
2018-03-23 21:32:14,468 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NodeManager metrics system stopped.
2018-03-23 21:32:14,508 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NodeManager metrics system shutdown complete.{code}",,
YARN-8202,"

When I execute a pi job with arguments:
{code:java}
-Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000{code}
and I have one node with 5GB of resource1, I get the following exception on every second and the job hangs:
{code:java}
2018-04-24 08:42:03,694 INFO org.apache.hadoop.ipc.Server: IPC Server handler 20 on 8030, call Call#386 Retry#0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate from 172.31.119.172:58138

org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:8192, resource1: 9223372036854775807G>
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)
    at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)
    at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)
    at org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)
    at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:433)
    at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)
    at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
{code}
*This is becauseorg.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils#validateResourceRequest does not take resource units into account.*



However, if I start a job with arguments:
{code:java}
-Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=1G 1 1000{code}
and I still have 5GB of resource1 on one node then the job runs successfully.



I also tried a thirdjob run, when I request 1GB of resource1 and I have no nodes with any amount of resource1, then I restart the node with 5GBs of resource1, the job ultimately completes, but just after the node with enough resources registered in RM, which is the desired behaviour.

","Anger, Joy, Sadness, Surprise","1, -1"
YARN-8211,"Yarn registry dns server is constantly getting BufferUnderflowException. 
{code:java}
2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76

2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:

java.nio.BufferUnderflowException

    at java.nio.Buffer.nextGetIndex(Buffer.java:500)

    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)

    at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)

    at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)

    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)

    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)

    at java.util.concurrent.FutureTask.run(FutureTask.java:266)

    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

    at java.lang.Thread.run(Thread.java:748){code}
",,
YARN-8223,"Loading an auxiliary jar from a local location on a node manager works as expected,
{noformat}
2018-04-26 15:09:26,179 INFO  util.ApplicationClassLoader (ApplicationClassLoader.java:<init>(98)) - classpath: [file:/grid/0/hadoop/yarn/local/aux-service-local.jar]
2018-04-26 15:09:26,179 INFO  util.ApplicationClassLoader (ApplicationClassLoader.java:<init>(99)) - system classes: [java., javax.accessibility., javax.activation., javax.activity., javax.annotation., javax.annotation.processing., javax.crypto., javax.imageio., javax.jws., javax.lang.model., -javax.management.j2ee., javax.management., javax.naming., javax.net., javax.print., javax.rmi., javax.script., -javax.security.auth.message., javax.security.auth., javax.security.cert., javax.security.sasl., javax.sound., javax.sql., javax.swing., javax.tools., javax.transaction., -javax.xml.registry., -javax.xml.rpc., javax.xml., org.w3c.dom., org.xml.sax., org.apache.commons.logging., org.apache.log4j., -org.apache.hadoop.hbase., org.apache.hadoop., core-default.xml, hdfs-default.xml, mapred-default.xml, yarn-default.xml]
2018-04-26 15:09:26,181 INFO  containermanager.AuxServices (AuxServices.java:serviceInit(252)) - The aux service:test_aux_local are using the custom classloader
2018-04-26 15:09:26,182 WARN  containermanager.AuxServices (AuxServices.java:serviceInit(268)) - The Auxiliary Service named 'test_aux_local' in the configuration is for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader which has a name of 'org.apache.auxtest.AuxServiceFromLocal with custom class loader'. Because these are not the same tools trying to send ServiceData and read Service Meta Data may have issues unless the refer to the name in the config.
2018-04-26 15:09:26,182 INFO  containermanager.AuxServices (AuxServices.java:addService(103)) - Adding auxiliary service org.apache.auxtest.AuxServiceFromLocal with custom class loader, ""test_aux_local""{noformat}
But loading the same jar from a location on HDFS fails with a ClassNotFoundException.
{noformat}
018-04-26 15:14:39,683 INFO  util.ApplicationClassLoader (ApplicationClassLoader.java:<init>(98)) - classpath: []
2018-04-26 15:14:39,683 INFO  util.ApplicationClassLoader (ApplicationClassLoader.java:<init>(99)) - system classes: [java., javax.accessibility., javax.activation., javax.activity., javax.annotation., javax.annotation.processing., javax.crypto., javax.imageio., javax.jws., javax.lang.model., -javax.management.j2ee., javax.management., javax.naming., javax.net., javax.print., javax.rmi., javax.script., -javax.security.auth.message., javax.security.auth., javax.security.cert., javax.security.sasl., javax.sound., javax.sql., javax.swing., javax.tools., javax.transaction., -javax.xml.registry., -javax.xml.rpc., javax.xml., org.w3c.dom., org.xml.sax., org.apache.commons.logging., org.apache.log4j., -org.apache.hadoop.hbase., org.apache.hadoop., core-default.xml, hdfs-default.xml, mapred-default.xml, yarn-default.xml]
2018-04-26 15:14:39,687 INFO  service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices failed in state INITED
java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)
	at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)
{noformat}
The difference between the 2 logs is the classpath variable in the 1st line of the log is empty in the HDFS case. It doesn't have the URL/filename of the jar file specified in the config.",Surprise,-1
YARN-8236,"Stack trace


{code:java}
2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR
java.lang.NullPointerException
at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)
at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)
at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269){code}

cc [~gsaha] [~csingh]",,
YARN-8244,"{code:java}
testStartMultipleContainers(org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing) Time elapsed: 22.198 s <<< FAILURE!
java.lang.AssertionError: ContainerState is not correct (timedout)
 at org.junit.Assert.fail(Assert.java:88)
 at org.junit.Assert.assertTrue(Assert.java:41)
 at org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest.waitForContainerState(BaseContainerManagerTest.java:344)
 at org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest.waitForContainerState(BaseContainerManagerTest.java:309)
 at org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing.testStartMultipleContainers(TestContainerSchedulerQueuing.java:256)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:497)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413){code}
{code:java}
2018-05-03 17:31:35,028 WARN [ContainersLauncher #1] launcher.ContainerLaunch (ContainerLaunch.java:call(329)) - Failed to launch container.
java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)
at java.util.HashMap$EntryIterator.next(HashMap.java:1471)
at java.util.HashMap$EntryIterator.next(HashMap.java:1469)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch$ShellScriptBuilder.orderEnvByDependencies(ContainerLaunch.java:1311)
at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.writeLaunchEnv(ContainerExecutor.java:388)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:290)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:101)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
2{code}",,
YARN-8331,"When a container is launching, in ContainerLaunch#launchContainer, state is SCHEDULED,
kill event was sent to this container, state : SCHEDULED->KILLING->DONE
 Then ContainerLaunch send CONTAINER_LAUNCHED event and start the container processes. These absent container processes will not be cleaned up anymore.


{code:java}
2018-05-21 13:11:56,114 INFO  [Thread-11] nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(94)) - USER=nobody	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_0_0000	CONTAINERID=container_0_0000_01_000000
2018-05-21 13:11:56,114 INFO  [NM ContainerManager dispatcher] application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_0_0000 transitioned from NEW to INITING
2018-05-21 13:11:56,114 INFO  [NM ContainerManager dispatcher] application.ApplicationImpl (ApplicationImpl.java:transition(446)) - Adding container_0_0000_01_000000 to application application_0_0000
2018-05-21 13:11:56,118 INFO  [NM ContainerManager dispatcher] application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_0_0000 transitioned from INITING to RUNNING
2018-05-21 13:11:56,119 INFO  [NM ContainerManager dispatcher] container.ContainerImpl (ContainerImpl.java:handle(2111)) - Container container_0_0000_01_000000 transitioned from NEW to SCHEDULED
2018-05-21 13:11:56,119 INFO  [NM ContainerManager dispatcher] containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_INIT for appId application_0_0000
2018-05-21 13:11:56,119 INFO  [NM ContainerManager dispatcher] scheduler.ContainerScheduler (ContainerScheduler.java:startContainer(504)) - Starting container [container_0_0000_01_000000]
2018-05-21 13:11:56,226 INFO  [NM ContainerManager dispatcher] container.ContainerImpl (ContainerImpl.java:handle(2111)) - Container container_0_0000_01_000000 transitioned from SCHEDULED to KILLING
2018-05-21 13:11:56,227 INFO  [NM ContainerManager dispatcher] containermanager.TestContainerManager (BaseContainerManagerTest.java:delete(287)) - Psuedo delete: user - nobody, type - FILE
2018-05-21 13:11:56,227 INFO  [NM ContainerManager dispatcher] nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(94)) - USER=nobody	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_0_0000	CONTAINERID=container_0_0000_01_000000
2018-05-21 13:11:56,238 INFO  [NM ContainerManager dispatcher] container.ContainerImpl (ContainerImpl.java:handle(2111)) - Container container_0_0000_01_000000 transitioned from KILLING to DONE
2018-05-21 13:11:56,238 INFO  [NM ContainerManager dispatcher] application.ApplicationImpl (ApplicationImpl.java:transition(489)) - Removing container_0_0000_01_000000 from application application_0_0000
2018-05-21 13:11:56,239 INFO  [NM ContainerManager dispatcher] monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStopMonitoringContainer(932)) - Stopping resource-monitoring for container_0_0000_01_000000
2018-05-21 13:11:56,239 INFO  [NM ContainerManager dispatcher] containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_STOP for appId application_0_0000
2018-05-21 13:11:56,274 WARN  [NM ContainerManager dispatcher] container.ContainerImpl (ContainerImpl.java:handle(2106)) - Can't handle this event at current state: Current: [DONE], eventType: [CONTAINER_LAUNCHED], container: [container_0_0000_01_000000]
org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
	at java.lang.Thread.run(Thread.java:748)
{code}",Surprise,-1
YARN-8357,"Line 972 in \{{ServiceClient}} returns a service with state \{{null}} which is why there is a NPE.
{code:java}
2018-05-24 04:39:22,911 INFO client.ServiceClient (ServiceClient.java:getStatus(1203)) - Service test1does not have an application ID
2018-05-24 04:39:22,911 ERROR webapp.ApiServer (ApiServer.java:updateService(480)) - Error while performing operation for app: test1

java.lang.NullPointerException

    at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)

    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)

    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)

    at java.security.AccessController.doPrivileged(Native Method)

    at javax.security.auth.Subject.doAs(Subject.java:422)

    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)

    at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)

    at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)

    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

    at java.lang.reflect.Method.invoke(Method.java:498)

    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)

    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)

    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)

    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)

    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)

    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)

    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)

    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)

    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)

    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)

    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)

    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)

    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
{code}",,
YARN-8383,"TimelineServer 1.5 start fails with NoClassDefFoundError.
{noformat}
2018-05-31 22:10:58,548 FATAL org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer: Error starting ApplicationHistoryServer
java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/JsonFactory
	at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.<clinit>(RollingLevelDBTimelineStore.java:174)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2306)
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2271)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393)
	at org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore.createSummaryStore(EntityGroupFSTimelineStore.java:239)
	at org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore.serviceInit(EntityGroupFSTimelineStore.java:146)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:115)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:180)
	at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:190)
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.core.JsonFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 15 more

{noformat}",,
YARN-8401,"{code}
2018-06-06 21:10:58,611 WARN org.eclipse.jetty.webapp.WebAppContext: Failed startup of context o.e.j.w.WebAppContext@108a46d6{/ui2,file:///opt/HA/310/install/hadoop/resourcemanager/share/hadoop/yarn/webapps/ui2/,null}
java.net.UnknownHostException: java.sun.com
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at java.net.Socket.connect(Socket.java:538)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
        at sun.net.www.http.HttpClient.New(HttpClient.java:308)
        at sun.net.www.http.HttpClient.New(HttpClient.java:326)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1168)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1104)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:998)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:932)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1512)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:646)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startEntity(XMLEntityManager.java:1300)
        at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.startDTDEntity(XMLEntityManager.java:1267)
        at com.sun.org.apache.xerces.internal.impl.XMLDTDScannerImpl.setInputSource(XMLDTDScannerImpl.java:263)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.dispatch(XMLDocumentScannerImpl.java:1164)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$DTDDriver.next(XMLDocumentScannerImpl.java:1050)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:964)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)
        at org.eclipse.jetty.xml.XmlParser.parse(XmlParser.java:255)
        at org.eclipse.jetty.webapp.Descriptor.parse(Descriptor.java:54)
        at org.eclipse.jetty.webapp.WebDescriptor.parse(WebDescriptor.java:207)
        at org.eclipse.jetty.webapp.MetaData.setWebXml(MetaData.java:189)
        at org.eclipse.jetty.webapp.WebXmlConfiguration.preConfigure(WebXmlConfiguration.java:60)
        at org.eclipse.jetty.webapp.WebAppContext.preConfigure(WebAppContext.java:485)
        at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:521)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)
        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)
        at org.eclipse.jetty.server.Server.start(Server.java:422)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105)
        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)
        at org.eclipse.jetty.server.Server.doStart(Server.java:389)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1134)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:431)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1170)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1279)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1480)

{code}",,
YARN-8403,"Some of the container execution related stack traces are printing in INFO or WARN level. 

{code}
2018-06-06 03:10:40,077 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:writeCredentials(1312)) - Writing credentials to the nmPrivate file /grid/0/hadoop/yarn/local/nmPrivate/container_e02_1528246317583_0048_01_000001.tokens
2018-06-06 03:10:40,087 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(975)) - Failed to download resource { { hdfs://mycluster.example.com:8020/user/hrt_qa/Streaming/InputDir, 1528254452720, FILE, null },pending,[(container_e02_1528246317583_0048_01_000001)],6074418082915225,DOWNLOADING}
org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed
        at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)
        at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)
        at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:409)
        at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:66)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)
        at java.io.FileOutputStream.open0(Native Method)
        at java.io.FileOutputStream.open(FileOutputStream.java:270)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:408)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:399)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:381)
        at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:298)
        ... 9 more
{code}

{code}
2018-06-06 03:10:41,547 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(182)) - IOException executing command:
java.io.InterruptedIOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)
        ... 5 more
2018-06-06 03:10:41,548 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:startLocalizer(407)) - Exit code from container container_e02_1528246317583_0048_01_000001 startLocalizer is : -1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: java.io.InterruptedIOException: java.lang.InterruptedException
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:183)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)
Caused by: java.io.InterruptedIOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 2 more
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)
        ... 5 more
2018-06-06 03:10:41,548 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(1249)) - Localizer failed for container_e02_1528246317583_0048_01_000001
java.io.IOException: Application application_1528246317583_0048 initialization failed (exitCode=-1) with output: null
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:411)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)
Caused by: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: java.io.InterruptedIOException: java.lang.InterruptedException
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:183)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)
... 1 more
Caused by: java.io.InterruptedIOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 2 more
Caused by: java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)
        ... 5 more
{code}

These logs are only present in NM. ( It does not show up in AM log) 
These stacktraces are in WARN or INFO level. Ideally, exception should be printed in ERROR log level. ","Sadness, Surprise",-1
YARN-8409,"In RM-HA env, kill ZK leader and then perform RM failover.

Sometimes, active RM gets NPE and fail to come up successfully
{code:java}

2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.

2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused

at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)

at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)

at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)

2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED

java.lang.NullPointerException

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)

at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)

at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)

2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election{code}",,
YARN-8591,"{code:java}
GET http://ctr-e138-1518143905142-417433-01-000004.hwx.site:8198/ws/v2/timeline/apps/application_1532578985272_0002/entities/YARN_CONTAINER?fields=ALL&_=1532670071899{code}
{code:java}
2018-07-27 05:32:03,468 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR
javax.ws.rs.WebApplicationException: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
        at org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter.doFilter(TimelineReaderWhitelistAuthorizationFilter.java:85)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:98)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1604)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:534)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)
        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:622)
{code}",,
YARN-8629,"When an application failed to launch container successfully, the cleanup of container also failed with below message.
{code}
2018-08-06 03:28:20,351 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.
java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
        at java.io.FileInputStream.<init>(FileInputStream.java:93)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
        at java.lang.Thread.run(Thread.java:748)
2018-08-06 03:28:20,372 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.{code}",,
ZOOKEEPER-1340,"Multi operations run by users are generating ERROR level messages in the server log even though they are typical user level operations that are not in any way impacting the server, example:

{noformat}
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@545] - >>>> Got user-level KeeperException when processing sessionid:0x13466e9828c0000 type:multi cxid:0x3 zxid:0x2 txntype:2 reqpath:n/a Error Path:/nonexisting Error:KeeperErrorCode = NoNode for /nonexisting
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@549] - >>>> ABORTING remaing MultiOp ops
{noformat}

This is misleading. We should demote these messages to INFO level at the highest. (this is what we do for other such user operations, e.g. nonode)
","Sadness, Surprise",-1
ZOOKEEPER-1781,"If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:

2013-10-02 18:09:07,600 [myid:1] - ERROR [SyncThread:1:SyncRequestProcessor@151] - Severe unrecoverable error, exiting
java.lang.IllegalArgumentException: n must be positive
        at java.util.Random.nextInt(Random.java:300)
        at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:93)

In source code,  it maybe be supposed that snapCount must be 2 or more:
{code:title=org.apache.zookeeper.server.SyncRequestProcessor.java|borderStyle=solid}
     91             // we do this in an attempt to ensure that not all ofthe servers
     92             // in the ensemble take a snapshot at the same time
     93             int randRoll = r.nextInt(snapCount/2);
{code}

I think this supposition is not bad because snapCount = 1 is not realistic setting...
But, it may be better to mention this restriction in documentation or add a validation in the source code.","Sadness, Surprise",-1
ZOOKEEPER-1799,"org.apache.zookeeper.test.SaslAuthFailDesignatedClientTest.testAuth often fails on SUSE with the following error stack trace:
{code}
junit.framework.AssertionFailedError: expected [0x141ccb60d870000] expected:<1> but was:<0>
	at org.apache.zookeeper.test.JMXEnv.ensureAll(JMXEnv.java:115)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:200)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:174)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:159)
	at org.apache.zookeeper.test.ClientBase.createClient(ClientBase.java:152)
	at org.apache.zookeeper.test.SaslAuthFailDesignatedClientTest.testAuth(SaslAuthFailDesignatedClientTest.java:87)
	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{code}

The reason is that this is a negative test. After authentication fails, the client connection is closed at the server side so does the session right before test case calls JMXEnv.ensureAll  to verify the session. Below are the log events show the sequence and you can see the session was already closed before client JMXEnv.ensureAll.

{code}
2013-10-18 10:56:25,320 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@595] - Established session 0x141ccb60d870000 with negotiated timeout 30000 for client /127.0.0.1:58272
2013-10-18 10:56:25,327 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:ZooKeeperServer@940] - Client failed to SASL authenticate: javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response.
2013-10-18 10:56:25,327 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:ZooKeeperServer@946] - Closing client connection due to SASL authentication failure.
2013-10-18 10:56:25,329 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@1001] - Closed socket connection for client /127.0.0.1:58272 which had sessionid 0x141ccb60d870000
....
2013-10-18 10:56:25,330 [myid:] - INFO  [main-SendThread(localhost:11221):ClientCnxn$SendThread@1089] - Unable to read additional data from server sessionid 0x141ccb60d870000, likely server has closed socket, closing socket connection and attempting reconnect
2013-10-18 10:56:25,332 [myid:] - INFO  [main:JMXEnv@105] - expect:0x141ccb60d870000
{code}",,
ZOOKEEPER-1862,"ServerCnxnTest#testServerCnxnExpiry test case is failing in the trunk build with the following exception
{code}
    [junit] 2014-01-11 10:13:07,696 [myid:] - INFO  [NIOServerCxnFactory.AcceptThread:0.0.0.0/0.0.0.0:11221:NIOServerCnxnFactory$AcceptThread@296] - Accepted socket connection from /127.0.0.1:63930
    [junit] 2014-01-11 10:13:09,000 [myid:] - INFO  [ConnnectionExpirer:NIOServerCnxn@1006] - Closed socket connection for client /127.0.0.1:63930 (no session established for client)
    [junit] 2014-01-11 10:13:10,697 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@62] - TEST METHOD FAILED testServerCnxnExpiry
    [junit] java.net.SocketException: Software caused connection abort: recv failed
    [junit] 	at java.net.SocketInputStream.socketRead0(Native Method)
    [junit] 	at java.net.SocketInputStream.read(SocketInputStream.java:150)
    [junit] 	at java.net.SocketInputStream.read(SocketInputStream.java:121)
    [junit] 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
    [junit] 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
    [junit] 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
    [junit] 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
    [junit] 	at java.io.BufferedReader.fill(BufferedReader.java:154)
    [junit] 	at java.io.BufferedReader.readLine(BufferedReader.java:317)
    [junit] 	at java.io.BufferedReader.readLine(BufferedReader.java:382)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.send4LetterWord(ServerCnxnTest.java:105)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.sendRequest(ServerCnxnTest.java:77)
    [junit] 	at org.apache.zookeeper.test.ServerCnxnTest.testServerCnxnExpiry(ServerCnxnTest.java:64)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:601)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:69)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:48)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:292)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    {code}

When analyzing the possible cause of the failure is:
 During connection expiry server will close the socket channel connection. After the socket closure, when the client tries to read a line of text will throw java.net.SocketException.

 In the failure scenario the testcase has established a socket connection and entering into the sleep. In the meantime the server side expiration would happen and closing the socket channel. Assume after the socket closure the testcase is trying to read the text using the previously established socket and is resulting in SocketException. There is a race between the reading the socket in the client side and socket closure in server side.
{code}
NIOServerCnxn#closeSock is closing the socket channel.
 sock.socket().shutdownOutput();
 sock.socket().shutdownInput();
 sock.socket().close();
 sock.close();
{code}",,
ZOOKEEPER-1864,"This bug was found when using ZK 3.5.0 with curator-test 2.3.0.
curator-test is building a QuorumPeerConfig from a Properties object and then when we try to run the quorum peer using that configuration, we get an NPE:
{noformat}
2014-01-19 21:58:39,768 [myid:] - ERROR [Thread-3:TestingZooKeeperServer$1@138] - From testing server (random state: false)
java.lang.NullPointerException
	at org.apache.zookeeper.server.quorum.QuorumPeer.setQuorumVerifier(QuorumPeer.java:1320)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)
	at org.apache.curator.test.TestingZooKeeperServer$1.run(TestingZooKeeperServer.java:134)
	at java.lang.Thread.run(Thread.java:722)
{noformat}
The reason that this happens is because QuorumPeerConfig:parseProperties only peforms a subset of what 'QuorumPeerConfig:parse(String path)' does. The exact additional task performed that we need in parseProperties is the dynamic config backwards compatibility check:
{noformat}
            // backward compatibility - dynamic configuration in the same file as static configuration params
            // see writeDynamicConfig() - we change the config file to new format if reconfig happens
            if (dynamicConfigFileStr == null) {
                configBackwardCompatibilityMode = true;
                configFileStr = path;................
                parseDynamicConfig(cfg, electionAlg, true);
                checkValidity();................
            }
{noformat}
",Surprise,-1
ZOOKEEPER-1898,"zookeeper-cli always return ""0"" as exit code whether the command has been successful or not.

Ex:

Unsuccessful:
{code}
-bash-4.1$ zookeeper-client aa
Connecting to localhost:2181
2014-03-20 14:43:01,361 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 14:43:01,368 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ip-172-17-0-105.redlabnet.internal
2014-03-20 14:43:01,369 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 14:43:01,370 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 14:43:01,371 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 14:43:01,371 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 14:43:01,372 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 14:43:01,373 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 14:43:01,374 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 14:43:01,375 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 14:43:01,375 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 14:43:01,376 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 14:43:01,377 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 14:43:01,377 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 14:43:01,378 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 14:43:01,382 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5220c1b
ZooKeeper -server host:port cmd args
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	rmr path
	delquota [-n|-b] path
	quit 
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close 
	ls2 path [watch]
	history 
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path

-bash-4.1$ echo $?
0
{code}

Successful:
{code}
-bash-4.1$ zookeeper-client ls /
Connecting to localhost:2181
2014-03-20 14:43:53,881 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.5-cdh5.0.0-beta-2--1, built on 02/07/2014 18:28 GMT
2014-03-20 14:43:53,889 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=ip-172-17-0-105.redlabnet.internal
2014-03-20 14:43:53,889 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.7.0_51
2014-03-20 14:43:53,890 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2014-03-20 14:43:53,891 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.51.x86_64/jre
2014-03-20 14:43:53,892 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/lib/zookeeper/bin/../build/classes:/usr/lib/zookeeper/bin/../build/lib/*.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12.jar:/usr/lib/zookeeper/bin/../lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/bin/../lib/netty-3.2.2.Final.jar:/usr/lib/zookeeper/bin/../lib/log4j-1.2.15.jar:/usr/lib/zookeeper/bin/../lib/jline-0.9.94.jar:/usr/lib/zookeeper/bin/../zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/bin/../src/java/lib/*.jar:/etc/zookeeper/conf::/etc/zookeeper/conf:/usr/lib/zookeeper/zookeeper.jar:/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.0.0-beta-2.jar:/usr/lib/zookeeper/lib/slf4j-log4j12.jar:/usr/lib/zookeeper/lib/slf4j-api-1.7.5.jar:/usr/lib/zookeeper/lib/log4j-1.2.15.jar:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar:/usr/lib/zookeeper/lib/jline-0.9.94.jar:/usr/lib/zookeeper/lib/netty-3.2.2.Final.jar
2014-03-20 14:43:53,893 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-03-20 14:43:53,894 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2014-03-20 14:43:53,894 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=<NA>
2014-03-20 14:43:53,895 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2014-03-20 14:43:53,896 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2014-03-20 14:43:53,897 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=2.6.32-431.3.1.el6.x86_64
2014-03-20 14:43:53,897 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=hdfs
2014-03-20 14:43:53,898 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/var/lib/hadoop-hdfs
2014-03-20 14:43:53,899 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/var/lib/hadoop-hdfs
2014-03-20 14:43:53,902 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5a9e40d2
2014-03-20 14:43:53,953 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@966] - Opening socket connection to server localhost.localdomain/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2014-03-20 14:43:53,963 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@849] - Socket connection established to localhost.localdomain/127.0.0.1:2181, initiating session
2014-03-20 14:43:53,977 [myid:] - INFO  [main-SendThread(localhost.localdomain:2181):ClientCnxn$SendThread@1207] - Session establishment complete on server localhost.localdomain/127.0.0.1:2181, sessionid = 0x144dbe27e1b0013, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[hadoop-ha, zookeeper]

-bash-4.1$ echo $?
0

{code}",Sadness,-1
ZOOKEEPER-2924,"From https://builds.apache.org/job/ZooKeeper_branch34_openjdk7/1682/

Same issue happens in jdk8 and jdk9 builds as well.

Issue has already been fixed by https://issues.apache.org/jira/browse/ZOOKEEPER-2484 , but I believe that the root cause here is that test startup / cleanup code is included in the tests instead of using try-finally block or Before-After methods.

As a consequence, when exception happens during test execution, ZK test server doesn't get shutdown properly and still listening on the port bound to the test class.

As mentioned above there could be 2 approaches to address this:
#1 Wrap cleanup code block with finally
#2 Use JUnit's Before-After methods for initialization and cleanup

Test where original issue happens:

{noformat}
...   
     [junit] 2017-10-12 15:05:20,135 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8c zxid:0x8d txntype:-1 req$
     [junit] 2017-10-12 15:05:20,137 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8d zxid:0x8e txntype:-1 req$
     [junit] 2017-10-12 15:05:20,139 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8e zxid:0x8f txntype:-1 req$
     [junit] 2017-10-12 15:05:20,142 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x8f zxid:0x90 txntype:-1 req$
     [junit] 2017-10-12 15:05:20,144 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:create cxid:0x90 zxid:0x91 txntype:-1 req$
     [junit] 2017-10-12 15:05:30,479 [myid:] - INFO  [SessionTracker:ZooKeeperServer@354] - Expiring session 0x104cd7b190c0000, timeout of 6000ms exceeded
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x104cd7b190c0000 type:ping cxid:0xfffffffffffffffe zxid:0xfffff$
     [junit] 2017-10-12 15:05:24,147 [myid:] - WARN  [main-SendThread(127.0.0.1:11221):ClientCnxn$SendThread@1111] - Client session timed out, have not heard from server in 4002ms for sessionid 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [main-SendThread(127.0.0.1:11221):ClientCnxn$SendThread@1159] - Client session timed out, have not heard from server in 4002ms for sessionid 0x104cd7b190c0000, closing socket connectio$
     [junit] 2017-10-12 15:05:21,479 [myid:] - INFO  [SessionTracker:SessionTrackerImpl@163] - SessionTrackerImpl exited loop!
     [junit] 2017-10-12 15:05:32,998 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@376] - Unable to read additional data from client sessionid 0x104cd7b190c0000, likely client has closed socket
     [junit] 2017-10-12 15:05:33,067 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11221:NIOServerCnxn@1040] - Closed socket connection for client /127.0.0.1:45735 which had sessionid 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:32,996 [myid:] - INFO  [ProcessThread(sid:0 cport:11221)::PrepRequestProcessor@487] - Processed session termination for sessionid: 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:33,889 [myid:] - INFO  [main:ZooKeeper@687] - Session: 0x104cd7b190c0000 closed
     [junit] 2017-10-12 15:05:33,890 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@520] - EventThread shut down for session: 0x104cd7b190c0000
     [junit] 2017-10-12 15:05:33,891 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@74] - TEST METHOD FAILED testRestoreWithTransactionErrors
     [junit] org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /invaliddir/test-
     [junit]     at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
     [junit]     at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
     [junit]     at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:786)
     [junit]     at org.apache.zookeeper.test.LoadFromLogTest.testRestoreWithTransactionErrors(LoadFromLogTest.java:368)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
     [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
     [junit]     at java.lang.reflect.Method.invoke(Method.java:606)
     [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
     [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
     [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
     [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
     [junit]     at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
     [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
     [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
     [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
     [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
     [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
     [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
     [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
     [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033)
{noformat}

Test #2 where port is already in use:

{noformat}
     [junit] 2017-10-12 15:05:33,899 [myid:] - INFO  [main:ZKTestCase$1@59] - STARTING testReloadSnapshotWithMissingParent
     [junit] 2017-10-12 15:05:33,899 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@53] - RUNNING TEST METHOD testReloadSnapshotWithMissingParent
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:ZooKeeperServer@173] - Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /home/jenkins/jenkins-slave/workspace/ZooKeeper_branch34_openjdk7/$
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:ServerCnxnFactory@117] - Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory
     [junit] 2017-10-12 15:05:33,900 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:11221
     [junit] 2017-10-12 15:05:33,901 [myid:] - INFO  [main:JUnit4ZKTestRunner$LoggedInvokeMethod@74] - TEST METHOD FAILED testReloadSnapshotWithMissingParent
     [junit] java.net.BindException: Address already in use
     [junit]     at sun.nio.ch.Net.bind0(Native Method)
     [junit]     at sun.nio.ch.Net.bind(Net.java:463)
     [junit]     at sun.nio.ch.Net.bind(Net.java:455)
     [junit]     at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
     [junit]     at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
     [junit]     at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:67)
     [junit]     at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:90)
     [junit]     at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:137)
     [junit]     at org.apache.zookeeper.server.ServerCnxnFactory.createFactory(ServerCnxnFactory.java:130)
     [junit]     at org.apache.zookeeper.test.LoadFromLogTest.testReloadSnapshotWithMissingParent(LoadFromLogTest.java:412)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
     [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
     [junit]     at java.lang.reflect.Method.invoke(Method.java:606)
     [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
     [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
     [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
     [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
     [junit]     at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:55)
     [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
     [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
     [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
     [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
     [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
     [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
     [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
     [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
     [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182)
     [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033)
{noformat}
",Sadness,-1
